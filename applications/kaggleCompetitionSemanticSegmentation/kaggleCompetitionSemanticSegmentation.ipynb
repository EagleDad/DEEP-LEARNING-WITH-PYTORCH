{"cells":[{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/EagleDad/DEEP-LEARNING-WITH-PYTORCH/blob/master/applications/kaggleCompetitionSemanticSegmentation/kaggleCompetitionSemanticSegmentation.ipynb)"],"metadata":{"id":"eA-wIciarM8n"}},{"cell_type":"code","source":["#\n","# Mount Google Drive here to store results of training and keep them if a session expires\n","#\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"V4hULau5rbaC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736771370244,"user_tz":-60,"elapsed":23753,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}},"outputId":"b0e083fb-ebbd-4f9f-995d-122fa2ab54cb"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["**Creating the required folder structures**"],"metadata":{"id":"dAdZ2Hh_rs55"}},{"cell_type":"code","source":["%mkdir -p /content/drive/MyDrive/DLPT/Project4"],"metadata":{"id":"sRJs0Lf1sX7x","executionInfo":{"status":"ok","timestamp":1736771385518,"user_tz":-60,"elapsed":229,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/DLPT/Project4"],"metadata":{"id":"ydcWB9lKru_h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736771393940,"user_tz":-60,"elapsed":228,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}},"outputId":"c86e26e4-9072-4252-9578-9b4919df9116"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/DLPT/Project4\n"]}]},{"cell_type":"code","source":["%pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"GDYwalZupA5M","executionInfo":{"status":"ok","timestamp":1736771411522,"user_tz":-60,"elapsed":257,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}},"outputId":"28c84644-50b9-47dd-c61a-16c97f94ee8b"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/DLPT/Project4'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["**Install kaggle access**"],"metadata":{"id":"yYUbWlQzr1AS"}},{"cell_type":"code","source":["!pip install -q kaggle"],"metadata":{"id":"dHNTeCQhr5rx","executionInfo":{"status":"ok","timestamp":1736771421393,"user_tz":-60,"elapsed":3856,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["**To be able to access data from kaggle, the API token needs to be loaded here**"],"metadata":{"id":"glugXJ0fr9kh"}},{"cell_type":"code","source":["from google.colab import files\n","_ = files.upload()"],"metadata":{"id":"xD2T8s5Pr-6w","colab":{"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"status":"ok","timestamp":1736771433904,"user_tz":-60,"elapsed":9490,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}},"outputId":"4ab3114a-e423-452a-8e6d-68ba9be16d23"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-11f6b9e9-4457-4bf5-8899-4ceaaf01c877\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-11f6b9e9-4457-4bf5-8899-4ceaaf01c877\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving kaggle.json to kaggle.json\n"]}]},{"cell_type":"code","source":["!mkdir ~/.kaggle"],"metadata":{"id":"UINmjLUGsFtI","executionInfo":{"status":"ok","timestamp":1736771437634,"user_tz":-60,"elapsed":236,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["!cp kaggle.json ~/.kaggle/"],"metadata":{"id":"rreEoxOAsHAI","executionInfo":{"status":"ok","timestamp":1736771442429,"user_tz":-60,"elapsed":221,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["!chmod 600 ~/.kaggle/kaggle.json"],"metadata":{"id":"BKfs47odsIXo","executionInfo":{"status":"ok","timestamp":1736771445729,"user_tz":-60,"elapsed":231,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["!kaggle datasets list"],"metadata":{"id":"Mpl81bkJsJ3Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736771449432,"user_tz":-60,"elapsed":1422,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}},"outputId":"904cc4be-a29d-42cf-a65a-9df6d8ba73a6"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["ref                                                            title                                             size  lastUpdated          downloadCount  voteCount  usabilityRating  \n","-------------------------------------------------------------  -----------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n","anandshaw2001/netflix-movies-and-tv-shows                      Netflix Movies and TV Shows                        1MB  2025-01-03 10:33:01           2551         73  1.0              \n","stealthtechnologies/predict-student-performance-dataset        Predict Student Performance                       12KB  2024-12-26 12:57:04           3530         83  1.0              \n","sebastianwillmann/beverage-sales                               Beverage Sales                                   119MB  2025-01-02 21:00:53            938         22  1.0              \n","ankushpanday1/heart-attack-in-youth-vs-adults-in-indonesia     Heart Attack in Youth vs Adults in Indonesia       4MB  2025-01-09 14:35:59            546         30  1.0              \n","suvroo/credit-card-behaviour-score                             Credit Card Behaviour Score                       70MB  2025-01-10 16:14:49            555         23  1.0              \n","akashsharma0105/phone-usage-in-india                           Phone Usage in India                             444KB  2025-01-11 10:06:50            462         23  1.0              \n","ankushpanday1/heart-attack-in-youth-vs-adult-in-france         Heart Attack in Youth VS Adult in France           8MB  2025-01-07 17:13:59           1078         36  1.0              \n","ankushpanday1/heart-attack-in-youth-vs-adult-in-germany        Heart Attack in Youth Vs Adult in Germany          6MB  2025-01-08 14:33:17           1190         32  1.0              \n","yamaerenay/spotify-dataset-1921-2020-160k-tracks               Spotify Dataset 1921-2020, 160k+ Tracks           16MB  2025-01-06 21:27:40            890         26  1.0              \n","arifmia/heart-attack-risk-dataset                              heart attack risk dataset                        992KB  2025-01-08 19:24:30           1247         24  0.7058824        \n","anandshaw2001/mobile-apps-screentime-analysis                  Mobile Apps ScreenTime Analysis                    2KB  2024-12-31 18:20:51           1704         44  1.0              \n","ajinilpatel/energy-consumption-prediction                      Energy consumption prediction                    240KB  2025-01-10 04:41:55            568         25  1.0              \n","stealthtechnologies/gdp-growth-of-african-countries            GDP Growth of African Countries                   13KB  2025-01-02 06:53:54            981         24  1.0              \n","taweilo/wine-quality-dataset-balanced-classification           Wine Quality dataset - Classification            377KB  2025-01-07 04:13:51            836         24  1.0              \n","ankushpanday1/heart-attack-risk-in-youth-vs-adult-in-pakistan  Heart Attack Risk in Youth Vs Adult in Pakistan    9MB  2025-01-10 15:22:34            381         22  1.0              \n","govindaramsriram/energy-consumption-dataset-linear-regression  Energy Consumption Dataset - Linear Regression    16KB  2025-01-06 16:09:37           1021         28  1.0              \n","oktayrdeki/heart-disease                                       Heart Disease                                    568KB  2024-12-29 13:26:49           2321         41  1.0              \n","prajwaldongre/global-financial-giants-by-revenue-2024          Global Financial Giants by Revenue 2024            2KB  2025-01-08 07:07:19            819         26  1.0              \n","umerhaddii/apple-stock-data-2025                               Apple Stock Data 2025                            301KB  2025-01-03 16:47:04           1332         47  1.0              \n","anandshaw2001/imdb-data                                        Full IMDb Movies Dataset                         153MB  2025-01-03 05:48:46            810         25  1.0              \n"]}]},{"cell_type":"markdown","source":["**Download kaggle imagesd**"],"metadata":{"id":"5fkto_SjpfOa"}},{"cell_type":"code","source":["#! kaggle competitions download -c '10 Monkey Species'\n","!kaggle competitions download -c opencv-pytorch-segmentation-project-round2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T1kUt8-mpfpZ","executionInfo":{"status":"ok","timestamp":1736771598433,"user_tz":-60,"elapsed":14219,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}},"outputId":"51d8c377-fb2f-4959-f18e-c9061816ca30"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading opencv-pytorch-segmentation-project-round2.zip to /content/drive/MyDrive/DLPT/Project4\n"," 99% 729M/738M [00:12<00:00, 58.1MB/s]\n","100% 738M/738M [00:12<00:00, 61.8MB/s]\n"]}]},{"cell_type":"code","source":["!unzip -q 'opencv-pytorch-segmentation-project-round2.zip'"],"metadata":{"id":"-qq0hvuT3IzD","executionInfo":{"status":"ok","timestamp":1736775200184,"user_tz":-60,"elapsed":100521,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["!rm -rf 'opencv-pytorch-segmentation-project-round2.zip'"],"metadata":{"id":"d1pWqqzGCYTZ","executionInfo":{"status":"ok","timestamp":1736778038191,"user_tz":-60,"elapsed":437,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"QVp6dFAvqF1D"},"source":["# <font style=\"color:blue\">Project 4: Kaggle Competition - Semantic Segmentation</font>\n","\n","#### Maximum Points: 100\n","\n","<div>\n","    <table>\n","        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n","        <tr><td><h3>1</h3></td> <td><h3>1.1. Dataset Class</h3></td> <td><h3>7</h3></td> </tr>\n","        <tr><td><h3>2</h3></td> <td><h3>1.2. Visualize dataset</h3></td> <td><h3>3</h3></td> </tr>\n","        <tr><td><h3>3</h3></td> <td><h3>2. Evaluation Metrics</h3></td> <td><h3>10</h3></td> </tr>\n","        <tr><td><h3>4</h3></td> <td><h3>3. Model</h3></td> <td><h3>10</h3></td> </tr>\n","        <tr><td><h3>5</h3></td> <td><h3>4.1. Train</h3></td> <td><h3>7</h3></td> </tr>\n","        <tr><td><h3>6</h3></td> <td><h3>4.2. Inference</h3></td> <td><h3>3</h3></td> </tr>\n","        <tr><td><h3>7</h3></td> <td><h3>5. Prepare Submission CSV</h3></td><td><h3>10</h3></td> </tr>\n","        <tr><td><h3>8</h3></td> <td><h3>6. Kaggle Profile Link</h3></td> <td><h3>50</h3></td> </tr>\n","    </table>\n","</div>\n","\n","---\n","\n","**In this project, you have participated in the Kaggle competition, and also submit the notebook and othe code in the course lab.**\n","\n","**This Kaggle competition is a semantic segmentation challenge.**\n","\n","<h2>Dataset Description </h2>\n","<p>The dataset consists of 3,269 images in 12 classes (including background). All images were taken from drones in a variety of scales. Samples are shown below:\n","<img src=\"https://www.dropbox.com/scl/fi/pswwraz1cc9srd9d4hxm3/data_montage.jpg?rlkey=074v9mc32et70ijl0dz3y0rvs&dl=1\" width=\"800\" height=\"800\">\n","<p>The data was splitted into public train set and private test set which is used for evaluation of submissions. You can split public subset into train and validation sets yourself.\n","Images are named with a unique <code>ImageId</code>. </p>\n","<p> You should segment and classify the images in the test set.</p>\n","<p>The dataset consists of landscape images taken from drones in a variety of scales.</p>\n","\n","**The notebook is divided into sections. You have to write code, as mention in the section.  For other helper functions, you can write `.py` files and import them in the notebook. You have to submit the notebook along with `.py` files. Your submitted code must be runnable without any bug.**"]},{"cell_type":"markdown","metadata":{"id":"xs3uca1NqF1G"},"source":["# <font style=\"color:green\">1. Data Exploration</font>\n","\n","In this section, you have to write your custom dataset class and visualize a few images (max five images) and its mask."]},{"cell_type":"markdown","metadata":{"id":"shTxJ2keqF1G"},"source":["## <font style=\"color:green\">1.1. Dataset Class [7 Points]</font>\n","\n","**In this sub-section, write your custom dataset class.**\n","\n","\n","**Note that there are not separate validation data, so you will have to create your validation set by dividing train data into train and validation data. Usually, in practice, we do `80:20` ratio for train and validation, respectively.**\n","\n","**for example:**\n","\n","```\n","class SemSegDataset(Dataset):\n","    \"\"\" Generic Dataset class for semantic segmentation datasets.\n","\n","        Arguments:\n","            data_path (string): Path to the dataset folder.\n","            images_folder (string): Name of the folder containing the images (related to the data_path).\n","            masks_folder (string): Name of the folder containing the masks (related to the data_path).\n","            csv_path (string): train or test csv file name\n","            image_ids (list): List of images.\n","            train_val_test (string): 'train', 'val' or 'test'\n","            transforms (callable, optional): A function/transform that inputs a sample\n","                and returns its transformed version.\n","            class_names (list, optional): Names of the classes.\n","            \n","\n","        Dataset folder structure:\n","            Folder containing the dataset should look like:\n","            - data_path\n","            -- images_folder\n","            -- masks_folder\n","\n","            Names of images in the images_folder and masks_folder should be the same for same samples.\n","    \"\"\"\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"a1-RHgTiqF1G"},"outputs":[],"source":["# dataset class implementation\n","class SemSegDataset(Dataset):\n","    \"\"\" Generic Dataset class for semantic segmentation datasets.\n","\n","        Arguments:\n","            data_path (string): Path to the dataset folder.\n","            images_folder (string): Name of the folder containing the images (related to the data_path).\n","            masks_folder (string): Name of the folder containing the masks (related to the data_path).\n","            csv_path (string): train or test csv file name\n","            image_ids (list): List of images.\n","            train_val_test (string): 'train', 'val' or 'test'\n","            transforms (callable, optional): A function/transform that inputs a sample\n","                and returns its transformed version.\n","            class_names (list, optional): Names of the classes.\n","            num_classes (int): Number of classes in the dataset.\n","            dataset_url (string, optional): url to remote repository containing the dataset.\n","            dataset_folder (string, optional): Folder containing the dataset (related to the git repo).\n","\n","        Dataset folder structure:\n","            Folder containing the dataset should look like:\n","            - data_path\n","            -- images_folder\n","            -- masks_folder\n","\n","            Names of images in the images_folder and masks_folder should match the same sample.\n","\n","    \"\"\"\n","    def __init__(\n","        self,\n","        data_path,\n","        images_folder,\n","        masks_folder,\n","        csv_folder,\n","        image_ids,\n","        train_val_test,\n","        transforms=None,\n","        class_names=None,\n","        num_classes = None,\n","        dataset_url=None,\n","        dataset_folder=None\n","    ):\n","\n","        self.num_classes = num_classes\n","        self.transforms = transforms\n","        self.class_names = class_names\n","        # check whether dataset loading parameters exist\n","        if not os.path.isdir(data_path) and dataset_url is not None and dataset_folder is not None:\n","            # download CamVid dataset to the predefined directory\n","            download_git_folder(dataset_url, dataset_folder, data_path)\n","        # get the map of image-mask pairs\n","        self.dataset = init_semantic_segmentation_dataset(data_path, images_folder, masks_folder)\n","\n","    def get_num_classes(self):\n","        \"\"\"Get number of classes in the dataset\"\"\"\n","        return self.num_classes\n","\n","    def get_class_name(self, idx):\n","        \"\"\"\n","            Get a specific class name\n","\n","            Arguments:\n","                idx (int): index of specific class.\n","\n","            Returns:\n","                If class_names are available and idx < number of classes,\n","                returns a specific class name, else returns an empty string.\n","        \"\"\"\n","        class_name = \"\"\n","        if self.class_names is not None and idx < len(self.num_classes):\n","            class_name = self.class_names[idx]\n","        return class_name\n","\n","    # get dataset's length\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    # get item by index\n","    def __getitem__(self, idx):\n","        sample = {\n","            \"image\": cv2.imread(self.dataset[idx][\"image\"])[..., ::-1],\n","            \"mask\": cv2.imread(self.dataset[idx][\"mask\"], 0)\n","        }\n","        # apply transforms to a sample\n","        if self.transforms is not None:\n","            sample = self.transforms(**sample)\n","            sample[\"mask\"] = sample[\"mask\"].long()\n","        return sample"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"JnK-O8AyqF1H"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"k2M9rw6eqF1H"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"VBcHk4A8qF1H"},"source":["## <font style=\"color:green\">1.2. Visualize dataset [3 Points]</font>\n","\n","**In this sub-section,  you have to plot a few images and its mask.**\n","\n","**for example:**\n","\n","---\n","\n","<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w12-data-sample.png\">\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"VfkRxAL0qF1I"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"rAvljzJvqF1I"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"EmMDYr6SqF1I"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"rwvJOCZ3qF1I"},"source":["# <font style=\"color:green\">2. Evaluation Metrics [10 Points]</font>\n","\n","<p>This competition is evaluated on the mean <a href='https://en.wikipedia.org/wiki/Sørensen–Dice_coefficient'>Dice coefficient</a\n",">. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by: </p>\n","\n","\n","$$DSC =  \\frac{2 |X \\cap Y|}{|X|+ |Y|}$$\n","$$ \\small \\mathrm{where}\\ X = Predicted\\ Set\\ of\\ Pixels,\\ \\ Y = Ground\\ Truth $$\n","The Dice coefficient is defined to be 1 when both X and Y are empty.\n","\n","**In this section, you have to implement the dice coefficient evaluation metric.**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"rB7ga_GKqF1I"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"OOVxr2H0qF1I"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"xaRhpebiqF1I"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"74wqUo-8qF1I"},"source":["# <font style=\"color:green\">3. Model [10 Points]</font>\n","\n","**In this section, you have to define your model.**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"WI2gjN0rqF1J"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"s7wUTFwOqF1J"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"HzuS0LvhqF1J"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"xkMJFO5zqF1J"},"source":["# <font style=\"color:green\">4. Train & Inference</font>\n","\n","- **In this section, you have to train the model and infer on sample data.**\n","\n","\n","- **You can write your trainer class in this section.**\n","\n","\n","- **If you are using any loss function other than PyTorch standard loss function, you have to define in this section.**\n","\n","\n","- **This section should also have optimizer and LR-schedular (if using) details.**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mtO1xhe2qF1J"},"source":["## <font style=\"color:green\">4.1. Train [7 Points]</font>\n","\n","**Write your training code in this sub-section.**\n","\n","\n","**This section must contain training plots (use matplotlib or share tensorboard.dev scalars logs).**\n","\n","**You must have to plot the following:**\n","- **train loss**\n","\n","\n","- **validation loss**\n","\n","\n","- **IoU for all twelve classes (0-11) and the mean IoU of all classes on validatin data.**\n","\n","**an example of matplotlib plot:**\n","\n","---\n","\n","<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w12-train-loss.png'>\n","\n","---\n","\n","<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w12-val-loss.png'>\n","\n","---\n","\n","<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w12-mean_iou.png'>\n","\n","---\n","\n","<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w12-iou-0.png'>\n","\n","---\n","\n","<center>*</center>\n","<center>*</center>\n","<center>*</center>\n","\n","---\n","\n","<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w12-iou-11.png'>\n","\n","---\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"MtuhIUkmqF1J"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"G-K-oOKpqF1J"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"69UhqmELqF1J"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"fUYn_MN6qF1J"},"source":["## <font style=\"color:green\">4.2. Inference [3 Points]</font>\n","\n","**Plot some sample inference in this sub-section.**\n","\n","**for example:**\n","\n","---\n","\n","<img src='https://www.learnopencv.com/wp-content/uploads/2020/04/c3-w12-sample-predtiction.png'>\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"runaUy8qqF1J"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"khsiYODAqF1J"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"-iXCfDCEqF1J"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"QUiGRxMGqF1J"},"source":["# <font style=\"color:green\">5. Prepare Submission CSV [10 Points]</font>\n","\n","**Write your code to prepare the submission CSV file.**\n","\n","\n","**Note that in the submission file, you have to write Encoded Pixels.**\n","\n","[Here is a blog to understand what is Encoded Pixels.](https://medium.com/analytics-vidhya/generating-masks-from-encoded-pixels-semantic-segmentation-18635e834ad0)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"ffk1zuUqqF1J"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"6C7V8LeiqF1J"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"TY2r3hm_qF1J"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"H1bBQZZOqF1J"},"source":["# <font style=\"color:green\">6. Kaggle Profile Link [50 Points]</font>\n","\n","Share your Kaggle profile link here with us so that we can give points for the competition score.\n","\n","You should have a minimum IoU of `0.60` on the test data to get all points. If the IoU is less than `0.55`, you will not get any points for the section.\n","\n","**You must have to submit `submission.csv` (prediction for images in `test.csv`) in `Submit Predictions` tab in Kaggle to get any evaluation in this section.**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"FHbW3qw3qF1J"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}