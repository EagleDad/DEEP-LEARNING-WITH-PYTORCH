{"cells":[{"cell_type":"markdown","metadata":{"id":"H44pKtxkE9xP"},"source":["# <font style=\"color:blue\">Faster RCNN Fine-tuning</font>\n","\n","We have already seen the below Faster RCNN flow-diagram. We have also used the PyTorch pre-trained object detection model (`torchvision.models.detection.fasterrcnn_resnet50_fpn`) to infer on data samples.\n","\n","We know that this model is trained on the [coco dataset](https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/) that has `91` classes. What if our dataset doesn't have the class which we are interested in? Even though our interest class is available and the number of classes much lower than than the coco dataset, it is better to fine-tune the model for better mAP (mean average precession).\n","\n","\n","If the model inference speed is slowfor our requirements, we might be interested in changing the backbone of the Faster RCNN model.\n","\n","\n","We will see resnet-50_fpn implementation building blocks. We will use the understanding to fine-tune the model.\n","\n","\n","#  <font style=\"color:blue\">1. Faster RCNN with Resnet-50 FPN Backbone</font>\n","\n","---\n","\n","![](https://www.researchgate.net/profile/Giang_Son_Tran/publication/324549019/figure/fig1/AS:649929152266241@1531966593689/Faster-R-CNN-Architecture-9.png)\n","\n","---\n","\n","**Let's start with mapping above building blocks with PyTorch `torchvision.models.detection.fasterrcnn_resnet50_fpn` implementation.**"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"eq9QpACQE9xR","executionInfo":{"status":"ok","timestamp":1724151343385,"user_tz":-120,"elapsed":5227,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[],"source":["import torch\n","import torchvision\n","\n","from PIL import Image\n","import torchvision.transforms as T\n"]},{"cell_type":"markdown","metadata":{"id":"LF9B2WW_E9xW"},"source":["**Find details of FasterRCNN with Resnet-50 FPN backbone [here](https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.detection.fasterrcnn_resnet50_fpn)**.\n","\n","Let's load pre-trained Faster RCNN with ResNet-50 FPN backbone detection model."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"XdbgjKbWH_M8","executionInfo":{"status":"ok","timestamp":1724151347425,"user_tz":-120,"elapsed":782,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[],"source":["if torch.cuda.is_available():\n","    device = \"cuda\"\n","else:\n","    device = \"cpu\""]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XhOaPzQdE9xX","executionInfo":{"status":"ok","timestamp":1724151354552,"user_tz":-120,"elapsed":3872,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}},"outputId":"f8f4e585-7e67-429e-e130-9f37f26467d4"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n","100%|██████████| 160M/160M [00:02<00:00, 56.6MB/s]\n"]}],"source":["# load fasterrcnn_resnet50_fpn\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","model = model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"Fi3HRlQoE9xb"},"source":["## <font style=\"color:green\">1.1. Inputs Samples</font>\n","\n","**Let's load two images and their target.**\n","\n","**[Download image1](https://www.dropbox.com/s/jet087pwhln5b2j/FudanPed00066.png?dl=1)**\n","\n","**[Download image2](https://www.dropbox.com/s/uv8676diqwrstvl/PennPed00011.png?dl=1)**"]},{"cell_type":"code","source":["!wget https://www.dropbox.com/s/jet087pwhln5b2j/FudanPed00066.png?dl=1 -O FudanPed00066.png"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jslNW-ivbk5F","executionInfo":{"status":"ok","timestamp":1724151466220,"user_tz":-120,"elapsed":3875,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}},"outputId":"9d9e4394-6f36-40c7-a583-cf1d664c6a93"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-08-20 10:57:40--  https://www.dropbox.com/s/jet087pwhln5b2j/FudanPed00066.png?dl=1\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6035:18::a27d:5512\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://www.dropbox.com/scl/fi/oz2bsjah06who81p82xq2/FudanPed00066.png?rlkey=rf725idesnozpgih57tpwacw5&dl=1 [following]\n","--2024-08-20 10:57:41--  https://www.dropbox.com/scl/fi/oz2bsjah06who81p82xq2/FudanPed00066.png?rlkey=rf725idesnozpgih57tpwacw5&dl=1\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc7432b8b6f336b36353e2d2bbed.dl.dropboxusercontent.com/cd/0/inline/CY82TOKz4mSzWhRbMPB5OiAzNxh075UoPb-6U6sgRCrMqr-nqbOwpinOnu4pLGDrl27lRjlTNkoB3Ef7mmfXiAEaza7_tIwCM7tje54IUpb1t20_ezN0MzJOHG_6bpFSsZctDcvLwYDSCMB2QkAeSbYx/file?dl=1# [following]\n","--2024-08-20 10:57:41--  https://uc7432b8b6f336b36353e2d2bbed.dl.dropboxusercontent.com/cd/0/inline/CY82TOKz4mSzWhRbMPB5OiAzNxh075UoPb-6U6sgRCrMqr-nqbOwpinOnu4pLGDrl27lRjlTNkoB3Ef7mmfXiAEaza7_tIwCM7tje54IUpb1t20_ezN0MzJOHG_6bpFSsZctDcvLwYDSCMB2QkAeSbYx/file?dl=1\n","Resolving uc7432b8b6f336b36353e2d2bbed.dl.dropboxusercontent.com (uc7432b8b6f336b36353e2d2bbed.dl.dropboxusercontent.com)... 162.125.85.15, 2620:100:6035:15::a27d:550f\n","Connecting to uc7432b8b6f336b36353e2d2bbed.dl.dropboxusercontent.com (uc7432b8b6f336b36353e2d2bbed.dl.dropboxusercontent.com)|162.125.85.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 241574 (236K) [application/binary]\n","Saving to: ‘FudanPed00066.png’\n","\n","FudanPed00066.png   100%[===================>] 235.91K   372KB/s    in 0.6s    \n","\n","2024-08-20 10:57:43 (372 KB/s) - ‘FudanPed00066.png’ saved [241574/241574]\n","\n"]}]},{"cell_type":"code","source":["!wget https://www.dropbox.com/s/uv8676diqwrstvl/PennPed00011.png?dl=1  -O PennPed00011.png"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wbZPXHTObzY7","executionInfo":{"status":"ok","timestamp":1724151471862,"user_tz":-120,"elapsed":3348,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}},"outputId":"37380bf4-4542-4b30-caab-e2eabb31eb92"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-08-20 10:57:46--  https://www.dropbox.com/s/uv8676diqwrstvl/PennPed00011.png?dl=1\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6035:18::a27d:5512\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://www.dropbox.com/scl/fi/kbl671j5gb29pvgs4oi4s/PennPed00011.png?rlkey=9vtf4gr60oy4uagcf14poqwuv&dl=1 [following]\n","--2024-08-20 10:57:47--  https://www.dropbox.com/scl/fi/kbl671j5gb29pvgs4oi4s/PennPed00011.png?rlkey=9vtf4gr60oy4uagcf14poqwuv&dl=1\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc14eb783cd1b4bf9292a22c2f7d.dl.dropboxusercontent.com/cd/0/inline/CY93qWDcAJWZ52yAe92CxjFbZpxQSmxwjaPDdOpOExaAApwuME9FuFIXDuGrycVlxDZHFzASTDTxPGsTCqcl5XflmJks2Xmwo9Bj1LJwnJB1KbXr0DRVEyf531HfrLccfjeZLmxiEuBwEMhJI6ucGyos/file?dl=1# [following]\n","--2024-08-20 10:57:48--  https://uc14eb783cd1b4bf9292a22c2f7d.dl.dropboxusercontent.com/cd/0/inline/CY93qWDcAJWZ52yAe92CxjFbZpxQSmxwjaPDdOpOExaAApwuME9FuFIXDuGrycVlxDZHFzASTDTxPGsTCqcl5XflmJks2Xmwo9Bj1LJwnJB1KbXr0DRVEyf531HfrLccfjeZLmxiEuBwEMhJI6ucGyos/file?dl=1\n","Resolving uc14eb783cd1b4bf9292a22c2f7d.dl.dropboxusercontent.com (uc14eb783cd1b4bf9292a22c2f7d.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6057:15::a27d:d0f\n","Connecting to uc14eb783cd1b4bf9292a22c2f7d.dl.dropboxusercontent.com (uc14eb783cd1b4bf9292a22c2f7d.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 340791 (333K) [application/binary]\n","Saving to: ‘PennPed00011.png’\n","\n","PennPed00011.png    100%[===================>] 332.80K   530KB/s    in 0.6s    \n","\n","2024-08-20 10:57:49 (530 KB/s) - ‘PennPed00011.png’ saved [340791/340791]\n","\n"]}]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TO0Q-EzFE9xc","outputId":"5cf35fa3-f25a-4ea7-91ad-7292dd85ff03","executionInfo":{"status":"ok","timestamp":1724151476425,"user_tz":-120,"elapsed":486,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Image 1 size: torch.Size([3, 359, 360])\n","Image 2 size: torch.Size([3, 376, 508])\n"]}],"source":["image1 = T.ToTensor()(Image.open('FudanPed00066.png'))\n","\n","bboxes1 = torch.tensor([[248.0, 50.0, 329.0, 351.0]])\n","labels1 = torch.tensor([1])\n","\n","image2 = T.ToTensor()(Image.open('PennPed00011.png'))\n","\n","bboxes2 = torch.tensor([[92.0, 62.0, 236.0, 344.0], [242.0, 52.0, 301.0, 355.0]])\n","labels2 = torch.tensor([1, 1])\n","\n","print('Image 1 size: {}'.format(image1.size()))\n","\n","print('Image 2 size: {}'.format(image2.size()))"]},{"cell_type":"markdown","metadata":{"id":"4Bs_MX6wE9xh"},"source":["**We can see that both images (`image1` and `image2`) have different sizes.**"]},{"cell_type":"markdown","metadata":{"id":"7eynvrGdE9xi"},"source":["## <font style=\"color:green\">1.2. Model Inference</font>\n","\n","- We need a list (not tensor) of images for the model inference.\n","\n","\n","- Images size may be different. This means we need not resize to a constant size. Faster RCNN PyTorch Implementation has its own image pre-process block."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"au8SsW99E9xj","outputId":"14fddcfa-008d-45a2-8ee9-57d85bbe4926","executionInfo":{"status":"ok","timestamp":1724151484326,"user_tz":-120,"elapsed":2684,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[{'boxes': tensor([[243.1263,  47.7870, 327.8619, 349.8769]], device='cuda:0',\n","       grad_fn=<StackBackward0>), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.9997], device='cuda:0', grad_fn=<IndexBackward0>)}, {'boxes': tensor([[ 89.9230,  59.4910, 225.3071, 342.8298],\n","        [244.2283,  49.8334, 304.4795, 362.8903],\n","        [245.9230, 127.6201, 276.5670, 197.8546],\n","        [252.0381,  15.8489, 369.9043, 367.3777],\n","        [245.7938,  99.7875, 294.3491, 198.7888],\n","        [243.9077, 121.8000, 276.3352, 198.4012],\n","        [247.6824,  51.5020, 301.1053, 203.1744],\n","        [245.5552,  95.3306, 295.3181, 199.0098],\n","        [274.8139,  96.3892, 301.2039, 188.7242],\n","        [123.6462,  56.9277, 191.9256, 338.0053],\n","        [240.7440,  44.0858, 333.5301, 235.6631],\n","        [267.6390, 100.0402, 299.7915, 187.5079]], device='cuda:0',\n","       grad_fn=<StackBackward0>), 'labels': tensor([ 1,  1, 27,  1, 27, 31,  1, 31, 27,  1,  1, 31], device='cuda:0'), 'scores': tensor([0.9996, 0.9931, 0.7120, 0.6038, 0.3407, 0.3300, 0.3079, 0.2703, 0.2572,\n","        0.0977, 0.0517, 0.0514], device='cuda:0', grad_fn=<IndexBackward0>)}]\n"]}],"source":["input_image1 = image1.clone()\n","\n","input_image2 = image2.clone()\n","\n","# input its image list\n","inputs = [input_image1.to(device), input_image2.to(device)]\n","\n","model.eval()\n","output = model(inputs)\n","\n","print(output)"]},{"cell_type":"markdown","metadata":{"id":"yJEE4S-LE9xn"},"source":["## <font style=\"color:green\">1.3. Model Training</font>\n","\n","- In training mode, `targets` are mandatory. Targets are required because they calculate loss. This loss can be used to find gradients by using `backward()`.\n","\n","\n","- Targets should be a list, and each target should have the following format:\n","\n","```\n","    {\n","        'boxes': bounding boxes tensor,\n","        'labels': label tensor\n","    \n","    }\n","```\n","\n","\n","- Object detection in Faster RCNN is done in two stages.\n","\n","\n","- First, it classifies all regions of the image in just two classes- background or object.\n","\n","\n","- In the second stage, it predicts classes of the object and improves its bounding box predictions."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3XuPMuK6E9xo","outputId":"1cbb929f-f240-4a22-906d-f633bff1a4c0","executionInfo":{"status":"ok","timestamp":1724151499811,"user_tz":-120,"elapsed":1340,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'loss_classifier': tensor(0.0272, device='cuda:0', grad_fn=<NllLossBackward0>),\n"," 'loss_box_reg': tensor(0.0288, device='cuda:0', grad_fn=<DivBackward0>),\n"," 'loss_objectness': tensor(0.0001, device='cuda:0',\n","        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n"," 'loss_rpn_box_reg': tensor(0.0048, device='cuda:0', grad_fn=<DivBackward0>)}"]},"metadata":{},"execution_count":9}],"source":["input_image1 = image1.clone()\n","\n","target1 = {\n","    'boxes': bboxes1.clone().to(device),\n","    'labels' : labels1.clone().to(device)\n","\n","}\n","\n","input_image2 = image2.clone()\n","\n","\n","target2 = {\n","    'boxes': bboxes2.clone().to(device),\n","    'labels' : labels2.clone().to(device)\n","\n","}\n","\n","inputs = [input_image1.to(device), input_image2.to(device)]\n","targets = [target1, target2]\n","\n","# change to train mode\n","model.train()\n","model(inputs, targets)"]},{"cell_type":"markdown","metadata":{"id":"HbTnro3DE9xs"},"source":["- `loss_objectness` and `loss_rpn_box_reg` are losses of the first stage.\n","\n","\n","- `loss_classifier` and `loss_box_reg` are losses of the second stage."]},{"cell_type":"markdown","metadata":{"id":"P6gwBq-XE9xt"},"source":["## <font style=\"color:green\">1.4. Model Building Blocks</font>\n","\n","**We will see building blocks of the model and modifying the cloned model for fine-tuning.**\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M4u27nkYE9xu","outputId":"ae9008e4-3473-43a9-d4ac-c32a5194da44","executionInfo":{"status":"ok","timestamp":1724151507193,"user_tz":-120,"elapsed":391,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["FasterRCNN(\n","  (transform): GeneralizedRCNNTransform(\n","      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n","  )\n","  (backbone): BackboneWithFPN(\n","    (body): IntermediateLayerGetter(\n","      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","      (relu): ReLU(inplace=True)\n","      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (layer1): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): FrozenBatchNorm2d(256, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(512, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(1024, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (5): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(2048, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","    )\n","    (fpn): FeaturePyramidNetwork(\n","      (inner_blocks): ModuleList(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (layer_blocks): ModuleList(\n","        (0-3): 4 x Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","      )\n","      (extra_blocks): LastLevelMaxPool()\n","    )\n","  )\n","  (rpn): RegionProposalNetwork(\n","    (anchor_generator): AnchorGenerator()\n","    (head): RPNHead(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): ReLU(inplace=True)\n","        )\n","      )\n","      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n","  (roi_heads): RoIHeads(\n","    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n","    (box_head): TwoMLPHead(\n","      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n","    )\n","    (box_predictor): FastRCNNPredictor(\n","      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n","    )\n","  )\n",")\n"]}],"source":["model.eval()\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"GNjceklXE9xz"},"source":["**We can see the model has the following building blocks of `FasterRCNN`:**\n","\n","- **`transform`:** This block pre-processes the input image.\n","\n","\n","- **`backbone`:** This is equivalent to **conv layers** in the above image.\n","\n","\n","- **`rpn`:** This is equivalent to **Region Proposal Network** in the above image.\n","\n","\n","- **`roi_heads`:** This is equivalent to **RoI Pooling**.\n","\n","\n","- **`box_predictor`:** This is equivalent to **classifier** in the above image."]},{"cell_type":"markdown","metadata":{"id":"P547n_I8E9x0"},"source":["### <font style=\"color:green\">transform</font>\n","\n","- This block pre-processes the input like normalizing, resizing, etc.\n","\n","Let's have a look at the pre-processed tensor size."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qXnN2R1nE9x1","outputId":"f047c52e-2dd7-4c0f-b312-68a15ac5b935","executionInfo":{"status":"ok","timestamp":1724151514231,"user_tz":-120,"elapsed":3,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor size: torch.Size([2, 3, 800, 1088])\n"]}],"source":["input_image1 = image1.clone()\n","\n","input_image2 = image2.clone()\n","\n","inputs = [input_image1.to(device), input_image2.to(device)]\n","\n","trans_image_list, trans_target_list = model.transform(inputs)\n","\n","print('Tensor size: {}'.format(trans_image_list.tensors.size()))\n"]},{"cell_type":"markdown","metadata":{"id":"oLsuRYeNE9x5"},"source":["Let's have a look at transforms parameters."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dBxYqu0lE9x6","outputId":"1247cf00-2b7b-4fdb-df32-f6dc7fff2064","executionInfo":{"status":"ok","timestamp":1724151520372,"user_tz":-120,"elapsed":872,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["transform ( GeneralizedRCNNTransform) parameters:\n","min_size: (800,)\n","max_size: 1333\n","image_mean: [0.485, 0.456, 0.406]\n","image_std: [0.229, 0.224, 0.225]\n"]}],"source":["print('transform ( GeneralizedRCNNTransform) parameters:')\n","print('min_size: {}'.format(model.transform.min_size))\n","print('max_size: {}'.format(model.transform.max_size))\n","print('image_mean: {}'.format(model.transform.image_mean))\n","print('image_std: {}'.format(model.transform.image_std))"]},{"cell_type":"markdown","metadata":{"id":"yxnUOYMHE9x9"},"source":["### <font style=\"color:magenta\">Transform params in Faster RCNN Fine-tune model</font>\n","\n","If we have smaller images for training, then we might be interested in changing the transform parameters."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"6S5eL7SFE9x-","executionInfo":{"status":"ok","timestamp":1724151524572,"user_tz":-120,"elapsed":826,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[],"source":["ft_min_size = 300\n","ft_max_size = 500\n","\n","ft_mean = [0.485, 0.456, 0.406]\n","ft_std = [0.229, 0.224, 0.225]"]},{"cell_type":"markdown","metadata":{"id":"WN7YRkZ-E9yB"},"source":["### <font style=\"color:green\">backbone (conv layers)</font>\n","\n","- It has used `resnet50` (trained on image net dataset) with `FPN` as the backbone for feature extraction. Don't worry if you don't know what is `FPN`. In short, it extracts features from different layers of `resnet50`. In object detection, features from different layers perform better than the immediate last layer. More details of the FPN find [here](https://arxiv.org/pdf/1612.03144.pdf).\n","\n","\n","- Generally, we use the pre-trained model (trained on extensive data set e.g., image-net) as a backbone in the object detection network.\n","\n","\n","\n","- We can change the backbone with another backbone (`resnet-18`, `vgg-16` etc. ) for fine-tuning.\n","\n","**Let's see the number of output channels of the backbone.**"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"3xpjfOZiE9yC","executionInfo":{"status":"ok","timestamp":1724151527995,"user_tz":-120,"elapsed":457,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[],"source":["backbone_out = model.backbone(trans_image_list.tensors)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EX1KGr72E9yF","outputId":"a42d2ec9-c8f6-4cfe-ff83-013c0b477c8a","executionInfo":{"status":"ok","timestamp":1724151529964,"user_tz":-120,"elapsed":2,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["0: torch.Size([2, 256, 200, 272])\n","1: torch.Size([2, 256, 100, 136])\n","2: torch.Size([2, 256, 50, 68])\n","3: torch.Size([2, 256, 25, 34])\n","pool: torch.Size([2, 256, 13, 17])\n"]}],"source":["for key, value in backbone_out.items():\n","    print('{}: {}'.format(key, value.size()))"]},{"cell_type":"markdown","metadata":{"id":"NEx-ANrAE9yJ"},"source":["**The output of the backbone is `OrderedDict[Tensor]` of five tuples**"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PT9AWXGfE9yK","outputId":"d7581ed9-274f-4df4-c61c-e4af51200b40","executionInfo":{"status":"ok","timestamp":1724151533231,"user_tz":-120,"elapsed":820,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of output channel of the backbone: 256\n"]}],"source":["print('Number of output channel of the backbone: {}'.format(model.backbone.out_channels))"]},{"cell_type":"markdown","metadata":{"id":"PjY7uvClE9yN"},"source":["### <font style=\"color:magenta\">Backbone of Faster RCNN Fine-tune model</font>\n","\n","Let's choose pre-trained AlexNet fr0m torchvision models."]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NGdxm2asE9yO","outputId":"a53dde56-c658-4241-dd82-7db236417b40","executionInfo":{"status":"ok","timestamp":1724151538866,"user_tz":-120,"elapsed":3310,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n","100%|██████████| 233M/233M [00:02<00:00, 120MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["AlexNet(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n","    (1): ReLU(inplace=True)\n","    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (4): ReLU(inplace=True)\n","    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (7): ReLU(inplace=True)\n","    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (9): ReLU(inplace=True)\n","    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n","  (classifier): Sequential(\n","    (0): Dropout(p=0.5, inplace=False)\n","    (1): Linear(in_features=9216, out_features=4096, bias=True)\n","    (2): ReLU(inplace=True)\n","    (3): Dropout(p=0.5, inplace=False)\n","    (4): Linear(in_features=4096, out_features=4096, bias=True)\n","    (5): ReLU(inplace=True)\n","    (6): Linear(in_features=4096, out_features=1000, bias=True)\n","  )\n",")\n"]}],"source":["import torchvision.models as models\n","\n","alexnet = models.alexnet(pretrained=True)\n","print(alexnet)"]},{"cell_type":"markdown","metadata":{"id":"dJf2Xx4mE9yR"},"source":["- For the Faster RCNN backbone, we are just interested in convolution features.\n","\n","\n","- Faster RCNN also needs the number of out-channel of the backbone."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"2lizdYvQE9yS","executionInfo":{"status":"ok","timestamp":1724151543057,"user_tz":-120,"elapsed":470,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[],"source":["ft_backbone = alexnet.features\n","\n","# number of out-channel in alexnet features is 256\n","ft_backbone.out_channels = 256"]},{"cell_type":"markdown","metadata":{"id":"6QpDz8QwE9yV"},"source":["### <font style=\"color:green\">rpn (Region Proposal Network)</font>\n","\n","- It takes features from the backbone and predicts the objectness (the region and whether it is object or background) and coordinates of the region.\n","\n","\n","**What is the meaning of the region here?**\n","\n","Generally, in object detection, we use anchor (a rectangular block) to denote the region.\n","\n","---\n","\n","<img src='https://www.learnopencv.com/wp-content/uploads/2020/03/c3-w8-anchors.png' align='middle'>\n","\n","---\n","\n","- In the above image, it has two feature maps `b` (`8 x 8 feature map (grid)`) and `c` (`4 x 4 feature map (grid)`). One element of the feature map represents segments of pixels in the original image `a`.\n","\n","\n","\n","- Each feature map has a set of anchors.\n","\n","\n","- We can change the number of anchors for each feature map as fine-tuning process.\n","\n","\n","Let's first see the `rpn` (Region Proposal Network) in `resnet-50-fpn` model.\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tQDHRHIOE9yV","outputId":"c238adb2-5b94-4034-fd4a-0efa2300079f","executionInfo":{"status":"ok","timestamp":1724151548626,"user_tz":-120,"elapsed":387,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["RegionProposalNetwork(\n","  (anchor_generator): AnchorGenerator()\n","  (head): RPNHead(\n","    (conv): Sequential(\n","      (0): Conv2dNormActivation(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): ReLU(inplace=True)\n","      )\n","    )\n","    (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","    (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","  )\n",")"]},"metadata":{},"execution_count":19}],"source":["model.rpn"]},{"cell_type":"markdown","metadata":{"id":"Yw2HDi6ME9yZ"},"source":["We can see that it has two parts- (1) `anchor_generator` and (2) `head`.\n","\n","**`anchor_generator`**"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9hU3OESxE9ya","outputId":"2da0b296-4f0d-485d-b028-d9b216280324","executionInfo":{"status":"ok","timestamp":1724151552662,"user_tz":-120,"elapsed":2,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Anchor sizes: ((32,), (64,), (128,), (256,), (512,))\n","Aspect ratios: ((0.5, 1.0, 2.0), (0.5, 1.0, 2.0), (0.5, 1.0, 2.0), (0.5, 1.0, 2.0), (0.5, 1.0, 2.0))\n"]}],"source":["print('Anchor sizes: {}'.format(model.rpn.anchor_generator.sizes))\n","print('Aspect ratios: {}'.format(model.rpn.anchor_generator.aspect_ratios))"]},{"cell_type":"markdown","metadata":{"id":"OwWKBDe_E9yd"},"source":["- Sizes (32, 64, ..) corresponds to numbers of pixels in original images.\n","\n","\n","- We can see `sizes` is a tuple of five tuples.\n","\n","\n","- Each tuple corresponds to a single label CNN features of `resnet50-fpn` (note that the output of the backbone is `OrderedDict[Tensor]` of five tuples) backbone.\n","\n","\n","- We also see that `aspect ratio` is also a tuple of `five` tuples. The first tuple corresponds to the first anchor tuple, second to second, and so on.\n","\n","\n","- As each label has one `anchor size` and each anchor has three `aspect ratios`, the number of anchors per feature map will be `three` (`1*3`).\n","\n","\n","- Backbone has five label output, and each label is associated with a different `anchor size`, so the total number of anchors will be `fifteen` (`5*3`).\n","\n","\n","- Changing the anchor size and ratios may be important for fine-tuning. For example, let's assume we have to detect just pedestrians. Having aspect ratios as (`(0.5, 1.0, 2.0)`) may not be very much useful as compared to (`(2.0, 2.5, 3.0)`).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZdlHzHHdE9ye"},"source":["### <font style=\"color:magenta\">Anchor of Faster RCNN Fine-tune model</font>\n","\n","- Since the AlexNet has a single label output; the anchor size should be a single tuple."]},{"cell_type":"code","execution_count":21,"metadata":{"id":"035W_T52E9yf","executionInfo":{"status":"ok","timestamp":1724151558768,"user_tz":-120,"elapsed":390,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[],"source":["from torchvision.models.detection.rpn import AnchorGenerator\n","\n","ft_anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256),),\n","                                      aspect_ratios=((0.5, 1.0, 2.0),))"]},{"cell_type":"markdown","metadata":{"id":"IAoPBgDZE9yi"},"source":["**`head`**\n","\n","**`cls_logits`:** It is just classifying whether the corresponding feature map is an object or a background. It uses logistics regression (means if value > 0.5 then object else background). That is why the number of output channels is `3` (one channel for one aspect ratio).\n","\n","\n","**`bbox_pred`:** To represent a bounding box, we need four numbers. So output channels are 12, four for each aspect ratio.\n"]},{"cell_type":"markdown","metadata":{"id":"niK-UhEtE9yj"},"source":["### <font style=\"color:green\">roi_heads (RoI Pooling)</font>"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S4TV5rMWE9yj","outputId":"72f42176-2454-4e95-b3b2-07f40cf1aa4e","executionInfo":{"status":"ok","timestamp":1724151562072,"user_tz":-120,"elapsed":824,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["RoIHeads(\n","  (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n","  (box_head): TwoMLPHead(\n","    (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n","    (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n","  )\n","  (box_predictor): FastRCNNPredictor(\n","    (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n","    (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":22}],"source":["model.roi_heads"]},{"cell_type":"markdown","metadata":{"id":"uf_beTFCE9yo"},"source":["**`box_roi_pool`**"]},{"cell_type":"markdown","metadata":{"id":"zhjXeTZFE9yp"},"source":["- Take bounding boxes predicted by `RegionProposalNetwork head` and `convolution features` from the `backbone`.\n","\n","\n","- For bounding boxes for which `objectness score` is greater than the `threshold`, it crop features from convolution layers and resized (e.g. `14 x 14`), then sub-sample feature from resized bounding box (e.g. if `sampling_ratio` is `2` then `14 x 14` will resize to `7 x 7`).\n","\n","\n","- These sub-sampled features converted to `1-d` vector and are stacked like a batch."]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bwnQk-vmE9yp","outputId":"9e25088e-d5e8-4162-bc74-68bce2795c5c","executionInfo":{"status":"ok","timestamp":1724151564991,"user_tz":-120,"elapsed":375,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Box RoI Pool Parameters:\n","featmap_names: ['0', '1', '2', '3']\n","output_size: (7, 7)\n","sampling_ratio: 2\n"]}],"source":["print('Box RoI Pool Parameters:')\n","print('featmap_names: {}'.format(model.roi_heads.box_roi_pool.featmap_names))\n","print('output_size: {}'.format(model.roi_heads.box_roi_pool.output_size))\n","print('sampling_ratio: {}'.format(model.roi_heads.box_roi_pool.sampling_ratio))"]},{"cell_type":"markdown","metadata":{"id":"Og3wNagDE9ys"},"source":["### <font style=\"color:magenta\">RoI Pooler of Faster RCNN Fine-tune model</font>\n","\n","Recall backbone output was five tuples ordered dictionary. Lets print it below."]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eSRj95amE9yt","outputId":"6b40d9a9-bfc5-4898-809c-a6348d668fd4","executionInfo":{"status":"ok","timestamp":1724151568355,"user_tz":-120,"elapsed":379,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["odict_keys(['0', '1', '2', '3', 'pool'])"]},"metadata":{},"execution_count":24}],"source":["# backbone output keys\n","backbone_out.keys()"]},{"cell_type":"markdown","metadata":{"id":"XAIjM9V9E9yw"},"source":["`featmap_names = ['0', '1', '2', '3']`, it means for region of interest pooling current implementation have not used `'pool'` layer output.\n","\n","Do we have ordered dictionary output of the ft_backbone (alexnet.features)?\n","\n","Let's check."]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r0beRfnvE9yx","outputId":"b2f99a44-03cb-4683-cb68-31d728b16d82","executionInfo":{"status":"ok","timestamp":1724151571101,"user_tz":-120,"elapsed":3,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Tensor"]},"metadata":{},"execution_count":25}],"source":["type(ft_backbone(torch.rand((2, 3, 300, 300))))"]},{"cell_type":"markdown","metadata":{"id":"xi0_rJG5E9y1"},"source":["Oh! It is just a **tensor**. If it is just a tensor, then we can use `featmap_names=['0']`"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"iJezZr_PE9y1","executionInfo":{"status":"ok","timestamp":1724151574401,"user_tz":-120,"elapsed":370,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[],"source":["from torchvision.ops import MultiScaleRoIAlign\n","\n","ft_roi_pooler = MultiScaleRoIAlign(featmap_names=['0'], output_size=4, sampling_ratio=1)"]},{"cell_type":"markdown","metadata":{"id":"rnlnc0KyE9y4"},"source":["**`box_head`**\n","\n","- It has two fully connected layers, which takes input from the output of `box_roi_pool`.\n","\n","\n","**`box_predictor`**\n","\n","- `box_head` output goes to two fully connected layers- one for class prediction and other for bounding prediction."]},{"cell_type":"markdown","metadata":{"id":"DpYUhX1QE9y5"},"source":["#  <font style=\"color:blue\">2. Faster RCNN with AlexNet Backbone</font>"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"SMlHAZ2yE9y6","executionInfo":{"status":"ok","timestamp":1724151576918,"user_tz":-120,"elapsed":1,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[],"source":["from torchvision.models.detection import FasterRCNN\n","\n","# let number of classes 4 (including background)\n","\n","ft_model = FasterRCNN(backbone=ft_backbone,\n","                      num_classes=2,\n","                      min_size=ft_min_size,\n","                      max_size=ft_max_size,\n","                      image_mean=ft_mean,\n","                      image_std=ft_std,\n","                      rpn_anchor_generator=ft_anchor_generator,\n","                      box_roi_pool=ft_roi_pooler)\n","ft_model = ft_model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"JZuh9ZD0E9y-"},"source":["## <font style=\"color:green\">2.1. Model Inference</font>"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w5kv0V2xE9y-","outputId":"461bce28-c099-4d0d-997f-3c1dad0a45a7","executionInfo":{"status":"ok","timestamp":1724151579553,"user_tz":-120,"elapsed":555,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[{'boxes': tensor([[1.4784e+02, 2.1231e+02, 2.6337e+02, 3.5900e+02],\n","        [3.1046e-01, 2.9715e+00, 3.6000e+02, 1.4312e+02],\n","        [1.4981e+02, 1.2446e+01, 2.6790e+02, 6.4198e+01],\n","        [8.9955e+01, 3.2534e+02, 1.4333e+02, 3.5132e+02],\n","        [1.8450e+02, 3.3306e+00, 2.4001e+02, 1.0453e+02],\n","        [2.2173e+02, 6.6896e+01, 2.7490e+02, 1.7499e+02],\n","        [2.1080e+02, 1.1460e+02, 2.7498e+02, 1.4621e+02],\n","        [0.0000e+00, 3.0497e+02, 7.7632e+01, 3.5804e+02],\n","        [6.0909e+01, 3.1428e+02, 9.9466e+01, 3.5075e+02],\n","        [2.1921e+02, 3.2367e+02, 2.7054e+02, 3.5079e+02],\n","        [1.0984e+02, 3.0621e+02, 1.3842e+02, 3.5646e+02],\n","        [3.3172e+02, 2.8238e+02, 3.5998e+02, 3.0908e+02],\n","        [3.2632e+02, 2.6157e+02, 3.6000e+02, 3.5900e+02],\n","        [1.7480e+02, 3.2451e+02, 2.2713e+02, 3.5207e+02],\n","        [1.6743e+02, 2.8602e+01, 2.3074e+02, 5.4277e+01],\n","        [1.0308e+02, 2.9903e+02, 1.5664e+02, 3.5854e+02],\n","        [3.4382e+01, 3.0478e+02, 1.1487e+02, 3.5814e+02],\n","        [1.3274e+02, 3.2582e+02, 1.8585e+02, 3.5213e+02],\n","        [2.3088e+02, 9.9332e+01, 2.6113e+02, 1.4782e+02],\n","        [3.4078e+02, 2.9434e+02, 3.6000e+02, 3.5900e+02],\n","        [5.4617e+00, 3.2389e+02, 5.7298e+01, 3.5169e+02],\n","        [0.0000e+00, 3.0904e+02, 1.5086e+01, 3.5826e+02],\n","        [0.0000e+00, 2.8656e+02, 3.3012e+01, 3.5809e+02],\n","        [8.5229e+01, 2.3320e+02, 2.4317e+02, 3.5900e+02],\n","        [2.8474e+01, 3.0345e+02, 5.9216e+01, 3.5327e+02],\n","        [1.5025e+02, 3.0832e+02, 1.7949e+02, 3.5757e+02],\n","        [1.1774e+02, 3.0290e+02, 1.9279e+02, 3.5881e+02],\n","        [1.9105e+02, 1.5128e+01, 2.2016e+02, 6.4563e+01],\n","        [0.0000e+00, 3.2337e+02, 1.9085e+01, 3.4889e+02],\n","        [1.9957e+02, 9.5221e+01, 2.8925e+02, 1.5630e+02],\n","        [2.7642e+02, 6.9966e+00, 3.6000e+02, 1.8163e+02],\n","        [3.4259e+02, 2.5274e+02, 3.5996e+02, 3.4998e+02],\n","        [3.5460e+02, 3.0603e+02, 3.6000e+02, 3.5857e+02],\n","        [0.0000e+00, 2.8626e+02, 1.9072e+02, 3.5787e+02],\n","        [3.0360e+02, 6.1148e+01, 3.5000e+02, 1.5234e+02],\n","        [1.5589e+02, 2.6948e+02, 2.5552e+02, 3.3596e+02],\n","        [4.1478e+01, 1.5889e+00, 2.1007e+02, 7.9736e+01],\n","        [3.1494e+02, 2.7019e+02, 3.4485e+02, 3.2239e+02],\n","        [9.5043e+01, 5.0479e+00, 2.6315e+02, 1.1142e+02],\n","        [0.0000e+00, 5.3349e-01, 3.0062e+01, 5.7002e+01],\n","        [3.1344e+02, 3.0722e+02, 3.4291e+02, 3.5900e+02],\n","        [3.0242e+02, 2.9673e+02, 3.5800e+02, 3.5900e+02],\n","        [1.7171e+02, 2.8415e+02, 2.2601e+02, 3.1179e+02],\n","        [9.6899e+01, 2.1032e+02, 1.4984e+02, 3.1759e+02],\n","        [2.6354e+02, 5.4940e+01, 3.1855e+02, 1.4935e+02],\n","        [4.9960e+01, 2.8144e+02, 1.0482e+02, 3.0794e+02],\n","        [8.8769e+01, 2.8285e+02, 1.4484e+02, 3.0917e+02],\n","        [0.0000e+00, 1.1754e+00, 1.2301e+02, 8.3104e+01],\n","        [2.2905e+02, 3.6519e+00, 3.4194e+02, 1.7265e+02],\n","        [2.9351e+02, 2.8321e+02, 3.4293e+02, 3.0954e+02],\n","        [1.4926e+01, 1.8134e+00, 6.8930e+01, 1.0552e+02],\n","        [6.3419e+00, 1.8795e+02, 1.7542e+02, 3.2559e+02],\n","        [2.2992e+02, 1.3658e+02, 2.6005e+02, 1.8869e+02],\n","        [1.5683e+02, 3.0194e+02, 2.3858e+02, 3.5900e+02],\n","        [1.3562e+02, 2.1393e+02, 1.8944e+02, 3.1483e+02],\n","        [1.8139e+02, 9.6445e-01, 2.3560e+02, 5.5752e+01],\n","        [0.0000e+00, 1.1761e+01, 5.1496e+01, 6.8903e+01],\n","        [0.0000e+00, 2.0189e+01, 2.8258e+02, 2.3125e+02],\n","        [3.3403e+02, 3.2297e+02, 3.6000e+02, 3.4985e+02],\n","        [2.1957e+02, 1.0978e+02, 2.6946e+02, 2.0740e+02],\n","        [0.0000e+00, 1.6863e+01, 1.6113e+01, 7.0287e+01],\n","        [3.5184e+02, 2.2322e+02, 3.5994e+02, 3.5900e+02],\n","        [1.7532e+02, 2.1956e+02, 2.2587e+02, 3.2172e+02],\n","        [3.4894e+02, 2.7210e+02, 3.5998e+02, 3.1024e+02],\n","        [2.8076e+01, 1.2348e+01, 5.6866e+01, 6.7267e+01],\n","        [5.4939e+01, 2.0603e+02, 1.0913e+02, 3.1562e+02],\n","        [0.0000e+00, 1.1669e-01, 2.8065e+01, 1.3833e+01],\n","        [0.0000e+00, 2.0418e-01, 1.4438e+01, 2.4205e+01],\n","        [3.5630e+02, 2.6713e+02, 3.5999e+02, 3.1929e+02],\n","        [7.5826e+01, 2.6852e+02, 1.6228e+02, 3.3229e+02],\n","        [3.3085e+02, 1.1374e+02, 3.6000e+02, 1.3847e+02],\n","        [9.9783e+00, 2.8202e+02, 6.4038e+01, 3.0816e+02],\n","        [6.7855e+01, 2.6771e+02, 9.9271e+01, 3.1500e+02],\n","        [0.0000e+00, 2.4404e+02, 1.0590e+02, 3.5813e+02],\n","        [1.3808e+02, 2.2435e+00, 1.9536e+02, 1.0836e+02],\n","        [0.0000e+00, 3.0168e+01, 2.7485e+01, 5.7440e+01],\n","        [3.1971e+02, 5.7211e+00, 3.6000e+02, 1.6219e+02],\n","        [5.7802e+01, 2.1061e+00, 1.1401e+02, 1.0577e+02],\n","        [1.8953e+02, 2.7758e+02, 2.2295e+02, 3.2314e+02],\n","        [2.1434e+01, 1.4173e-01, 6.2240e+01, 1.8672e+01],\n","        [0.0000e+00, 2.0615e-01, 5.3568e+01, 2.6677e+01],\n","        [3.1625e+02, 9.9019e+01, 3.4298e+02, 1.5339e+02],\n","        [5.7629e+01, 2.8238e+02, 2.9743e+02, 3.5870e+02],\n","        [5.5307e+01, 2.5320e+02, 1.1161e+02, 3.5602e+02],\n","        [1.5102e+02, 1.2933e+01, 1.7963e+02, 6.6034e+01],\n","        [1.6684e+02, 1.5518e+02, 2.3029e+02, 1.8310e+02],\n","        [9.9905e+01, 1.0082e+01, 2.1677e+02, 6.7917e+01],\n","        [3.3472e+02, 7.1759e+01, 3.6000e+02, 9.7444e+01],\n","        [2.1720e+01, 2.6586e-01, 1.2980e+02, 2.7462e+01],\n","        [0.0000e+00, 2.5989e+02, 3.9027e+01, 3.3459e+02],\n","        [3.3959e+02, 3.3443e+01, 3.6000e+02, 1.3649e+02],\n","        [0.0000e+00, 2.6429e+02, 7.7761e+01, 3.3064e+02],\n","        [2.5835e+02, 3.2254e+02, 3.1096e+02, 3.4792e+02],\n","        [2.8381e+02, 2.6606e+02, 3.5944e+02, 3.3284e+02],\n","        [2.2742e+02, 3.6040e+01, 2.8384e+02, 1.4128e+02],\n","        [5.8946e+00, 2.9645e+01, 6.3578e+01, 5.7347e+01],\n","        [1.3064e+02, 2.8275e+02, 1.8627e+02, 3.0973e+02],\n","        [1.3758e+02, 1.0265e+00, 1.9334e+02, 5.7005e+01],\n","        [1.0854e+02, 2.6934e+02, 1.4043e+02, 3.1767e+02],\n","        [2.1829e+02, 2.8542e+02, 2.6895e+02, 3.1041e+02]], device='cuda:0',\n","       grad_fn=<StackBackward0>), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1], device='cuda:0'), 'scores': tensor([0.5574, 0.5542, 0.5518, 0.5511, 0.5506, 0.5491, 0.5476, 0.5462, 0.5459,\n","        0.5458, 0.5430, 0.5427, 0.5425, 0.5425, 0.5419, 0.5404, 0.5403, 0.5400,\n","        0.5393, 0.5381, 0.5378, 0.5378, 0.5377, 0.5372, 0.5366, 0.5362, 0.5361,\n","        0.5349, 0.5341, 0.5333, 0.5331, 0.5325, 0.5319, 0.5315, 0.5313, 0.5309,\n","        0.5297, 0.5288, 0.5286, 0.5285, 0.5284, 0.5281, 0.5276, 0.5265, 0.5262,\n","        0.5258, 0.5256, 0.5255, 0.5251, 0.5238, 0.5235, 0.5235, 0.5228, 0.5226,\n","        0.5225, 0.5223, 0.5223, 0.5223, 0.5219, 0.5215, 0.5212, 0.5210, 0.5210,\n","        0.5206, 0.5204, 0.5197, 0.5195, 0.5195, 0.5189, 0.5182, 0.5182, 0.5177,\n","        0.5177, 0.5171, 0.5170, 0.5170, 0.5168, 0.5167, 0.5167, 0.5167, 0.5164,\n","        0.5161, 0.5161, 0.5155, 0.5153, 0.5150, 0.5150, 0.5149, 0.5146, 0.5145,\n","        0.5143, 0.5142, 0.5141, 0.5141, 0.5138, 0.5135, 0.5134, 0.5134, 0.5134,\n","        0.5128], device='cuda:0', grad_fn=<IndexBackward0>)}, {'boxes': tensor([[7.0057e+01, 2.4019e+01, 1.1168e+02, 6.9246e+01],\n","        [7.0091e+01, 5.4347e+01, 1.0072e+02, 1.1041e+02],\n","        [2.5080e+01, 5.5394e-01, 1.4059e+02, 2.5262e+01],\n","        [6.4428e+01, 4.0670e+00, 1.2304e+02, 9.7408e+01],\n","        [9.6081e+01, 3.2979e+01, 1.4323e+02, 6.2090e+01],\n","        [7.3069e+01, 7.5485e+00, 1.0128e+02, 6.2961e+01],\n","        [0.0000e+00, 1.2301e+00, 1.3557e+02, 5.3081e+01],\n","        [4.8391e+01, 3.4613e-01, 1.0564e+02, 1.5596e+01],\n","        [5.2904e+01, 1.3904e+00, 1.1506e+02, 5.4372e+01],\n","        [4.8193e+01, 3.2476e+01, 9.8899e+01, 5.9686e+01],\n","        [6.7177e+01, 6.0642e-01, 1.0615e+02, 2.3521e+01],\n","        [1.1166e+02, 2.3461e+01, 1.5417e+02, 6.7865e+01],\n","        [1.2003e+02, 1.7367e+00, 2.8596e+02, 8.1966e+01],\n","        [5.5047e+00, 3.3792e+02, 6.3140e+01, 3.6509e+02],\n","        [6.8857e+01, 3.9362e+01, 1.1943e+02, 1.4915e+02],\n","        [5.9864e+01, 3.5992e+00, 3.7396e+02, 9.3506e+01],\n","        [0.0000e+00, 4.0194e+00, 1.0912e+02, 1.3515e+02],\n","        [0.0000e+00, 3.3823e+02, 2.1361e+01, 3.6469e+02],\n","        [8.9929e+01, 3.5277e-01, 1.4712e+02, 1.8091e+01],\n","        [8.1409e+01, 3.1347e+02, 1.6716e+02, 3.7600e+02],\n","        [9.2810e+01, 1.5607e+00, 1.5353e+02, 5.7982e+01],\n","        [9.2292e+01, 3.3668e+02, 1.4772e+02, 3.6585e+02],\n","        [2.2624e+01, 7.3885e+00, 1.4326e+02, 2.0250e+02],\n","        [1.1618e+02, 4.5093e-01, 1.4564e+02, 2.3772e+01],\n","        [1.4143e+01, 5.4113e+01, 1.4903e+02, 1.1436e+02],\n","        [1.1412e+02, 3.2860e+02, 1.4566e+02, 3.7600e+02],\n","        [3.0556e+01, 4.1024e-01, 5.8951e+01, 2.5675e+01],\n","        [1.6418e+02, 2.9995e+02, 4.1860e+02, 3.7600e+02],\n","        [1.0526e+02, 3.2943e+00, 1.6192e+02, 8.9822e+01],\n","        [2.2274e+02, 8.9927e+00, 4.5280e+02, 3.7600e+02],\n","        [3.0498e+02, 3.3716e+02, 3.6174e+02, 3.6570e+02],\n","        [7.0192e+01, 3.3481e+02, 1.0214e+02, 3.7600e+02],\n","        [4.3551e+02, 3.3752e+02, 4.9139e+02, 3.6563e+02],\n","        [3.7031e+02, 3.2450e+02, 3.9940e+02, 3.7600e+02],\n","        [3.3518e+01, 1.0146e+00, 3.0052e+02, 4.8002e+01],\n","        [4.3563e+02, 7.5145e+01, 4.9818e+02, 1.0199e+02],\n","        [1.2155e+02, 1.2440e+00, 2.1834e+02, 4.2764e+01],\n","        [1.3436e+00, 4.0531e+01, 8.7313e+01, 1.2149e+02],\n","        [2.1444e+01, 2.5380e+00, 7.6246e+01, 9.6617e+01],\n","        [3.9048e+02, 3.3589e+02, 4.4893e+02, 3.6485e+02],\n","        [5.0872e+01, 7.5786e+01, 1.1227e+02, 1.0133e+02],\n","        [3.4194e+02, 3.1669e+02, 4.2306e+02, 3.7600e+02],\n","        [0.0000e+00, 3.1699e+02, 3.7488e+01, 3.7600e+02],\n","        [5.5606e+01, 3.0775e+02, 1.1793e+02, 3.7600e+02],\n","        [3.8078e+02, 3.1569e+02, 4.6474e+02, 3.7600e+02],\n","        [0.0000e+00, 2.6500e+00, 5.7705e+01, 1.2240e+02],\n","        [2.3801e+01, 3.1797e+01, 7.8239e+01, 1.4853e+02],\n","        [1.0290e+02, 3.7620e+00, 2.2931e+02, 1.0259e+02],\n","        [1.6889e+01, 7.0244e-01, 7.6137e+01, 4.8236e+01],\n","        [1.6172e+02, 1.3074e+01, 2.4854e+02, 8.4867e+01],\n","        [1.3364e+02, 4.8886e-01, 1.9529e+02, 1.7844e+01],\n","        [1.7712e+02, 3.0357e+01, 2.3522e+02, 5.9288e+01],\n","        [2.7554e+02, 4.0075e+01, 5.0800e+02, 1.4818e+02],\n","        [0.0000e+00, 2.8781e-01, 9.1592e+01, 2.5040e+01],\n","        [4.5459e+02, 1.0189e+02, 4.8482e+02, 1.5790e+02],\n","        [1.0210e+01, 1.5576e-01, 6.7912e+01, 1.2762e+01],\n","        [0.0000e+00, 1.7815e+02, 3.3088e+01, 2.5134e+02],\n","        [1.3619e+02, 2.2263e+00, 1.9899e+02, 6.3635e+01],\n","        [4.4709e+02, 6.5673e+01, 4.8822e+02, 1.0717e+02],\n","        [2.7920e+02, 3.3196e+02, 3.2672e+02, 3.7202e+02],\n","        [6.8287e+01, 4.8876e-01, 1.7678e+02, 1.9836e+01],\n","        [2.9882e+01, 7.3464e+00, 5.9677e+01, 6.4144e+01],\n","        [4.9140e+01, 3.3532e+02, 1.0425e+02, 3.6426e+02],\n","        [0.0000e+00, 2.7745e+02, 1.0301e+02, 3.7600e+02],\n","        [4.2660e+02, 4.9789e+01, 5.0774e+02, 1.2516e+02],\n","        [2.8529e+01, 5.0822e+01, 5.7453e+01, 1.0935e+02],\n","        [3.4707e+02, 3.3693e+02, 4.0538e+02, 3.6649e+02],\n","        [4.2398e+02, 9.6002e+01, 5.0723e+02, 1.7073e+02],\n","        [2.4873e+02, 3.1903e+02, 3.3295e+02, 3.7600e+02],\n","        [3.2797e+02, 3.2432e+02, 3.5854e+02, 3.7522e+02],\n","        [2.9762e+02, 3.2105e+02, 3.7812e+02, 3.7600e+02],\n","        [3.7615e+02, 9.5889e+01, 4.6181e+02, 1.7154e+02],\n","        [4.3611e+02, 1.1985e+02, 4.9839e+02, 1.4623e+02],\n","        [4.1281e+02, 9.9855e+01, 4.4116e+02, 1.5492e+02],\n","        [3.1174e+02, 7.3884e+01, 5.0800e+02, 1.8025e+02],\n","        [4.5620e+02, 9.5321e-02, 4.8493e+02, 2.5830e+01],\n","        [9.9139e+01, 3.9488e+00, 2.6731e+02, 1.5507e+02],\n","        [0.0000e+00, 1.8856e+02, 1.6163e+01, 2.4107e+02],\n","        [4.1363e+02, 3.2306e+02, 4.4261e+02, 3.7600e+02],\n","        [0.0000e+00, 3.1332e+02, 8.6323e+01, 3.7130e+02],\n","        [8.2279e+01, 3.9568e+01, 1.6788e+02, 1.1855e+02],\n","        [0.0000e+00, 1.4561e-01, 2.6650e+01, 1.4560e+01],\n","        [0.0000e+00, 2.7553e-01, 1.4841e+01, 2.7550e+01],\n","        [1.7165e+02, 3.3777e+02, 2.3428e+02, 3.6656e+02],\n","        [4.5534e+02, 2.3099e+02, 4.8510e+02, 2.8448e+02],\n","        [0.0000e+00, 8.9275e+00, 5.0423e+01, 6.8486e+01],\n","        [0.0000e+00, 4.6544e-01, 4.0587e+01, 4.3056e+01],\n","        [1.5726e+02, 6.0643e-01, 1.8708e+02, 2.1461e+01],\n","        [3.7188e+02, 2.7788e+02, 4.0161e+02, 3.3054e+02],\n","        [4.2314e+02, 3.1773e+02, 5.0467e+02, 3.7600e+02],\n","        [1.3780e+02, 3.2610e+01, 1.8997e+02, 6.1408e+01],\n","        [1.5065e+02, 3.3213e+02, 1.9219e+02, 3.7106e+02],\n","        [1.2076e+02, 3.1195e+02, 2.0976e+02, 3.7600e+02],\n","        [2.5891e+02, 3.3704e+02, 3.2098e+02, 3.6400e+02],\n","        [4.5608e+02, 3.2413e+02, 4.8563e+02, 3.7600e+02],\n","        [4.3298e+02, 2.0683e+02, 4.9581e+02, 2.3444e+02],\n","        [3.9328e+02, 7.5358e+01, 4.5198e+02, 1.0190e+02],\n","        [1.0028e+01, 7.4768e+01, 7.1189e+01, 1.0114e+02],\n","        [8.3744e+00, 2.9563e+01, 6.4714e+01, 5.7501e+01],\n","        [1.5226e+02, 2.1752e+01, 1.9944e+02, 6.2983e+01]], device='cuda:0',\n","       grad_fn=<StackBackward0>), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1], device='cuda:0'), 'scores': tensor([0.5719, 0.5714, 0.5692, 0.5690, 0.5638, 0.5606, 0.5579, 0.5570, 0.5560,\n","        0.5537, 0.5519, 0.5508, 0.5499, 0.5483, 0.5455, 0.5448, 0.5428, 0.5426,\n","        0.5414, 0.5408, 0.5394, 0.5382, 0.5361, 0.5360, 0.5339, 0.5333, 0.5333,\n","        0.5327, 0.5325, 0.5324, 0.5323, 0.5314, 0.5298, 0.5285, 0.5281, 0.5278,\n","        0.5273, 0.5273, 0.5271, 0.5266, 0.5261, 0.5258, 0.5257, 0.5251, 0.5244,\n","        0.5243, 0.5240, 0.5236, 0.5230, 0.5230, 0.5228, 0.5227, 0.5227, 0.5223,\n","        0.5215, 0.5213, 0.5208, 0.5204, 0.5203, 0.5199, 0.5198, 0.5195, 0.5194,\n","        0.5183, 0.5182, 0.5179, 0.5177, 0.5172, 0.5169, 0.5169, 0.5165, 0.5162,\n","        0.5161, 0.5156, 0.5156, 0.5153, 0.5149, 0.5146, 0.5139, 0.5138, 0.5136,\n","        0.5134, 0.5134, 0.5128, 0.5125, 0.5123, 0.5123, 0.5120, 0.5117, 0.5111,\n","        0.5108, 0.5106, 0.5106, 0.5103, 0.5101, 0.5097, 0.5092, 0.5090, 0.5084,\n","        0.5082], device='cuda:0', grad_fn=<IndexBackward0>)}]\n"]}],"source":["input_image1 = image1.clone()\n","\n","input_image2 = image2.clone()\n","\n","# input is image list\n","inputs = [input_image1.to(device), input_image2.to(device)]\n","\n","ft_model.eval()\n","output = ft_model(inputs)\n","\n","print(output)"]},{"cell_type":"markdown","metadata":{"id":"TU1Va_EsE9zB"},"source":["## <font style=\"color:green\">2.2. Model Training</font>"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EAXi7c95E9zC","outputId":"74d737e6-3607-49ec-c92b-38dc49e50317","executionInfo":{"status":"ok","timestamp":1724151583749,"user_tz":-120,"elapsed":503,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'loss_classifier': tensor(0.6702, device='cuda:0', grad_fn=<NllLossBackward0>),\n"," 'loss_box_reg': tensor(0.0670, device='cuda:0', grad_fn=<DivBackward0>),\n"," 'loss_objectness': tensor(0.6757, device='cuda:0',\n","        grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n"," 'loss_rpn_box_reg': tensor(0.0073, device='cuda:0', grad_fn=<DivBackward0>)}"]},"metadata":{},"execution_count":29}],"source":["input_image1 = image1.clone()\n","\n","target1 = {\n","    'boxes': bboxes1.clone().to(device),\n","    'labels' : labels1.clone().to(device)\n","\n","}\n","\n","input_image2 = image2.clone()\n","\n","\n","target2 = {\n","    'boxes': bboxes2.clone().to(device),\n","    'labels' : labels2.clone().to(device)\n","\n","}\n","\n","inputs = [input_image1.to(device), input_image2.to(device)]\n","targets = [target1, target2]\n","\n","# change to train mode\n","ft_model.train()\n","ft_model(inputs, targets)"]},{"cell_type":"code","metadata":{"id":"4_qGiCNGE9zF"},"source":["**In the next section, we will see the training and validation of Faster RCNN model.**"],"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"}},"nbformat":4,"nbformat_minor":0}