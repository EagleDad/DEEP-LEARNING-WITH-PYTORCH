{"cells":[{"cell_type":"markdown","metadata":{"id":"MjBeTMLJQJuQ"},"source":["# <font style=\"color:blue\">Object Detection with Region Based Convolutional Neural Network.</font>"]},{"cell_type":"markdown","metadata":{"id":"-uYOhP5GR2L0"},"source":["In this notebook we will discuss in brief about the two stage object detector which combines region proposals with CNNs - **R-CNN**.\n","\n","It has been the state-of-the-art in Object Detection for a long time."]},{"cell_type":"markdown","metadata":{"id":"5SOkRRaEl40c"},"source":["# <font style=\"color:blue\">Understanding Region Proposal in R-CNN</font>\n","R-CNN methods use the Selective search approach of finding the region proposals instead of exhaustive search. Let us have a look at the evolution of the R-CNN methods.\n","\n","### <font style=\"color:green\">Sliding Window Approach</font>\n","  Sliding window is one of the oldest approaches in object detection where the input image is split into multiple crops and each crop of the image is classified and if the crop contains a class, then the crop is decided as the bounding box. But this approach is never used in practice as each input image may have 1000s of such crops and each crop passing through the network for classification may take time.\n","  \n","### <font style=\"color:green\">Region Proposal (RCNN)</font>\n","  Image processing techniques are used to make list of proposed regions in the input image which are then sent through the network for classification. But this is computationally more efficient than sliding window approach as only fewer potential crops which may contain the object are classified by the network.\n","  \n","  ![](https://cdn-images-1.medium.com/max/800/1*REPHY47zAyzgbNKC6zlvBQ.png)\n","  Image Source :  [Ross Girshick et al](https://arxiv.org/pdf/1311.2524.pdf)\n","  \n","  RCNN is better than sliding window, but it is still computationally expensive as the network has to classify all the region proposals. It takes around 30-40s for inference of a single image.\n","  \n","  \n","### <font style=\"color:green\">Fast Region Proposal (Fast RCNN)</font>\n","  In Fast RCNN, rather than getting region proposals and classifying each region proposal, the input image is sent into the CNN network which gives a feature map of the image. Again some region proposals are used but now we get the region proposals from the feature map of the image and these feature maps are classified. This reduces the computation as some of the CNN layers are common for the whole image.\n","  \n","  ![](https://cdn-images-1.medium.com/max/800/1*0pMP3aY8blSpva5tvWbnKA.png)\n","  Image Source : [Ross Girshick](https://arxiv.org/pdf/1504.08083.pdf)\n","  \n","\n","### <font style=\"color:green\">Faster R-CNN</font>\n","  The idea of Faster R-CNN is to use CNNs to propose potential region of interest and the network is called Region Proposal Network. After getting the region proposals , it is just like Fast RCNN - we use every region for classification.\n","![](https://www.researchgate.net/profile/Giang_Son_Tran/publication/324549019/figure/fig1/AS:649929152266241@1531966593689/Faster-R-CNN-Architecture-9.png)"]},{"cell_type":"markdown","metadata":{"id":"dNWsNDZQ8HF3"},"source":["# <font style=\"color:blue\">Object Detection with Faster-RCNN</font>\n","The model comes built-in with the Torchvision library. We can simply load the model using `torchvision.models`. We also want to use the pre-trained weights so that we can check the detection with our own images. (We will see how to train a custom Object Detector using your own data in the next section.)\n","\n","### <font style=\"color:green\">Input </font>\n","The pretrained Faster-RCNN ResNet-50 model we are going to use expects the input image tensor to be in the form ```[n, c, h, w]```\n","where\n","- n is the number of images\n","- c is the number of channels , for RGB images it is 3\n","- h is the height of the image\n","- w is the width of the image\n","\n","### <font style=\"color:green\">Output </font>\n","The model will return\n","- Bounding boxes [x0, y0, x1, y1]  are all predicted classes of shape (N,4) where N is the number of classes predicted by the model to be present in the image.\n","- Labels of all predicted classes.\n","- Scores of each predicted label.\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qWOd_NvOYY-P","executionInfo":{"status":"ok","timestamp":1724143530333,"user_tz":-120,"elapsed":9433,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}},"outputId":"1704cb56-cce3-4bf2-c230-d1b05a1b0bc6"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n","100%|██████████| 160M/160M [00:02<00:00, 56.8MB/s]\n"]}],"source":["# import necessary libraries\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision.transforms as T\n","import torchvision\n","import torch\n","import numpy as np\n","import cv2\n","\n","# get the pretrained model from torchvision.models\n","# Note: pretrained=True will get the pretrained weights for the model.\n","# model.eval() to use the model for inference\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n","model.eval()\n","\n","# Class labels from official PyTorch documentation for the pretrained model\n","# Note that there are some N/A's\n","# for complete list check https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/\n","# we will use the same list for this notebookpretrained=True\n","COCO_INSTANCE_CATEGORY_NAMES = [\n","    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n","    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n","    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n","    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n","    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n","    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n","    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n","    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n","    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n","    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n","    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n","    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n","]"]},{"cell_type":"markdown","metadata":{"id":"4xneod6q8me8"},"source":["# <font style=\"color:blue\">Detect Objects</font>\n","We use the function below to load the image and pass it through the model. The output prediction consists of\n","- the predicted bounding boxes\n","- class to which it belongs\n","- confidence of prediction"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"iU50DIZb8me8","executionInfo":{"status":"ok","timestamp":1724143540860,"user_tz":-120,"elapsed":673,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[],"source":["def get_prediction(img_path, threshold):\n","    \"\"\"\n","    get_prediction\n","    parameters:\n","      - img_path - path of the input image\n","      - threshold - threshold value for prediction score\n","    method:\n","      - Image is obtained from the image path\n","      - the image is converted to image tensor using PyTorch's Transforms\n","      - image is passed through the model to get the predictions\n","      - class, box coordinates are obtained, but only prediction score > threshold\n","        are chosen.\n","\n","    \"\"\"\n","    img = Image.open(img_path)\n","    transform = T.Compose([T.ToTensor()])\n","    img = transform(img)\n","    pred = model([img])\n","    pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0]['labels'].numpy())]\n","    pred_boxes = [[(int(i[0]), int(i[1])), (int(i[2]), int(i[3]))] for i in list(pred[0]['boxes'].detach().numpy())]\n","    pred_score = list(pred[0]['scores'].detach().numpy())\n","    pred_t = [pred_score.index(x) for x in pred_score if x>threshold][-1]\n","    pred_boxes = pred_boxes[:pred_t+1]\n","    pred_class = pred_class[:pred_t+1]\n","    return pred_boxes, pred_class"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"rPWomM-t8me9","executionInfo":{"status":"ok","timestamp":1724143544461,"user_tz":-120,"elapsed":476,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[],"source":["def detect_and_display(img_path, threshold=0.5, rect_th=3, text_size=3, text_th=3):\n","    \"\"\"\n","    detect_and_display\n","    parameters:\n","      - img_path - path of the input image\n","      - threshold - threshold value for prediction score\n","      - rect_th - thickness of bounding box\n","      - text_size - size of the class label text\n","      - text_th - thickness of the text\n","    method:\n","      - prediction is obtained from get_prediction method\n","      - for each prediction, bounding box is drawn and text is written\n","        with opencv\n","      - the final image is displayed\n","    \"\"\"\n","    boxes, pred_cls = get_prediction(img_path, threshold)\n","    img = cv2.imread(img_path)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    for i in range(len(boxes)):\n","        cv2.rectangle(img, boxes[i][0], boxes[i][1],color=(0, 255, 0), thickness=rect_th)\n","        cv2.putText(img,pred_cls[i], boxes[i][0], cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th)\n","    plt.figure(figsize=(20,30))\n","    plt.imshow(img)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"QlvKgMDq8mfH"},"source":["# <font style=\"color:blue\">Examples </font>\n","Let us have a look at some sample predictions. You should try it out on your own images."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1WoSRBhMfJgvwVTv7V5YIQei4_8Q_VJlb"},"id":"R8Q4sr3sYZD3","outputId":"731ae04c-c1e1-4977-bd42-ace1240a2ac9","executionInfo":{"status":"ok","timestamp":1724143572183,"user_tz":-120,"elapsed":24256,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# download an image for inference\n","!wget https://www.wsha.org/wp-content/uploads/banner-diverse-group-of-people-2.jpg -O people.jpg\n","\n","# use the api pipeline for object detection\n","# the threshold is set manually, the model sometimes predict\n","# random structures as some object, so we set a threshold to filter\n","# better prediction scores.\n","detect_and_display('./people.jpg', threshold=0.8)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1YTI38TIeqgf2rIkUT8zOZA7Y_rcaARsR"},"id":"Qwlq1NJKS4dS","outputId":"32b9aab9-cff1-4d96-8c55-0fbf602a6f25","executionInfo":{"status":"ok","timestamp":1724143604943,"user_tz":-120,"elapsed":27890,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["!wget https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/10best-cars-group-cropped-1542126037.jpg -O cars.jpg\n","\n","detect_and_display('./cars.jpg', rect_th=6, text_th=5, text_size=5)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1xxV7fHbep6zMbM3lG7l_og6DQGs_XCBx"},"id":"2W86i2oxWyH2","outputId":"39e937fc-ad6b-4f46-cecd-660340df8096","executionInfo":{"status":"ok","timestamp":1724143639329,"user_tz":-120,"elapsed":25842,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["!wget https://cdn.pixabay.com/photo/2013/07/05/01/08/traffic-143391_960_720.jpg -O traffic_scene.jpg\n","\n","detect_and_display('./traffic_scene.jpg', rect_th=2, text_th=1, text_size=1)"]},{"cell_type":"markdown","metadata":{"id":"8st2TuBIQkSJ"},"source":["# <font style=\"color:blue\">Comparing the inference time of model in CPU & GPU</font>\n","Let us see how fast the object detection algorithm works. We will do a comparison of the time it takes on GPU vs the CPU"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"9OYNHMIih4up","executionInfo":{"status":"ok","timestamp":1724143669976,"user_tz":-120,"elapsed":492,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[],"source":["import time\n","\n","def check_inference_time(image_path, gpu=False):\n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n","    model.eval()\n","    img = Image.open(image_path)\n","    transform = T.Compose([T.ToTensor()])\n","    img = transform(img)\n","    if gpu:\n","        model.cuda()\n","        img = img.cuda()\n","    else:\n","        model.cpu()\n","        img = img.cpu()\n","    start_time = time.time()\n","    pred = model([img])\n","    end_time = time.time()\n","    return end_time-start_time"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ptbJ57cUSfut","outputId":"7494031c-cd91-4d07-ee01-e95b73be2b72","scrolled":true,"executionInfo":{"status":"ok","timestamp":1724143742191,"user_tz":-120,"elapsed":68933,"user":{"displayName":"Dirk Adler","userId":"10888330956412535296"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Average Time taken by the model with GPU = 0.3001190423965454s\n","Average Timen take by the model with CPU = 5.123904013633728s\n"]}],"source":["testCount=10\n","cpu_time = sum([check_inference_time('./traffic_scene.jpg', gpu=False) for _ in range(testCount)])/testCount\n","gpu_time = sum([check_inference_time('./traffic_scene.jpg', gpu=True) for _ in range(testCount)])/testCount\n","\n","\n","print('\\n\\nAverage Time taken by the model with GPU = {}s\\nAverage Timen take by the model with CPU = {}s'.format(gpu_time, cpu_time))"]},{"cell_type":"markdown","metadata":{"id":"jEoxhiTO8mfK"},"source":["In the next section, we will learn how to train a custom Object Detector on your own data."]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}