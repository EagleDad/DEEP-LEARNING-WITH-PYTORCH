{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V2cvA6soQQFV"
   },
   "source": [
    "# MLP with Single Hidden Layer using PyTorch\n",
    "\n",
    "1. Define an MLP with variable number of inputs (num_inputs), outputs (num_outputs), and nodes in hidden layer (num_hidden_layer_nodes).  \n",
    "2. Use ReLU activation for each node \n",
    "3. Use MSE loss\n",
    "4. Use SGD optimizer\n",
    "\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/01/mlp.png\" alt=\"mlp\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PPyTXeGJQcuY",
    "outputId": "81d37dac-ed1a-4cd6-c51f-0be62d487340"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10581.46484375\n",
      "1 9755.71875\n",
      "2 9161.2998046875\n",
      "3 8637.4951171875\n",
      "4 8135.08154296875\n",
      "5 7634.916015625\n",
      "6 7127.32080078125\n",
      "7 6610.970703125\n",
      "8 6087.37255859375\n",
      "9 5563.396484375\n",
      "10 5048.9482421875\n",
      "11 4551.427734375\n",
      "12 4078.5126953125\n",
      "13 3634.744873046875\n",
      "14 3225.676025390625\n",
      "15 2852.9521484375\n",
      "16 2516.31201171875\n",
      "17 2214.470947265625\n",
      "18 1946.52880859375\n",
      "19 1709.119873046875\n",
      "20 1499.035400390625\n",
      "21 1313.63623046875\n",
      "22 1151.3740234375\n",
      "23 1009.0540771484375\n",
      "24 883.8927001953125\n",
      "25 774.8145141601562\n",
      "26 679.2298583984375\n",
      "27 596.13232421875\n",
      "28 524.094482421875\n",
      "29 462.2840270996094\n",
      "30 411.1573791503906\n",
      "31 372.46551513671875\n",
      "32 353.15338134765625\n",
      "33 370.2618408203125\n",
      "34 465.53564453125\n",
      "35 733.2091064453125\n",
      "36 1382.131591796875\n",
      "37 2739.41796875\n",
      "38 5025.63037109375\n",
      "39 6997.83056640625\n",
      "40 6102.90185546875\n",
      "41 2681.674560546875\n",
      "42 893.9385375976562\n",
      "43 422.04876708984375\n",
      "44 282.59185791015625\n",
      "45 214.13916015625\n",
      "46 170.45101928710938\n",
      "47 139.27392578125\n",
      "48 115.84429931640625\n",
      "49 97.64486694335938\n",
      "50 83.17706298828125\n",
      "51 71.48747253417969\n",
      "52 61.88473892211914\n",
      "53 53.90485382080078\n",
      "54 47.20309829711914\n",
      "55 41.52906799316406\n",
      "56 36.68505859375\n",
      "57 32.50862121582031\n",
      "58 28.904644012451172\n",
      "59 25.77912139892578\n",
      "60 23.050823211669922\n",
      "61 20.654983520507812\n",
      "62 18.55205535888672\n",
      "63 16.697175979614258\n",
      "64 15.058679580688477\n",
      "65 13.605284690856934\n",
      "66 12.309417724609375\n",
      "67 11.15357780456543\n",
      "68 10.123547554016113\n",
      "69 9.203814506530762\n",
      "70 8.37911605834961\n",
      "71 7.638558864593506\n",
      "72 6.973365306854248\n",
      "73 6.375555515289307\n",
      "74 5.834893226623535\n",
      "75 5.346087455749512\n",
      "76 4.903974533081055\n",
      "77 4.503565311431885\n",
      "78 4.140599250793457\n",
      "79 3.8096649646759033\n",
      "80 3.5095105171203613\n",
      "81 3.236736536026001\n",
      "82 2.988213062286377\n",
      "83 2.7608530521392822\n",
      "84 2.552963972091675\n",
      "85 2.362893581390381\n",
      "86 2.1883134841918945\n",
      "87 2.028787851333618\n",
      "88 1.8825016021728516\n",
      "89 1.748182773590088\n",
      "90 1.6247444152832031\n",
      "91 1.511286735534668\n",
      "92 1.4065481424331665\n",
      "93 1.3101508617401123\n",
      "94 1.221179723739624\n",
      "95 1.139177918434143\n",
      "96 1.0634140968322754\n",
      "97 0.993725597858429\n",
      "98 0.9285997152328491\n",
      "99 0.8685770630836487\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Get reproducible results\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define the model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hidden_layer_nodes, num_outputs):\n",
    "        # Initialize super class\n",
    "        super().__init__()\n",
    "\n",
    "        # Add hidden layer \n",
    "        self.linear1 = nn.Linear(num_inputs, num_hidden_layer_nodes)\n",
    "\n",
    "        # Add output layer\n",
    "        self.linear2 = nn.Linear(num_hidden_layer_nodes, num_outputs)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through hidden layer with \n",
    "        x = F.relu(self.linear1(x))\n",
    "        \n",
    "        # Foward pass to output layer\n",
    "        return self.linear2(x)\n",
    "\n",
    "# Num data points\n",
    "num_data = 1000\n",
    "\n",
    "# Network parameters\n",
    "num_inputs = 1000\n",
    "num_hidden_layer_nodes = 100\n",
    "num_outputs = 10\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 100 \n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(num_data, num_inputs)\n",
    "y = torch.randn(num_data, num_outputs)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = MLP(num_inputs, num_hidden_layer_nodes, num_outputs)\n",
    "\n",
    "# Define loss function\n",
    "loss_function = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "for t in range(num_epochs):\n",
    "\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = loss_function(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Calculate gradient using backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update model parameters (weights)\n",
    "    optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WyjPulcDSJQo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "MLP-without-sequential",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
