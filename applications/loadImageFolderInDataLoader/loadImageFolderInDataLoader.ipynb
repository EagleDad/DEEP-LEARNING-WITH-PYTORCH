{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <font style=\"color:blue\">Load Image Folder in DataLoader<\/font>\n",
                "\n",
                "Till now, we have experimented with datasets available with the PyTorch Torchvision library.\\\n",
                "\n",
                "In this notebook, we will see how to load images from a folder.\n",
                "\n",
                "Data available in torchvision-dataset are preprocessed images. But in the real world, we have to do it on our own. \n",
                "\n",
                "To illustrate a few preprocessing, we have chosen the [10 Monkey Species](https:\/\/www.kaggle.com\/slothkong\/10-monkey-species) dataset from Kaggle. You can download data from [here](https:\/\/www.kaggle.com\/slothkong\/10-monkey-species\/download). You need to extract data. We have already uploaded the extracted data in the lab.\n",
                "\n",
                "**Label mapping:**\n",
                "\n",
                "| Label | Monkey Species |\n",
                "| --- | --- |\n",
                "| n0 | alouatta_palliata |\n",
                "| n1 | erythrocebus_patas |\n",
                "| n2 | cacajao_calvus |\n",
                "| n3 | macaca_fuscata |  \n",
                "| n4 | cebuella_pygmea |\n",
                "| n5 | cebus_capucinus |\n",
                "| n6 | mico_argentatus |\n",
                "| n7 | saimiri_sciureus | \n",
                "| n8 | aotus_nigriceps |\n",
                "| n9 | trachypithecus_johnii |\n",
                "\n",
                "\n",
                "**Extracted Folder structure:**\n",
                "\n",
                "```\n",
                "data_root\n",
                "\u251c\u2500\u2500 training\n",
                "\u2502\u00a0\u00a0 \u2514\u2500\u2500 training\n",
                "\u2502\u00a0\u00a0     \u251c\u2500\u2500 n0\n",
                "\u2502\u00a0\u00a0     \u251c\u2500\u2500 n1\n",
                "\u2502\u00a0\u00a0     \u251c\u2500\u2500 n2\n",
                "\u2502\u00a0\u00a0     \u251c\u2500\u2500 n3\n",
                "\u2502\u00a0\u00a0     \u251c\u2500\u2500 n4\n",
                "\u2502\u00a0\u00a0     \u251c\u2500\u2500 n5\n",
                "\u2502\u00a0\u00a0     \u251c\u2500\u2500 n6\n",
                "\u2502\u00a0\u00a0     \u251c\u2500\u2500 n7\n",
                "\u2502\u00a0\u00a0     \u251c\u2500\u2500 n8\n",
                "\u2502\u00a0\u00a0     \u2514\u2500\u2500 n9\n",
                "\u2514\u2500\u2500 validation\n",
                "    \u2514\u2500\u2500 validation\n",
                "        \u251c\u2500\u2500 n0\n",
                "        \u251c\u2500\u2500 n1\n",
                "        \u251c\u2500\u2500 n2\n",
                "        \u251c\u2500\u2500 n3\n",
                "        \u251c\u2500\u2500 n4\n",
                "        \u251c\u2500\u2500 n5\n",
                "        \u251c\u2500\u2500 n6\n",
                "        \u251c\u2500\u2500 n7\n",
                "        \u251c\u2500\u2500 n8\n",
                "        \u2514\u2500\u2500 n9\n",
                "\n",
                "```\n",
                "\n",
                "`10-monkey-species\/training\/training` has `n0-n10` folder; each folder has images of the corresponding class. Similarly, `10-monkey-species\/validation\/validation` has `n0-n10` folder.\n",
                "\n",
                "\n",
                "**PyTorch has inbuilt functionality (`torchvision.datasets.ImageFolder` class) to load such structured image folder:**\n",
                "\n",
                "```\n",
                "torchvision.datasets.ImageFolder(root, transform=None, target_transform=None, loader=<function default_loader>, is_valid_file=None)\n",
                "```\n",
                "\n",
                "This is a generic data loader where the images are arranged in this way:\n",
                "\n",
                "```\n",
                "root\/n0\/xxx.png\n",
                "root\/n0\/xxy.jpg\n",
                "root\/n0\/xxz.png\n",
                "\n",
                "root\/n1\/123.jpg\n",
                "root\/n1\/nsdf3.png\n",
                "root\/n1\/asd932_.png\n",
                "\n",
                "    :\n",
                "    :\n",
                "    \n",
                "root\/n9\/1b23.jpg\n",
                "root\/n9\/nsasdf3.png\n",
                "root\/n9\/as2wdd932_.png\n",
                "    \n",
                "```\n",
                "\n",
                "Parameters:\n",
                "\n",
                "- `root` (string) \u2013 Root directory path.\n",
                "\n",
                "- `transform` (callable, optional) \u2013 A function\/transform that takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop\n",
                "\n",
                "- `target_transform` (callable, optional) \u2013 A function\/transform that takes in the target and transforms it.\n",
                "\n",
                "- `loader` (callable, optional) \u2013 A function to load an image given its path.\n",
                "\n",
                "- `is_valid_file` \u2013 A function that takes path of an Image file and check if the file is a valid file (used to check of corrupt files)\n",
                "\n",
                "Find more details [here](https:\/\/pytorch.org\/docs\/stable\/torchvision\/datasets.html#torchvision.datasets.ImageFolder)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "import numpy as np\n",
                "import torch\n",
                "\n",
                "from torchvision import datasets, transforms"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <font style=\"color:blue\">1. Explore Training Data<\/font>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:green\">1.1 Load Data<\/font>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# Load training data\n",
                "\n",
                "train_root = \"..\/resource\/lib\/publicdata\/images\/10-monkey-species\/training\/training\"\n",
                "\n",
                "train_data = datasets.ImageFolder(root=train_root)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:green\">1.2 Explore Data<\/font>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <font style=\"color:green\">Get Classes<\/font>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# classes\n",
                "\n",
                "train_data.classes"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <font style=\"color:green\">Class to Index Mapping<\/font>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Class to index mappings\n",
                "\n",
                "train_data.class_to_idx"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <font style=\"color:green\">Data Length<\/font>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# data length\n",
                "\n",
                "len(train_data)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <font style=\"color:green\">Get Image and Target<\/font>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# get image and target\n",
                "\n",
                "img, target = train_data[5]\n",
                "\n",
                "print('PIL image size: {}, target: {}'.format(img.size, target))\n",
                "\n",
                "plt.imshow(img)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Note: In PIL image `size` attribute gives `width x heigh`**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "# get image and target\n",
                "img, target = train_data[500]\n",
                "\n",
                "print('image size: {}, target: {}'.format(img.size, target))\n",
                "\n",
                "plt.imshow(img)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**We can see that different images have different sizes. For training, we need fixed-size images. So we can not use these images as it is for training. We can use `torchvision.transforms.Resize` to resize images.**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <font style=\"color:blue\">2. Dataset with Preprocessing<\/font>\n",
                "\n",
                "**Let's resized training data to `224 x 224`.**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# transform=transforms.Resize(224, 224)\n",
                "train_data = datasets.ImageFolder(root=train_root, transform=transforms.Resize((224, 224)))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "img, target = train_data[5]\n",
                "\n",
                "print('image size: {}, target: {}'.format(img.size, target))\n",
                "\n",
                "plt.imshow(img)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "img, target = train_data[500]\n",
                "\n",
                "print('image size: {}, target: {}'.format(img.size, target))\n",
                "\n",
                "plt.imshow(img)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**We can see that directly resizing to `224 x 224` is distorting the image because aspect ratio (`width:height`) is not taken care of by this way of resizing.**\n",
                "\n",
                "If we give integer instead of a tuple as an argument to `transforms.resize`, it will resize lower pixel value to given integer value, and higher pixel value will be such that it will maintain aspect ratio. \n",
                "\n",
                "**Let's have a look.**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_data = datasets.ImageFolder(root=train_root, transform=transforms.Resize(224))\n",
                "\n",
                "img, target = train_data[5]\n",
                "\n",
                "print('image size: {}, target: {}'.format(img.size, target))\n",
                "\n",
                "plt.imshow(img)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**We can see height has a lower value, which resized to 224.**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <font style=\"color:blue\">3. Dataloader with ImageFolder Dataset<\/font>\n",
                "\n",
                "Suppose we need `224 x 224` sized images. Generally, the image of interest uses to be at the center of the big image. So we can use the following transform:\n",
                "\n",
                "```\n",
                "transforms.Compose([\n",
                "    transforms.Resize(256),\n",
                "    transforms.CenterCrop(224)\n",
                "    ])\n",
                "```\n",
                "\n",
                "Let's write down `get_data` method, which will return the training and validation data loader."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "def get_data(batch_size, data_root, num_workers=4):\n",
                "    \n",
                "    preprocess = transforms.Compose([\n",
                "    transforms.Resize(256),\n",
                "    transforms.CenterCrop(224),\n",
                "    transforms.ToTensor()\n",
                "    ])\n",
                "    \n",
                "    # train dataloader\n",
                "    \n",
                "    train_data_path = os.path.join(data_root, 'training', 'training')\n",
                "    \n",
                "    train_loader = torch.utils.data.DataLoader(\n",
                "        datasets.ImageFolder(root=train_data_path, transform=preprocess),\n",
                "        batch_size=batch_size,\n",
                "        shuffle=True,\n",
                "        num_workers=num_workers\n",
                "    )\n",
                "    \n",
                "    # test dataloader\n",
                "    \n",
                "    test_data_path = os.path.join(data_root, 'validation', 'validation')\n",
                "    \n",
                "    test_loader = torch.utils.data.DataLoader(\n",
                "        datasets.ImageFolder(root=test_data_path, transform=preprocess),\n",
                "        batch_size=batch_size,\n",
                "        shuffle=False,\n",
                "        num_workers=num_workers\n",
                "    )\n",
                "    return train_loader, test_loader"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <font style=\"color:green\">Plot few Images<\/font>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_root = \"..\/resource\/lib\/publicdata\/images\/10-monkey-species\"\n",
                "\n",
                "train_loader, test_loader = get_data(10, data_root)\n",
                "\n",
                "plt.rcParams[\"figure.figsize\"] = (15, 6)\n",
                "plt.figure\n",
                "for images, labels in train_loader:\n",
                "    for i in range(len(labels)):\n",
                "        plt.subplot(2, 5, i+1)\n",
                "        img = transforms.functional.to_pil_image(images[i])\n",
                "        plt.imshow(img)\n",
                "        plt.gca().set_title('Target: {0}'.format(labels[i]))\n",
                "    plt.show()\n",
                "    break\n",
                "    "
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8",
            "language": "python",
            "name": "python38"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text\/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}