{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <font color='blue'>Table of contents</font>\n",
                "\n",
                "- [LeNet5 Architecture](#lenet)\n",
                "- [Display the Network](#display)\n",
                "- [Get the Fashion-MNIST Data](#get-data)\n",
                "- [System Configuration](#sys-config)\n",
                "- [Training Configuration](#train-config)\n",
                "- [System Setup](#sys-setup)\n",
                "- [Training](#training)\n",
                "- [Validation](#validation)\n",
                "- [Main function](#main)\n",
                "- [Plot Loss](#plot-loss)\n",
                "- [Miscellaneous](#misc)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <font style=\"color:blue\">Convolutional Neural Network Using Batch Normalization</font>\n",
                "\n",
                "In this notebook, we  add batch norm layers to the LeNet network, and see how it affects network training and convergence.\n",
                "\n",
                "Instead of the MNIST dataset, which overfits easily, we will use the Fashion MNIST dataset.\n",
                "\n",
                "The figure below shows some samples from the Fashion MNIST dataset.\n",
                "\n",
                "<img src=\"https://www.learnopencv.com/wp-content/uploads/2021/01/c3-w3-fashion-mnist-sprite.jpg\" width=\"600\">\n",
                "\n",
                "There are 10 classes. Each training and testing example is assigned to one of the following labels:\n",
                "\n",
                "| Label | Description |\n",
                "| --- | --- |\n",
                "| 0 | T-shirt/top |\n",
                "| 1 | Trouser |\n",
                "| 2 | Pullover |\n",
                "| 3 | Dress |\n",
                "| 4 | Coat |\n",
                "| 5 | Sandal |\n",
                "| 6 | Shirt |\n",
                "| 7 | Sneaker |\n",
                "| 8 | Bag |\n",
                "| 9 | Ankle boot |\n",
                "\n",
                "\n",
                "\n",
                "We want to classify images in this dataset, using the LeNet network."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt  # one of the best graphics library for python"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import time\n",
                "\n",
                "from typing import Iterable\n",
                "from dataclasses import dataclass\n",
                "\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import torch.nn.functional as F\n",
                "\n",
                "from torchvision import datasets, transforms"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:green\">1. LeNet Architecture with BatchNorm</font><a name=\"lenet\"></a>\n",
                "\n",
                "We have already explained the architecture for LeNet in the previous notebook.\n",
                "\n",
                "Here, we create another model called LeNetBN, adding Batch Normalization layers to the 2 convolution blocks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "class LeNet(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "\n",
                "        # convolution layers\n",
                "        self._body = nn.Sequential(\n",
                "            # First convolution Layer\n",
                "            # input size = (32, 32), output size = (28, 28)\n",
                "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
                "            # ReLU activation\n",
                "            nn.ReLU(inplace=True),\n",
                "            # Max pool 2-d\n",
                "            nn.MaxPool2d(kernel_size=2),\n",
                "            \n",
                "            # Second convolution layer\n",
                "            # input size = (14, 14), output size = (10, 10)\n",
                "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.MaxPool2d(kernel_size=2),\n",
                "            # output size = (5, 5)\n",
                "        )\n",
                "        \n",
                "        # Fully connected layers\n",
                "        self._head = nn.Sequential(\n",
                "            # First fully connected layer\n",
                "            # in_features = total number of weights in last conv layer = 16 * 5 * 5\n",
                "            nn.Linear(in_features=16 * 5 * 5, out_features=120), \n",
                "            \n",
                "            # ReLU activation\n",
                "            nn.ReLU(inplace=True),\n",
                "            \n",
                "            # second fully connected layer\n",
                "            # in_features = output of last linear layer = 120 \n",
                "            nn.Linear(in_features=120, out_features=84), \n",
                "            \n",
                "            # ReLU activation\n",
                "            nn.ReLU(inplace=True),\n",
                "            \n",
                "            # Third fully connected layer which is also output layer\n",
                "            # in_features = output of last linear layer = 84\n",
                "            # and out_features = number of classes = 10 (MNIST data 0-9)\n",
                "            nn.Linear(in_features=84, out_features=10)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        # apply feature extractor\n",
                "        x = self._body(x)\n",
                "        # flatten the output of conv layers\n",
                "        # dimension should be batch_size * number_of weight_in_last conv_layer\n",
                "        x = x.view(x.size()[0], -1)\n",
                "        # apply classification head\n",
                "        x = self._head(x)\n",
                "        return x"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "class LeNetBN(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "\n",
                "        # convolution layers\n",
                "        self._body = nn.Sequential(\n",
                "            # First convolution Layer\n",
                "            # input size = (32, 32), output size = (28, 28)\n",
                "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
                "            nn.BatchNorm2d(6),\n",
                "            # ReLU activation\n",
                "            nn.ReLU(inplace=True),\n",
                "            # Max pool 2-d\n",
                "            nn.MaxPool2d(kernel_size=2),\n",
                "            \n",
                "            # Second convolution layer\n",
                "            # input size = (14, 14), output size = (10, 10)\n",
                "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
                "            nn.BatchNorm2d(16),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.MaxPool2d(kernel_size=2),\n",
                "            # output size = (5, 5)\n",
                "        )\n",
                "        \n",
                "        # Fully connected layers\n",
                "        self._head = nn.Sequential(\n",
                "            # First fully connected layer\n",
                "            # in_features = total number of weight in last conv layer = 16 * 5 * 5\n",
                "            nn.Linear(in_features=16 * 5 * 5, out_features=120), \n",
                "            \n",
                "            # ReLU activation\n",
                "            nn.ReLU(inplace=True),\n",
                "            \n",
                "            # second fully connected layer\n",
                "            # in_features = output of last linear layer = 120 \n",
                "            nn.Linear(in_features=120, out_features=84), \n",
                "            \n",
                "            # ReLU activation\n",
                "            nn.ReLU(inplace=True),\n",
                "            \n",
                "            # Third fully connected layer. It is also output layer\n",
                "            # in_features = output of last linear layer = 84\n",
                "            # and out_features = number of classes = 10 (MNIST data 0-9)\n",
                "            nn.Linear(in_features=84, out_features=10)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        # apply feature extractor\n",
                "        x = self._body(x)\n",
                "        # flatten the output of conv layers\n",
                "        # dimension should be batch_size * number_of weights_in_last conv_layer\n",
                "        x = x.view(x.size()[0], -1)\n",
                "        # apply classification head\n",
                "        x = self._head(x)\n",
                "        return x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:green\">2. Display the Network</font><a name=\"display\"></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "LeNet(\n",
                        "  (_body): Sequential(\n",
                        "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
                        "    (1): ReLU(inplace=True)\n",
                        "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
                        "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
                        "    (4): ReLU(inplace=True)\n",
                        "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
                        "  )\n",
                        "  (_head): Sequential(\n",
                        "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
                        "    (1): ReLU(inplace=True)\n",
                        "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
                        "    (3): ReLU(inplace=True)\n",
                        "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
                        "  )\n",
                        ")\n",
                        "LeNetBN(\n",
                        "  (_body): Sequential(\n",
                        "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
                        "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                        "    (2): ReLU(inplace=True)\n",
                        "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
                        "    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
                        "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
                        "    (6): ReLU(inplace=True)\n",
                        "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
                        "  )\n",
                        "  (_head): Sequential(\n",
                        "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
                        "    (1): ReLU(inplace=True)\n",
                        "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
                        "    (3): ReLU(inplace=True)\n",
                        "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
                        "  )\n",
                        ")\n"
                    ]
                }
            ],
            "source": [
                "lenet_model = LeNet()\n",
                "print(lenet_model)\n",
                "lenetBN_model = LeNetBN()\n",
                "print(lenetBN_model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "lines_to_next_cell": 2
            },
            "source": [
                "## <font style=\"color:green\">3. Get Fashion-MNIST Data</font><a name=\"get-data\"></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "def get_data(batch_size, data_root='data', num_workers=1):\n",
                "    \n",
                "    train_test_transforms = transforms.Compose([\n",
                "        # Resize to 32X32\n",
                "        transforms.Resize((32, 32)),\n",
                "        # this re-scales image tensor values between 0-1. image_tensor /= 255\n",
                "        transforms.ToTensor(),\n",
                "        # subtract mean (0.2860) and divide by variance (0.3530).\n",
                "        # This mean and variance is calculated on training data (verify for yourself)\n",
                "        transforms.Normalize((0.2860, ), (0.3530, ))\n",
                "    ])\n",
                "    \n",
                "    # train dataloader\n",
                "    train_loader = torch.utils.data.DataLoader(\n",
                "        datasets.FashionMNIST(root=data_root, train=True, download=True, transform=train_test_transforms),\n",
                "        batch_size=batch_size,\n",
                "        shuffle=True,\n",
                "        num_workers=num_workers\n",
                "    )\n",
                "    \n",
                "    # test dataloader\n",
                "    test_loader = torch.utils.data.DataLoader(\n",
                "        datasets.FashionMNIST(root=data_root, train=False, download=True, transform=train_test_transforms),\n",
                "        batch_size=batch_size,\n",
                "        shuffle=False,\n",
                "        num_workers=num_workers\n",
                "    )\n",
                "    return train_loader, test_loader"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:green\">4. System Configuration</font><a name=\"sys-config\"></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class SystemConfiguration:\n",
                "    '''\n",
                "    Describes the common system setting needed for reproducible training\n",
                "    '''\n",
                "    seed: int = 42  # seed number to set the state of all random number generators\n",
                "    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\n",
                "    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:green\">5. Training Configuration</font><a name=\"train-config\"></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class TrainingConfiguration:\n",
                "    '''\n",
                "    Describes configuration of the training process\n",
                "    '''\n",
                "    batch_size: int = 32  # amount of data to pass through the network at each forward-backward iteration\n",
                "    epochs_count: int = 20  # number of times the whole dataset will be passed through the network\n",
                "    learning_rate: float = 0.01  # determines the speed of network's weights update\n",
                "    log_interval: int = 100  # how many batches to wait between logging training status\n",
                "    test_interval: int = 1  # how many epochs to wait before another test. Set to 1 to get val loss at each epoch\n",
                "    data_root: str = \"data\"  # folder to save MNIST data (default: data)\n",
                "    num_workers: int = 10  # number of concurrent processes used to prepare data\n",
                "    device: str = 'cuda'  # device to use for training.\n",
                "    \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:green\">6. System Setup</font><a name=\"sys-setup\"></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "def setup_system(system_config: SystemConfiguration) -> None:\n",
                "    torch.manual_seed(system_config.seed)\n",
                "    if torch.cuda.is_available():\n",
                "        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n",
                "        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:green\">7. Training</font><a name=\"training\"></a>\n",
                "We are familiar with the training pipeline used in PyTorch."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "def train(\n",
                "    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n",
                "    train_loader: torch.utils.data.DataLoader, epoch_idx: int\n",
                ") -> None:\n",
                "    \n",
                "    # change model in training mode\n",
                "    model.train()\n",
                "    \n",
                "    # to get batch loss\n",
                "    batch_loss = np.array([])\n",
                "    \n",
                "    # to get batch accuracy\n",
                "    batch_acc = np.array([])\n",
                "        \n",
                "    for batch_idx, (data, target) in enumerate(train_loader):\n",
                "        \n",
                "        # clone target\n",
                "        indx_target = target.clone()\n",
                "        # send data to device (its is medatory if GPU has to be used)\n",
                "        data = data.to(train_config.device)\n",
                "        # send target to device\n",
                "        target = target.to(train_config.device)\n",
                "\n",
                "        # reset parameters gradient to zero\n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        # forward pass to the model\n",
                "        output = model(data)\n",
                "        \n",
                "        # cross entropy loss\n",
                "        loss = F.cross_entropy(output, target)\n",
                "        \n",
                "        # find gradients w.r.t training parameters\n",
                "        loss.backward()\n",
                "        # Update parameters using gradients\n",
                "        optimizer.step()\n",
                "        \n",
                "        batch_loss = np.append(batch_loss, [loss.item()])\n",
                "        \n",
                "        # get probability score using softmax\n",
                "        prob = F.softmax(output, dim=1)\n",
                "            \n",
                "        # get the index of the max probability\n",
                "        pred = prob.data.max(dim=1)[1]  \n",
                "                        \n",
                "        # correct prediction\n",
                "        correct = pred.cpu().eq(indx_target).sum()\n",
                "            \n",
                "        # accuracy\n",
                "        acc = float(correct) / float(len(data))\n",
                "        \n",
                "        batch_acc = np.append(batch_acc, [acc])\n",
                "\n",
                "        if batch_idx % train_config.log_interval == 0 and batch_idx > 0:              \n",
                "            print(\n",
                "                'Train Epoch: {} [{}/{}] Loss: {:.6f} Acc: {:.4f}'.format(\n",
                "                    epoch_idx, batch_idx * len(data), len(train_loader.dataset), loss.item(), acc\n",
                "                )\n",
                "            )\n",
                "            \n",
                "    epoch_loss = batch_loss.mean()\n",
                "    epoch_acc = batch_acc.mean()\n",
                "    return epoch_loss, epoch_acc"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:green\">8. Validation</font><a name=\"validation\"></a>\n",
                "\n",
                "After every few epochs **`validation`** is called, with the `trained model` and `test_loader` to get validation loss and accuracy.\n",
                "\n",
                "**Note:** We use `model.eval()` to enable evaluation mode of the model. This will stop calculating the running estimate of mean and variance of data. Using instead just the mean and variance computed while training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "def validate(\n",
                "    train_config: TrainingConfiguration,\n",
                "    model: nn.Module,\n",
                "    test_loader: torch.utils.data.DataLoader,\n",
                ") -> float:\n",
                "    # \n",
                "    model.eval()\n",
                "    test_loss = 0\n",
                "    count_corect_predictions = 0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for data, target in test_loader:\n",
                "            indx_target = target.clone()\n",
                "            data = data.to(train_config.device)\n",
                "\n",
                "            target = target.to(train_config.device)\n",
                "\n",
                "            output = model(data)\n",
                "            # add loss for each mini batch\n",
                "            test_loss += F.cross_entropy(output, target).item()\n",
                "\n",
                "            # get probability score using softmax\n",
                "            prob = F.softmax(output, dim=1)\n",
                "\n",
                "            # get the index of the max probability\n",
                "            pred = prob.data.max(dim=1)[1] \n",
                "\n",
                "            # add correct prediction count\n",
                "            count_corect_predictions += pred.cpu().eq(indx_target).sum()\n",
                "\n",
                "        # average over number of mini-batches\n",
                "        test_loss = test_loss / len(test_loader)  \n",
                "\n",
                "        # average over number of dataset\n",
                "        accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n",
                "\n",
                "        print(\n",
                "            '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
                "                test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n",
                "            )\n",
                "        )\n",
                "    return test_loss, accuracy/100.0"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:green\">9. Main</font><a name=\"main\"></a>\n",
                "\n",
                "\n",
                "Here, we use the configuration parameters defined above and start  training. \n",
                "\n",
                "1. Set up system parameters like CPU/GPU, number of threads etc.\n",
                "1. Load the data using dataloaders.\n",
                "1. Create an instance of the LeNet model.\n",
                "1. Specify optimizer to use.\n",
                "1. Set up variables to track loss and accuracy and start training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "def main(model, system_configuration=SystemConfiguration(), training_configuration=TrainingConfiguration()):\n",
                "    \n",
                "    # system configuration\n",
                "    setup_system(system_configuration)\n",
                "\n",
                "    # batch size\n",
                "    batch_size_to_set = training_configuration.batch_size\n",
                "    # num_workers\n",
                "    num_workers_to_set = training_configuration.num_workers\n",
                "    # epochs\n",
                "    epoch_num_to_set = training_configuration.epochs_count\n",
                "\n",
                "    # if GPU is available use training config, \n",
                "    # else lower batch_size, num_workers and epochs count\n",
                "    if torch.cuda.is_available():\n",
                "        device = \"cuda\"\n",
                "    else:\n",
                "        device = \"cpu\"\n",
                "        batch_size_to_set = 16\n",
                "        num_workers_to_set = 2\n",
                "        epoch_num_to_set = 10\n",
                "\n",
                "    # data loader\n",
                "    train_loader, test_loader = get_data(\n",
                "        batch_size=batch_size_to_set,\n",
                "        data_root=training_configuration.data_root,\n",
                "        num_workers=num_workers_to_set\n",
                "    )\n",
                "    \n",
                "    # Update training configuration\n",
                "    training_configuration = TrainingConfiguration(\n",
                "        device=device,\n",
                "        epochs_count=epoch_num_to_set,\n",
                "        batch_size=batch_size_to_set,\n",
                "        num_workers=num_workers_to_set\n",
                "    )\n",
                "        \n",
                "    # send model to device (GPU/CPU)\n",
                "    model.to(training_configuration.device)\n",
                "\n",
                "    # optimizer\n",
                "    optimizer = optim.SGD(\n",
                "        model.parameters(),\n",
                "        lr=training_configuration.learning_rate\n",
                "    )\n",
                "\n",
                "    best_loss = torch.tensor(np.inf)\n",
                "    \n",
                "    # epoch train/test loss\n",
                "    epoch_train_loss = np.array([])\n",
                "    epoch_test_loss = np.array([])\n",
                "    \n",
                "    # epch train/test accuracy\n",
                "    epoch_train_acc = np.array([])\n",
                "    epoch_test_acc = np.array([])\n",
                "    \n",
                "    # trainig time measurement\n",
                "    t_begin = time.time()\n",
                "    for epoch in range(training_configuration.epochs_count):\n",
                "        \n",
                "        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch)\n",
                "        \n",
                "        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
                "        \n",
                "        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
                "\n",
                "        elapsed_time = time.time() - t_begin\n",
                "        speed_epoch = elapsed_time / (epoch + 1)\n",
                "        speed_batch = speed_epoch / len(train_loader)\n",
                "        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n",
                "        \n",
                "        print(\n",
                "            \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n",
                "                elapsed_time, speed_epoch, speed_batch, eta\n",
                "            )\n",
                "        )\n",
                "\n",
                "        if epoch % training_configuration.test_interval == 0:\n",
                "            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n",
                "            \n",
                "            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
                "        \n",
                "            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
                "            \n",
                "            if current_loss < best_loss:\n",
                "                best_loss = current_loss\n",
                "                \n",
                "    print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))\n",
                "    \n",
                "    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
                        "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 26421880/26421880 [00:03<00:00, 7172641.80it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
                        "\n",
                        "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
                        "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 29515/29515 [00:00<00:00, 1600616.52it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
                        "\n",
                        "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 4422102/4422102 [00:00<00:00, 6810319.70it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
                        "\n",
                        "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
                        "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 5148/5148 [00:00<?, ?it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
                        "\n",
                        "Train Epoch: 0 [1600/60000] Loss: 2.306480 Acc: 0.0625\n",
                        "Train Epoch: 0 [3200/60000] Loss: 2.268298 Acc: 0.2500\n",
                        "Train Epoch: 0 [4800/60000] Loss: 2.244463 Acc: 0.3125\n",
                        "Train Epoch: 0 [6400/60000] Loss: 2.002825 Acc: 0.3750\n",
                        "Train Epoch: 0 [8000/60000] Loss: 1.223319 Acc: 0.5625\n",
                        "Train Epoch: 0 [9600/60000] Loss: 0.967538 Acc: 0.7500\n",
                        "Train Epoch: 0 [11200/60000] Loss: 1.045186 Acc: 0.6250\n",
                        "Train Epoch: 0 [12800/60000] Loss: 0.999885 Acc: 0.6250\n",
                        "Train Epoch: 0 [14400/60000] Loss: 0.798885 Acc: 0.6875\n",
                        "Train Epoch: 0 [16000/60000] Loss: 0.489598 Acc: 0.8750\n",
                        "Train Epoch: 0 [17600/60000] Loss: 0.804046 Acc: 0.6250\n",
                        "Train Epoch: 0 [19200/60000] Loss: 0.784255 Acc: 0.8125\n",
                        "Train Epoch: 0 [20800/60000] Loss: 0.837688 Acc: 0.7500\n",
                        "Train Epoch: 0 [22400/60000] Loss: 0.626993 Acc: 0.6875\n",
                        "Train Epoch: 0 [24000/60000] Loss: 0.185992 Acc: 1.0000\n",
                        "Train Epoch: 0 [25600/60000] Loss: 0.863186 Acc: 0.6875\n",
                        "Train Epoch: 0 [27200/60000] Loss: 0.857818 Acc: 0.6250\n",
                        "Train Epoch: 0 [28800/60000] Loss: 0.343735 Acc: 1.0000\n",
                        "Train Epoch: 0 [30400/60000] Loss: 0.701753 Acc: 0.7500\n",
                        "Train Epoch: 0 [32000/60000] Loss: 0.667750 Acc: 0.6875\n",
                        "Train Epoch: 0 [33600/60000] Loss: 0.308063 Acc: 1.0000\n",
                        "Train Epoch: 0 [35200/60000] Loss: 0.981657 Acc: 0.5625\n",
                        "Train Epoch: 0 [36800/60000] Loss: 0.632547 Acc: 0.7500\n",
                        "Train Epoch: 0 [38400/60000] Loss: 0.260296 Acc: 0.8750\n",
                        "Train Epoch: 0 [40000/60000] Loss: 0.506135 Acc: 0.7500\n",
                        "Train Epoch: 0 [41600/60000] Loss: 0.388376 Acc: 0.8125\n",
                        "Train Epoch: 0 [43200/60000] Loss: 0.526814 Acc: 0.8125\n",
                        "Train Epoch: 0 [44800/60000] Loss: 0.565610 Acc: 0.8125\n",
                        "Train Epoch: 0 [46400/60000] Loss: 0.500528 Acc: 0.8125\n",
                        "Train Epoch: 0 [48000/60000] Loss: 0.671156 Acc: 0.7500\n",
                        "Train Epoch: 0 [49600/60000] Loss: 0.776893 Acc: 0.6875\n",
                        "Train Epoch: 0 [51200/60000] Loss: 0.273452 Acc: 0.9375\n",
                        "Train Epoch: 0 [52800/60000] Loss: 0.538400 Acc: 0.7500\n",
                        "Train Epoch: 0 [54400/60000] Loss: 0.846124 Acc: 0.6250\n",
                        "Train Epoch: 0 [56000/60000] Loss: 0.252969 Acc: 0.9375\n",
                        "Train Epoch: 0 [57600/60000] Loss: 0.301152 Acc: 0.9375\n",
                        "Train Epoch: 0 [59200/60000] Loss: 0.310887 Acc: 0.9375\n",
                        "Elapsed 21.29s, 21.29 s/epoch, 0.01 s/batch, ets 191.61s\n",
                        "\n",
                        "Test set: Average loss: 0.5220, Accuracy: 8054/10000 (81%)\n",
                        "\n",
                        "Train Epoch: 1 [1600/60000] Loss: 0.459358 Acc: 0.8750\n",
                        "Train Epoch: 1 [3200/60000] Loss: 0.536433 Acc: 0.7500\n",
                        "Train Epoch: 1 [4800/60000] Loss: 0.382011 Acc: 0.9375\n",
                        "Train Epoch: 1 [6400/60000] Loss: 0.364942 Acc: 0.9375\n",
                        "Train Epoch: 1 [8000/60000] Loss: 0.815194 Acc: 0.7500\n",
                        "Train Epoch: 1 [9600/60000] Loss: 0.410389 Acc: 0.8750\n",
                        "Train Epoch: 1 [11200/60000] Loss: 0.440761 Acc: 0.8125\n",
                        "Train Epoch: 1 [12800/60000] Loss: 0.575450 Acc: 0.7500\n",
                        "Train Epoch: 1 [14400/60000] Loss: 0.278805 Acc: 0.8750\n",
                        "Train Epoch: 1 [16000/60000] Loss: 0.459124 Acc: 0.8125\n",
                        "Train Epoch: 1 [17600/60000] Loss: 0.478600 Acc: 0.8750\n",
                        "Train Epoch: 1 [19200/60000] Loss: 0.726083 Acc: 0.7500\n",
                        "Train Epoch: 1 [20800/60000] Loss: 0.577417 Acc: 0.8125\n",
                        "Train Epoch: 1 [22400/60000] Loss: 0.532473 Acc: 0.7500\n",
                        "Train Epoch: 1 [24000/60000] Loss: 0.520807 Acc: 0.8125\n",
                        "Train Epoch: 1 [25600/60000] Loss: 0.657064 Acc: 0.7500\n",
                        "Train Epoch: 1 [27200/60000] Loss: 0.223299 Acc: 0.9375\n",
                        "Train Epoch: 1 [28800/60000] Loss: 0.322603 Acc: 0.8125\n",
                        "Train Epoch: 1 [30400/60000] Loss: 0.816724 Acc: 0.7500\n",
                        "Train Epoch: 1 [32000/60000] Loss: 0.223879 Acc: 0.8750\n",
                        "Train Epoch: 1 [33600/60000] Loss: 0.231285 Acc: 0.9375\n",
                        "Train Epoch: 1 [35200/60000] Loss: 0.272470 Acc: 0.8750\n",
                        "Train Epoch: 1 [36800/60000] Loss: 0.455394 Acc: 0.8125\n",
                        "Train Epoch: 1 [38400/60000] Loss: 0.315618 Acc: 0.9375\n",
                        "Train Epoch: 1 [40000/60000] Loss: 0.320488 Acc: 0.9375\n",
                        "Train Epoch: 1 [41600/60000] Loss: 0.501513 Acc: 0.8750\n",
                        "Train Epoch: 1 [43200/60000] Loss: 0.496386 Acc: 0.8125\n",
                        "Train Epoch: 1 [44800/60000] Loss: 0.376681 Acc: 0.8125\n",
                        "Train Epoch: 1 [46400/60000] Loss: 0.307117 Acc: 0.8125\n",
                        "Train Epoch: 1 [48000/60000] Loss: 0.501610 Acc: 0.8125\n",
                        "Train Epoch: 1 [49600/60000] Loss: 0.547452 Acc: 0.8125\n",
                        "Train Epoch: 1 [51200/60000] Loss: 0.295354 Acc: 0.9375\n",
                        "Train Epoch: 1 [52800/60000] Loss: 0.353255 Acc: 0.8750\n",
                        "Train Epoch: 1 [54400/60000] Loss: 0.883327 Acc: 0.7500\n",
                        "Train Epoch: 1 [56000/60000] Loss: 0.207296 Acc: 1.0000\n",
                        "Train Epoch: 1 [57600/60000] Loss: 0.650120 Acc: 0.6875\n",
                        "Train Epoch: 1 [59200/60000] Loss: 0.337195 Acc: 0.8125\n",
                        "Elapsed 47.26s, 23.63 s/epoch, 0.01 s/batch, ets 189.02s\n",
                        "\n",
                        "Test set: Average loss: 0.4423, Accuracy: 8405/10000 (84%)\n",
                        "\n",
                        "Train Epoch: 2 [1600/60000] Loss: 0.387926 Acc: 0.8750\n",
                        "Train Epoch: 2 [3200/60000] Loss: 0.705013 Acc: 0.8750\n",
                        "Train Epoch: 2 [4800/60000] Loss: 0.409677 Acc: 0.8125\n",
                        "Train Epoch: 2 [6400/60000] Loss: 0.493801 Acc: 0.8750\n",
                        "Train Epoch: 2 [8000/60000] Loss: 0.222734 Acc: 1.0000\n",
                        "Train Epoch: 2 [9600/60000] Loss: 0.122092 Acc: 1.0000\n",
                        "Train Epoch: 2 [11200/60000] Loss: 0.346602 Acc: 0.8125\n",
                        "Train Epoch: 2 [12800/60000] Loss: 0.289387 Acc: 0.8750\n",
                        "Train Epoch: 2 [14400/60000] Loss: 0.504196 Acc: 0.7500\n",
                        "Train Epoch: 2 [16000/60000] Loss: 0.209690 Acc: 0.8750\n",
                        "Train Epoch: 2 [17600/60000] Loss: 0.712460 Acc: 0.7500\n",
                        "Train Epoch: 2 [19200/60000] Loss: 1.037285 Acc: 0.6875\n",
                        "Train Epoch: 2 [20800/60000] Loss: 0.671681 Acc: 0.7500\n",
                        "Train Epoch: 2 [22400/60000] Loss: 0.378720 Acc: 0.8750\n",
                        "Train Epoch: 2 [24000/60000] Loss: 0.179978 Acc: 0.9375\n",
                        "Train Epoch: 2 [25600/60000] Loss: 0.282468 Acc: 0.9375\n",
                        "Train Epoch: 2 [27200/60000] Loss: 0.316091 Acc: 0.8750\n",
                        "Train Epoch: 2 [28800/60000] Loss: 0.322952 Acc: 0.8750\n",
                        "Train Epoch: 2 [30400/60000] Loss: 0.427958 Acc: 0.8750\n",
                        "Train Epoch: 2 [32000/60000] Loss: 0.193490 Acc: 0.9375\n",
                        "Train Epoch: 2 [33600/60000] Loss: 0.411887 Acc: 0.9375\n",
                        "Train Epoch: 2 [35200/60000] Loss: 0.390707 Acc: 0.8750\n",
                        "Train Epoch: 2 [36800/60000] Loss: 0.525955 Acc: 0.8125\n",
                        "Train Epoch: 2 [38400/60000] Loss: 0.292809 Acc: 0.8750\n",
                        "Train Epoch: 2 [40000/60000] Loss: 0.874464 Acc: 0.7500\n",
                        "Train Epoch: 2 [41600/60000] Loss: 0.826607 Acc: 0.6250\n",
                        "Train Epoch: 2 [43200/60000] Loss: 0.287152 Acc: 0.9375\n",
                        "Train Epoch: 2 [44800/60000] Loss: 0.421659 Acc: 0.9375\n",
                        "Train Epoch: 2 [46400/60000] Loss: 0.542806 Acc: 0.7500\n",
                        "Train Epoch: 2 [48000/60000] Loss: 0.122802 Acc: 1.0000\n",
                        "Train Epoch: 2 [49600/60000] Loss: 0.638086 Acc: 0.7500\n",
                        "Train Epoch: 2 [51200/60000] Loss: 0.183997 Acc: 0.9375\n",
                        "Train Epoch: 2 [52800/60000] Loss: 0.168677 Acc: 0.9375\n",
                        "Train Epoch: 2 [54400/60000] Loss: 0.401163 Acc: 0.8125\n",
                        "Train Epoch: 2 [56000/60000] Loss: 0.572565 Acc: 0.7500\n",
                        "Train Epoch: 2 [57600/60000] Loss: 0.268353 Acc: 0.9375\n",
                        "Train Epoch: 2 [59200/60000] Loss: 0.215495 Acc: 0.9375\n",
                        "Elapsed 76.75s, 25.58 s/epoch, 0.01 s/batch, ets 179.09s\n",
                        "\n",
                        "Test set: Average loss: 0.3999, Accuracy: 8524/10000 (85%)\n",
                        "\n",
                        "Train Epoch: 3 [1600/60000] Loss: 0.278826 Acc: 0.8750\n",
                        "Train Epoch: 3 [3200/60000] Loss: 0.133302 Acc: 0.9375\n",
                        "Train Epoch: 3 [4800/60000] Loss: 0.219544 Acc: 0.9375\n",
                        "Train Epoch: 3 [6400/60000] Loss: 0.345827 Acc: 0.8750\n",
                        "Train Epoch: 3 [8000/60000] Loss: 0.199279 Acc: 0.9375\n",
                        "Train Epoch: 3 [9600/60000] Loss: 0.741775 Acc: 0.8750\n",
                        "Train Epoch: 3 [11200/60000] Loss: 0.196651 Acc: 0.9375\n",
                        "Train Epoch: 3 [12800/60000] Loss: 0.726288 Acc: 0.8125\n",
                        "Train Epoch: 3 [14400/60000] Loss: 0.149061 Acc: 1.0000\n",
                        "Train Epoch: 3 [16000/60000] Loss: 0.151693 Acc: 0.9375\n",
                        "Train Epoch: 3 [17600/60000] Loss: 0.921443 Acc: 0.6875\n",
                        "Train Epoch: 3 [19200/60000] Loss: 0.242773 Acc: 0.8750\n",
                        "Train Epoch: 3 [20800/60000] Loss: 0.063724 Acc: 1.0000\n",
                        "Train Epoch: 3 [22400/60000] Loss: 0.623419 Acc: 0.8750\n",
                        "Train Epoch: 3 [24000/60000] Loss: 0.406650 Acc: 0.8125\n",
                        "Train Epoch: 3 [25600/60000] Loss: 0.331044 Acc: 0.7500\n",
                        "Train Epoch: 3 [27200/60000] Loss: 0.275502 Acc: 0.8750\n",
                        "Train Epoch: 3 [28800/60000] Loss: 0.370965 Acc: 0.8750\n",
                        "Train Epoch: 3 [30400/60000] Loss: 0.256937 Acc: 0.8750\n",
                        "Train Epoch: 3 [32000/60000] Loss: 0.215158 Acc: 0.9375\n",
                        "Train Epoch: 3 [33600/60000] Loss: 0.324192 Acc: 0.8750\n",
                        "Train Epoch: 3 [35200/60000] Loss: 0.154181 Acc: 0.9375\n",
                        "Train Epoch: 3 [36800/60000] Loss: 0.478503 Acc: 0.8125\n",
                        "Train Epoch: 3 [38400/60000] Loss: 0.269749 Acc: 0.8750\n",
                        "Train Epoch: 3 [40000/60000] Loss: 1.109787 Acc: 0.5625\n",
                        "Train Epoch: 3 [41600/60000] Loss: 0.434876 Acc: 0.8125\n",
                        "Train Epoch: 3 [43200/60000] Loss: 0.587502 Acc: 0.7500\n",
                        "Train Epoch: 3 [44800/60000] Loss: 0.113320 Acc: 0.9375\n",
                        "Train Epoch: 3 [46400/60000] Loss: 0.251300 Acc: 0.9375\n",
                        "Train Epoch: 3 [48000/60000] Loss: 1.127537 Acc: 0.6250\n",
                        "Train Epoch: 3 [49600/60000] Loss: 0.185002 Acc: 0.9375\n",
                        "Train Epoch: 3 [51200/60000] Loss: 1.283975 Acc: 0.6875\n",
                        "Train Epoch: 3 [52800/60000] Loss: 0.403196 Acc: 0.8750\n",
                        "Train Epoch: 3 [54400/60000] Loss: 0.504194 Acc: 0.8125\n",
                        "Train Epoch: 3 [56000/60000] Loss: 0.250384 Acc: 0.9375\n",
                        "Train Epoch: 3 [57600/60000] Loss: 0.303601 Acc: 0.9375\n",
                        "Train Epoch: 3 [59200/60000] Loss: 0.319363 Acc: 0.8750\n",
                        "Elapsed 103.55s, 25.89 s/epoch, 0.01 s/batch, ets 155.33s\n",
                        "\n",
                        "Test set: Average loss: 0.3650, Accuracy: 8619/10000 (86%)\n",
                        "\n",
                        "Train Epoch: 4 [1600/60000] Loss: 0.112087 Acc: 1.0000\n",
                        "Train Epoch: 4 [3200/60000] Loss: 0.251052 Acc: 0.9375\n",
                        "Train Epoch: 4 [4800/60000] Loss: 0.250635 Acc: 0.9375\n",
                        "Train Epoch: 4 [6400/60000] Loss: 0.432153 Acc: 0.8125\n",
                        "Train Epoch: 4 [8000/60000] Loss: 0.229863 Acc: 0.9375\n",
                        "Train Epoch: 4 [9600/60000] Loss: 0.277335 Acc: 0.8750\n",
                        "Train Epoch: 4 [11200/60000] Loss: 0.125187 Acc: 0.9375\n",
                        "Train Epoch: 4 [12800/60000] Loss: 0.281289 Acc: 0.8750\n",
                        "Train Epoch: 4 [14400/60000] Loss: 0.326511 Acc: 0.8750\n",
                        "Train Epoch: 4 [16000/60000] Loss: 0.114512 Acc: 0.9375\n",
                        "Train Epoch: 4 [17600/60000] Loss: 0.296466 Acc: 0.8750\n",
                        "Train Epoch: 4 [19200/60000] Loss: 0.582938 Acc: 0.8125\n",
                        "Train Epoch: 4 [20800/60000] Loss: 0.188256 Acc: 0.9375\n",
                        "Train Epoch: 4 [22400/60000] Loss: 0.131272 Acc: 1.0000\n",
                        "Train Epoch: 4 [24000/60000] Loss: 0.475546 Acc: 0.7500\n",
                        "Train Epoch: 4 [25600/60000] Loss: 0.305363 Acc: 0.8750\n",
                        "Train Epoch: 4 [27200/60000] Loss: 0.049370 Acc: 1.0000\n",
                        "Train Epoch: 4 [28800/60000] Loss: 0.287853 Acc: 0.8750\n",
                        "Train Epoch: 4 [30400/60000] Loss: 0.204278 Acc: 0.9375\n",
                        "Train Epoch: 4 [32000/60000] Loss: 0.176648 Acc: 0.8750\n",
                        "Train Epoch: 4 [33600/60000] Loss: 0.195028 Acc: 1.0000\n",
                        "Train Epoch: 4 [35200/60000] Loss: 0.480978 Acc: 0.8125\n",
                        "Train Epoch: 4 [36800/60000] Loss: 0.814153 Acc: 0.6250\n",
                        "Train Epoch: 4 [38400/60000] Loss: 0.287184 Acc: 0.8750\n",
                        "Train Epoch: 4 [40000/60000] Loss: 0.157881 Acc: 0.9375\n",
                        "Train Epoch: 4 [41600/60000] Loss: 0.514232 Acc: 0.8125\n",
                        "Train Epoch: 4 [43200/60000] Loss: 0.313151 Acc: 0.8750\n",
                        "Train Epoch: 4 [44800/60000] Loss: 0.282701 Acc: 0.9375\n",
                        "Train Epoch: 4 [46400/60000] Loss: 0.331451 Acc: 0.8750\n",
                        "Train Epoch: 4 [48000/60000] Loss: 0.211793 Acc: 1.0000\n",
                        "Train Epoch: 4 [49600/60000] Loss: 0.381266 Acc: 0.8750\n",
                        "Train Epoch: 4 [51200/60000] Loss: 0.178024 Acc: 1.0000\n",
                        "Train Epoch: 4 [52800/60000] Loss: 0.689224 Acc: 0.7500\n",
                        "Train Epoch: 4 [54400/60000] Loss: 0.237222 Acc: 0.9375\n",
                        "Train Epoch: 4 [56000/60000] Loss: 0.422996 Acc: 0.7500\n",
                        "Train Epoch: 4 [57600/60000] Loss: 0.466664 Acc: 0.8750\n",
                        "Train Epoch: 4 [59200/60000] Loss: 0.410246 Acc: 0.8750\n",
                        "Elapsed 130.34s, 26.07 s/epoch, 0.01 s/batch, ets 130.34s\n",
                        "\n",
                        "Test set: Average loss: 0.3579, Accuracy: 8686/10000 (87%)\n",
                        "\n",
                        "Train Epoch: 5 [1600/60000] Loss: 0.344542 Acc: 0.8750\n",
                        "Train Epoch: 5 [3200/60000] Loss: 0.591312 Acc: 0.8750\n",
                        "Train Epoch: 5 [4800/60000] Loss: 0.311843 Acc: 0.9375\n",
                        "Train Epoch: 5 [6400/60000] Loss: 0.061383 Acc: 1.0000\n",
                        "Train Epoch: 5 [8000/60000] Loss: 0.424873 Acc: 0.8750\n",
                        "Train Epoch: 5 [9600/60000] Loss: 0.543112 Acc: 0.8125\n",
                        "Train Epoch: 5 [11200/60000] Loss: 0.133784 Acc: 0.9375\n",
                        "Train Epoch: 5 [12800/60000] Loss: 0.238097 Acc: 0.9375\n",
                        "Train Epoch: 5 [14400/60000] Loss: 0.356836 Acc: 0.7500\n",
                        "Train Epoch: 5 [16000/60000] Loss: 0.186877 Acc: 0.8750\n",
                        "Train Epoch: 5 [17600/60000] Loss: 0.502795 Acc: 0.8750\n",
                        "Train Epoch: 5 [19200/60000] Loss: 0.359940 Acc: 0.8750\n",
                        "Train Epoch: 5 [20800/60000] Loss: 0.395708 Acc: 0.8125\n",
                        "Train Epoch: 5 [22400/60000] Loss: 0.418228 Acc: 0.6875\n",
                        "Train Epoch: 5 [24000/60000] Loss: 0.368111 Acc: 0.8750\n",
                        "Train Epoch: 5 [25600/60000] Loss: 0.178497 Acc: 1.0000\n",
                        "Train Epoch: 5 [27200/60000] Loss: 0.134753 Acc: 0.9375\n",
                        "Train Epoch: 5 [28800/60000] Loss: 0.597165 Acc: 0.6875\n",
                        "Train Epoch: 5 [30400/60000] Loss: 0.312477 Acc: 0.8125\n",
                        "Train Epoch: 5 [32000/60000] Loss: 0.288135 Acc: 0.8750\n",
                        "Train Epoch: 5 [33600/60000] Loss: 0.080018 Acc: 1.0000\n",
                        "Train Epoch: 5 [35200/60000] Loss: 0.137757 Acc: 1.0000\n",
                        "Train Epoch: 5 [36800/60000] Loss: 0.217136 Acc: 0.8750\n",
                        "Train Epoch: 5 [38400/60000] Loss: 0.317043 Acc: 0.8750\n",
                        "Train Epoch: 5 [40000/60000] Loss: 0.248457 Acc: 0.9375\n",
                        "Train Epoch: 5 [41600/60000] Loss: 0.071276 Acc: 1.0000\n",
                        "Train Epoch: 5 [43200/60000] Loss: 0.285324 Acc: 0.8750\n",
                        "Train Epoch: 5 [44800/60000] Loss: 0.338941 Acc: 0.8125\n",
                        "Train Epoch: 5 [46400/60000] Loss: 0.886140 Acc: 0.6250\n",
                        "Train Epoch: 5 [48000/60000] Loss: 0.421165 Acc: 0.8750\n",
                        "Train Epoch: 5 [49600/60000] Loss: 0.447436 Acc: 0.8750\n",
                        "Train Epoch: 5 [51200/60000] Loss: 0.115256 Acc: 0.9375\n",
                        "Train Epoch: 5 [52800/60000] Loss: 0.034733 Acc: 1.0000\n",
                        "Train Epoch: 5 [54400/60000] Loss: 0.319025 Acc: 0.8750\n",
                        "Train Epoch: 5 [56000/60000] Loss: 0.194885 Acc: 0.9375\n",
                        "Train Epoch: 5 [57600/60000] Loss: 0.344895 Acc: 0.8750\n",
                        "Train Epoch: 5 [59200/60000] Loss: 0.135373 Acc: 0.9375\n",
                        "Elapsed 198.61s, 33.10 s/epoch, 0.01 s/batch, ets 132.41s\n",
                        "\n",
                        "Test set: Average loss: 0.3492, Accuracy: 8734/10000 (87%)\n",
                        "\n",
                        "Train Epoch: 6 [1600/60000] Loss: 0.228081 Acc: 0.8750\n",
                        "Train Epoch: 6 [3200/60000] Loss: 0.205748 Acc: 0.9375\n",
                        "Train Epoch: 6 [4800/60000] Loss: 0.396973 Acc: 0.8125\n",
                        "Train Epoch: 6 [6400/60000] Loss: 0.382938 Acc: 0.8750\n",
                        "Train Epoch: 6 [8000/60000] Loss: 0.236511 Acc: 0.9375\n",
                        "Train Epoch: 6 [9600/60000] Loss: 0.188687 Acc: 0.9375\n",
                        "Train Epoch: 6 [11200/60000] Loss: 0.643982 Acc: 0.7500\n",
                        "Train Epoch: 6 [12800/60000] Loss: 0.161743 Acc: 0.8750\n",
                        "Train Epoch: 6 [14400/60000] Loss: 0.155580 Acc: 0.8750\n",
                        "Train Epoch: 6 [16000/60000] Loss: 0.256074 Acc: 0.8750\n",
                        "Train Epoch: 6 [17600/60000] Loss: 0.097147 Acc: 0.9375\n",
                        "Train Epoch: 6 [19200/60000] Loss: 0.081901 Acc: 1.0000\n",
                        "Train Epoch: 6 [20800/60000] Loss: 0.333220 Acc: 0.8125\n",
                        "Train Epoch: 6 [22400/60000] Loss: 0.450473 Acc: 0.8750\n",
                        "Train Epoch: 6 [24000/60000] Loss: 0.139884 Acc: 0.9375\n",
                        "Train Epoch: 6 [25600/60000] Loss: 0.167257 Acc: 0.9375\n",
                        "Train Epoch: 6 [27200/60000] Loss: 0.258802 Acc: 0.9375\n",
                        "Train Epoch: 6 [28800/60000] Loss: 0.448743 Acc: 0.7500\n",
                        "Train Epoch: 6 [30400/60000] Loss: 0.151600 Acc: 1.0000\n",
                        "Train Epoch: 6 [32000/60000] Loss: 0.371112 Acc: 0.8125\n",
                        "Train Epoch: 6 [33600/60000] Loss: 0.256000 Acc: 0.9375\n",
                        "Train Epoch: 6 [35200/60000] Loss: 0.276301 Acc: 0.9375\n",
                        "Train Epoch: 6 [36800/60000] Loss: 0.064939 Acc: 1.0000\n",
                        "Train Epoch: 6 [38400/60000] Loss: 0.373299 Acc: 0.8750\n",
                        "Train Epoch: 6 [40000/60000] Loss: 0.058018 Acc: 1.0000\n",
                        "Train Epoch: 6 [41600/60000] Loss: 0.203292 Acc: 0.8750\n",
                        "Train Epoch: 6 [43200/60000] Loss: 0.445108 Acc: 0.8125\n",
                        "Train Epoch: 6 [44800/60000] Loss: 0.541469 Acc: 0.8125\n",
                        "Train Epoch: 6 [46400/60000] Loss: 0.138538 Acc: 0.9375\n",
                        "Train Epoch: 6 [48000/60000] Loss: 0.271270 Acc: 0.8750\n",
                        "Train Epoch: 6 [49600/60000] Loss: 0.288955 Acc: 0.8750\n",
                        "Train Epoch: 6 [51200/60000] Loss: 0.225630 Acc: 0.8750\n",
                        "Train Epoch: 6 [52800/60000] Loss: 0.319808 Acc: 0.8750\n",
                        "Train Epoch: 6 [54400/60000] Loss: 0.223517 Acc: 0.9375\n",
                        "Train Epoch: 6 [56000/60000] Loss: 0.200179 Acc: 0.9375\n",
                        "Train Epoch: 6 [57600/60000] Loss: 0.285954 Acc: 0.8750\n",
                        "Train Epoch: 6 [59200/60000] Loss: 0.067394 Acc: 1.0000\n",
                        "Elapsed 260.43s, 37.20 s/epoch, 0.01 s/batch, ets 111.61s\n",
                        "\n",
                        "Test set: Average loss: 0.3248, Accuracy: 8812/10000 (88%)\n",
                        "\n",
                        "Train Epoch: 7 [1600/60000] Loss: 0.416312 Acc: 0.8750\n",
                        "Train Epoch: 7 [3200/60000] Loss: 0.228165 Acc: 0.9375\n",
                        "Train Epoch: 7 [4800/60000] Loss: 0.208045 Acc: 0.9375\n",
                        "Train Epoch: 7 [6400/60000] Loss: 0.833408 Acc: 0.8125\n",
                        "Train Epoch: 7 [8000/60000] Loss: 0.262056 Acc: 0.9375\n",
                        "Train Epoch: 7 [9600/60000] Loss: 0.287875 Acc: 0.8750\n",
                        "Train Epoch: 7 [11200/60000] Loss: 0.184537 Acc: 0.9375\n",
                        "Train Epoch: 7 [12800/60000] Loss: 0.400734 Acc: 0.8750\n",
                        "Train Epoch: 7 [14400/60000] Loss: 0.347870 Acc: 0.8125\n",
                        "Train Epoch: 7 [16000/60000] Loss: 0.198775 Acc: 1.0000\n",
                        "Train Epoch: 7 [17600/60000] Loss: 0.306945 Acc: 0.7500\n",
                        "Train Epoch: 7 [19200/60000] Loss: 0.739320 Acc: 0.7500\n",
                        "Train Epoch: 7 [20800/60000] Loss: 0.068736 Acc: 1.0000\n",
                        "Train Epoch: 7 [22400/60000] Loss: 0.074264 Acc: 1.0000\n",
                        "Train Epoch: 7 [24000/60000] Loss: 0.077660 Acc: 1.0000\n",
                        "Train Epoch: 7 [25600/60000] Loss: 0.451366 Acc: 0.8750\n",
                        "Train Epoch: 7 [27200/60000] Loss: 0.153501 Acc: 0.9375\n",
                        "Train Epoch: 7 [28800/60000] Loss: 0.348435 Acc: 0.8125\n",
                        "Train Epoch: 7 [30400/60000] Loss: 0.167938 Acc: 0.9375\n",
                        "Train Epoch: 7 [32000/60000] Loss: 0.159290 Acc: 0.9375\n",
                        "Train Epoch: 7 [33600/60000] Loss: 0.212547 Acc: 0.8750\n",
                        "Train Epoch: 7 [35200/60000] Loss: 0.557571 Acc: 0.7500\n",
                        "Train Epoch: 7 [36800/60000] Loss: 0.423611 Acc: 0.8750\n",
                        "Train Epoch: 7 [38400/60000] Loss: 0.697925 Acc: 0.6875\n",
                        "Train Epoch: 7 [40000/60000] Loss: 0.018639 Acc: 1.0000\n",
                        "Train Epoch: 7 [41600/60000] Loss: 0.333450 Acc: 0.8750\n",
                        "Train Epoch: 7 [43200/60000] Loss: 0.055450 Acc: 1.0000\n",
                        "Train Epoch: 7 [44800/60000] Loss: 0.142691 Acc: 0.9375\n",
                        "Train Epoch: 7 [46400/60000] Loss: 0.106240 Acc: 0.9375\n",
                        "Train Epoch: 7 [48000/60000] Loss: 0.050171 Acc: 1.0000\n",
                        "Train Epoch: 7 [49600/60000] Loss: 0.184824 Acc: 0.9375\n",
                        "Train Epoch: 7 [51200/60000] Loss: 0.177300 Acc: 1.0000\n",
                        "Train Epoch: 7 [52800/60000] Loss: 0.210171 Acc: 0.9375\n",
                        "Train Epoch: 7 [54400/60000] Loss: 0.360385 Acc: 0.8750\n",
                        "Train Epoch: 7 [56000/60000] Loss: 0.373641 Acc: 0.8750\n",
                        "Train Epoch: 7 [57600/60000] Loss: 0.218526 Acc: 0.8750\n",
                        "Train Epoch: 7 [59200/60000] Loss: 0.353372 Acc: 0.8750\n",
                        "Elapsed 295.25s, 36.91 s/epoch, 0.01 s/batch, ets 73.81s\n",
                        "\n",
                        "Test set: Average loss: 0.3106, Accuracy: 8841/10000 (88%)\n",
                        "\n",
                        "Train Epoch: 8 [1600/60000] Loss: 0.016040 Acc: 1.0000\n",
                        "Train Epoch: 8 [3200/60000] Loss: 0.746233 Acc: 0.6875\n",
                        "Train Epoch: 8 [4800/60000] Loss: 0.383273 Acc: 0.8125\n",
                        "Train Epoch: 8 [6400/60000] Loss: 0.382942 Acc: 0.8750\n",
                        "Train Epoch: 8 [8000/60000] Loss: 0.089721 Acc: 1.0000\n",
                        "Train Epoch: 8 [9600/60000] Loss: 0.362397 Acc: 0.9375\n",
                        "Train Epoch: 8 [11200/60000] Loss: 0.075060 Acc: 1.0000\n",
                        "Train Epoch: 8 [12800/60000] Loss: 0.163137 Acc: 0.9375\n",
                        "Train Epoch: 8 [14400/60000] Loss: 0.699424 Acc: 0.8125\n",
                        "Train Epoch: 8 [16000/60000] Loss: 0.058028 Acc: 1.0000\n",
                        "Train Epoch: 8 [17600/60000] Loss: 0.430814 Acc: 0.8125\n",
                        "Train Epoch: 8 [19200/60000] Loss: 0.055655 Acc: 1.0000\n",
                        "Train Epoch: 8 [20800/60000] Loss: 0.357826 Acc: 0.9375\n",
                        "Train Epoch: 8 [22400/60000] Loss: 0.197971 Acc: 0.9375\n",
                        "Train Epoch: 8 [24000/60000] Loss: 0.354172 Acc: 0.8750\n",
                        "Train Epoch: 8 [25600/60000] Loss: 0.204236 Acc: 0.9375\n",
                        "Train Epoch: 8 [27200/60000] Loss: 0.138193 Acc: 0.9375\n",
                        "Train Epoch: 8 [28800/60000] Loss: 0.122276 Acc: 0.9375\n",
                        "Train Epoch: 8 [30400/60000] Loss: 0.356153 Acc: 0.9375\n",
                        "Train Epoch: 8 [32000/60000] Loss: 0.177982 Acc: 0.9375\n",
                        "Train Epoch: 8 [33600/60000] Loss: 0.118294 Acc: 0.9375\n",
                        "Train Epoch: 8 [35200/60000] Loss: 0.074057 Acc: 1.0000\n",
                        "Train Epoch: 8 [36800/60000] Loss: 0.239061 Acc: 0.8750\n",
                        "Train Epoch: 8 [38400/60000] Loss: 0.401444 Acc: 0.8125\n",
                        "Train Epoch: 8 [40000/60000] Loss: 0.260469 Acc: 0.9375\n",
                        "Train Epoch: 8 [41600/60000] Loss: 0.103069 Acc: 1.0000\n",
                        "Train Epoch: 8 [43200/60000] Loss: 0.244986 Acc: 0.8750\n",
                        "Train Epoch: 8 [44800/60000] Loss: 0.334080 Acc: 0.8125\n",
                        "Train Epoch: 8 [46400/60000] Loss: 0.341254 Acc: 0.8125\n",
                        "Train Epoch: 8 [48000/60000] Loss: 0.154537 Acc: 0.9375\n",
                        "Train Epoch: 8 [49600/60000] Loss: 0.121361 Acc: 0.9375\n",
                        "Train Epoch: 8 [51200/60000] Loss: 0.093035 Acc: 1.0000\n",
                        "Train Epoch: 8 [52800/60000] Loss: 0.306976 Acc: 0.9375\n",
                        "Train Epoch: 8 [54400/60000] Loss: 0.048412 Acc: 1.0000\n",
                        "Train Epoch: 8 [56000/60000] Loss: 0.554658 Acc: 0.7500\n",
                        "Train Epoch: 8 [57600/60000] Loss: 0.277150 Acc: 0.9375\n",
                        "Train Epoch: 8 [59200/60000] Loss: 0.307330 Acc: 0.8750\n",
                        "Elapsed 327.17s, 36.35 s/epoch, 0.01 s/batch, ets 36.35s\n",
                        "\n",
                        "Test set: Average loss: 0.3055, Accuracy: 8884/10000 (89%)\n",
                        "\n",
                        "Train Epoch: 9 [1600/60000] Loss: 0.292586 Acc: 0.8750\n",
                        "Train Epoch: 9 [3200/60000] Loss: 0.121777 Acc: 0.9375\n",
                        "Train Epoch: 9 [4800/60000] Loss: 0.070496 Acc: 1.0000\n",
                        "Train Epoch: 9 [6400/60000] Loss: 0.158332 Acc: 0.9375\n",
                        "Train Epoch: 9 [8000/60000] Loss: 0.042044 Acc: 1.0000\n",
                        "Train Epoch: 9 [9600/60000] Loss: 0.038422 Acc: 1.0000\n",
                        "Train Epoch: 9 [11200/60000] Loss: 0.118567 Acc: 0.9375\n",
                        "Train Epoch: 9 [12800/60000] Loss: 0.167511 Acc: 0.8750\n",
                        "Train Epoch: 9 [14400/60000] Loss: 0.209284 Acc: 0.8750\n",
                        "Train Epoch: 9 [16000/60000] Loss: 0.170749 Acc: 1.0000\n",
                        "Train Epoch: 9 [17600/60000] Loss: 0.285992 Acc: 0.9375\n",
                        "Train Epoch: 9 [19200/60000] Loss: 0.245371 Acc: 0.9375\n",
                        "Train Epoch: 9 [20800/60000] Loss: 0.186454 Acc: 0.9375\n",
                        "Train Epoch: 9 [22400/60000] Loss: 0.100720 Acc: 0.9375\n",
                        "Train Epoch: 9 [24000/60000] Loss: 0.221164 Acc: 0.9375\n",
                        "Train Epoch: 9 [25600/60000] Loss: 0.218133 Acc: 0.8750\n",
                        "Train Epoch: 9 [27200/60000] Loss: 0.202934 Acc: 0.8750\n",
                        "Train Epoch: 9 [28800/60000] Loss: 0.397618 Acc: 0.7500\n",
                        "Train Epoch: 9 [30400/60000] Loss: 0.169573 Acc: 0.9375\n",
                        "Train Epoch: 9 [32000/60000] Loss: 0.352673 Acc: 0.8125\n",
                        "Train Epoch: 9 [33600/60000] Loss: 0.769693 Acc: 0.5000\n",
                        "Train Epoch: 9 [35200/60000] Loss: 0.316889 Acc: 0.8750\n",
                        "Train Epoch: 9 [36800/60000] Loss: 0.240661 Acc: 0.8750\n",
                        "Train Epoch: 9 [38400/60000] Loss: 0.346854 Acc: 0.8125\n",
                        "Train Epoch: 9 [40000/60000] Loss: 0.080391 Acc: 1.0000\n",
                        "Train Epoch: 9 [41600/60000] Loss: 0.153665 Acc: 0.9375\n",
                        "Train Epoch: 9 [43200/60000] Loss: 0.168232 Acc: 1.0000\n",
                        "Train Epoch: 9 [44800/60000] Loss: 0.525340 Acc: 0.8125\n",
                        "Train Epoch: 9 [46400/60000] Loss: 0.166106 Acc: 0.9375\n",
                        "Train Epoch: 9 [48000/60000] Loss: 0.180066 Acc: 0.9375\n",
                        "Train Epoch: 9 [49600/60000] Loss: 0.203508 Acc: 0.8750\n",
                        "Train Epoch: 9 [51200/60000] Loss: 0.591776 Acc: 0.8750\n",
                        "Train Epoch: 9 [52800/60000] Loss: 0.150178 Acc: 0.9375\n",
                        "Train Epoch: 9 [54400/60000] Loss: 0.041385 Acc: 1.0000\n",
                        "Train Epoch: 9 [56000/60000] Loss: 0.233254 Acc: 0.9375\n",
                        "Train Epoch: 9 [57600/60000] Loss: 0.254253 Acc: 0.8750\n",
                        "Train Epoch: 9 [59200/60000] Loss: 0.478499 Acc: 0.8125\n",
                        "Elapsed 355.61s, 35.56 s/epoch, 0.01 s/batch, ets 0.00s\n",
                        "\n",
                        "Test set: Average loss: 0.3114, Accuracy: 8863/10000 (89%)\n",
                        "\n",
                        "Total time: 360.63, Best Loss: 0.305\n",
                        "Train Epoch: 0 [1600/60000] Loss: 1.951546 Acc: 0.3750\n",
                        "Train Epoch: 0 [3200/60000] Loss: 1.075915 Acc: 0.8750\n",
                        "Train Epoch: 0 [4800/60000] Loss: 0.781488 Acc: 0.8750\n",
                        "Train Epoch: 0 [6400/60000] Loss: 0.780253 Acc: 0.6250\n",
                        "Train Epoch: 0 [8000/60000] Loss: 0.461866 Acc: 0.8750\n",
                        "Train Epoch: 0 [9600/60000] Loss: 0.469007 Acc: 0.8750\n",
                        "Train Epoch: 0 [11200/60000] Loss: 0.634006 Acc: 0.8125\n",
                        "Train Epoch: 0 [12800/60000] Loss: 0.962529 Acc: 0.6250\n",
                        "Train Epoch: 0 [14400/60000] Loss: 0.494311 Acc: 0.7500\n",
                        "Train Epoch: 0 [16000/60000] Loss: 0.522550 Acc: 0.8125\n",
                        "Train Epoch: 0 [17600/60000] Loss: 0.354610 Acc: 0.9375\n",
                        "Train Epoch: 0 [19200/60000] Loss: 0.369335 Acc: 0.8125\n",
                        "Train Epoch: 0 [20800/60000] Loss: 0.597298 Acc: 0.8125\n",
                        "Train Epoch: 0 [22400/60000] Loss: 0.616869 Acc: 0.7500\n",
                        "Train Epoch: 0 [24000/60000] Loss: 0.140364 Acc: 1.0000\n",
                        "Train Epoch: 0 [25600/60000] Loss: 0.691667 Acc: 0.7500\n",
                        "Train Epoch: 0 [27200/60000] Loss: 0.482956 Acc: 0.8125\n",
                        "Train Epoch: 0 [28800/60000] Loss: 0.171637 Acc: 1.0000\n",
                        "Train Epoch: 0 [30400/60000] Loss: 0.632181 Acc: 0.7500\n",
                        "Train Epoch: 0 [32000/60000] Loss: 0.597182 Acc: 0.7500\n",
                        "Train Epoch: 0 [33600/60000] Loss: 0.210560 Acc: 1.0000\n",
                        "Train Epoch: 0 [35200/60000] Loss: 0.709175 Acc: 0.6875\n",
                        "Train Epoch: 0 [36800/60000] Loss: 0.612910 Acc: 0.6250\n",
                        "Train Epoch: 0 [38400/60000] Loss: 0.102453 Acc: 0.9375\n",
                        "Train Epoch: 0 [40000/60000] Loss: 0.427416 Acc: 0.8125\n",
                        "Train Epoch: 0 [41600/60000] Loss: 0.264870 Acc: 0.8750\n",
                        "Train Epoch: 0 [43200/60000] Loss: 0.440884 Acc: 0.7500\n",
                        "Train Epoch: 0 [44800/60000] Loss: 0.443253 Acc: 0.8750\n",
                        "Train Epoch: 0 [46400/60000] Loss: 0.245038 Acc: 0.9375\n",
                        "Train Epoch: 0 [48000/60000] Loss: 0.804264 Acc: 0.7500\n",
                        "Train Epoch: 0 [49600/60000] Loss: 0.621704 Acc: 0.8125\n",
                        "Train Epoch: 0 [51200/60000] Loss: 0.220832 Acc: 0.9375\n",
                        "Train Epoch: 0 [52800/60000] Loss: 0.216027 Acc: 0.9375\n",
                        "Train Epoch: 0 [54400/60000] Loss: 0.561653 Acc: 0.7500\n",
                        "Train Epoch: 0 [56000/60000] Loss: 0.268546 Acc: 0.8750\n",
                        "Train Epoch: 0 [57600/60000] Loss: 0.191089 Acc: 0.9375\n",
                        "Train Epoch: 0 [59200/60000] Loss: 0.224361 Acc: 0.8750\n",
                        "Elapsed 24.13s, 24.13 s/epoch, 0.01 s/batch, ets 217.20s\n",
                        "\n",
                        "Test set: Average loss: 0.4217, Accuracy: 8526/10000 (85%)\n",
                        "\n",
                        "Train Epoch: 1 [1600/60000] Loss: 0.237875 Acc: 0.8750\n",
                        "Train Epoch: 1 [3200/60000] Loss: 0.489116 Acc: 0.7500\n",
                        "Train Epoch: 1 [4800/60000] Loss: 0.239841 Acc: 0.8750\n",
                        "Train Epoch: 1 [6400/60000] Loss: 0.237364 Acc: 0.9375\n",
                        "Train Epoch: 1 [8000/60000] Loss: 0.690736 Acc: 0.7500\n",
                        "Train Epoch: 1 [9600/60000] Loss: 0.265752 Acc: 0.9375\n",
                        "Train Epoch: 1 [11200/60000] Loss: 0.262530 Acc: 0.9375\n",
                        "Train Epoch: 1 [12800/60000] Loss: 0.742175 Acc: 0.8125\n",
                        "Train Epoch: 1 [14400/60000] Loss: 0.409109 Acc: 0.8750\n",
                        "Train Epoch: 1 [16000/60000] Loss: 0.423011 Acc: 0.8750\n",
                        "Train Epoch: 1 [17600/60000] Loss: 0.342699 Acc: 0.9375\n",
                        "Train Epoch: 1 [19200/60000] Loss: 0.701693 Acc: 0.6875\n",
                        "Train Epoch: 1 [20800/60000] Loss: 0.497089 Acc: 0.8750\n",
                        "Train Epoch: 1 [22400/60000] Loss: 0.299341 Acc: 0.8125\n",
                        "Train Epoch: 1 [24000/60000] Loss: 0.307779 Acc: 0.9375\n",
                        "Train Epoch: 1 [25600/60000] Loss: 0.518053 Acc: 0.7500\n",
                        "Train Epoch: 1 [27200/60000] Loss: 0.127654 Acc: 1.0000\n",
                        "Train Epoch: 1 [28800/60000] Loss: 0.433673 Acc: 0.8125\n",
                        "Train Epoch: 1 [30400/60000] Loss: 0.471949 Acc: 0.8750\n",
                        "Train Epoch: 1 [32000/60000] Loss: 0.330930 Acc: 0.8750\n",
                        "Train Epoch: 1 [33600/60000] Loss: 0.134785 Acc: 1.0000\n",
                        "Train Epoch: 1 [35200/60000] Loss: 0.163652 Acc: 0.9375\n",
                        "Train Epoch: 1 [36800/60000] Loss: 0.273835 Acc: 0.8750\n",
                        "Train Epoch: 1 [38400/60000] Loss: 0.374293 Acc: 0.8750\n",
                        "Train Epoch: 1 [40000/60000] Loss: 0.086710 Acc: 1.0000\n",
                        "Train Epoch: 1 [41600/60000] Loss: 0.385639 Acc: 0.8750\n",
                        "Train Epoch: 1 [43200/60000] Loss: 0.319455 Acc: 0.9375\n",
                        "Train Epoch: 1 [44800/60000] Loss: 0.400561 Acc: 0.8750\n",
                        "Train Epoch: 1 [46400/60000] Loss: 0.401620 Acc: 0.8750\n",
                        "Train Epoch: 1 [48000/60000] Loss: 0.448004 Acc: 0.7500\n",
                        "Train Epoch: 1 [49600/60000] Loss: 0.337044 Acc: 0.8750\n",
                        "Train Epoch: 1 [51200/60000] Loss: 0.159419 Acc: 0.9375\n",
                        "Train Epoch: 1 [52800/60000] Loss: 0.276656 Acc: 0.8750\n",
                        "Train Epoch: 1 [54400/60000] Loss: 0.674391 Acc: 0.7500\n",
                        "Train Epoch: 1 [56000/60000] Loss: 0.175847 Acc: 0.9375\n",
                        "Train Epoch: 1 [57600/60000] Loss: 0.336111 Acc: 0.8125\n",
                        "Train Epoch: 1 [59200/60000] Loss: 0.299975 Acc: 0.8125\n",
                        "Elapsed 55.40s, 27.70 s/epoch, 0.01 s/batch, ets 221.60s\n",
                        "\n",
                        "Test set: Average loss: 0.3437, Accuracy: 8764/10000 (88%)\n",
                        "\n",
                        "Train Epoch: 2 [1600/60000] Loss: 0.431821 Acc: 0.8750\n",
                        "Train Epoch: 2 [3200/60000] Loss: 0.791429 Acc: 0.8750\n",
                        "Train Epoch: 2 [4800/60000] Loss: 0.424480 Acc: 0.7500\n",
                        "Train Epoch: 2 [6400/60000] Loss: 0.398870 Acc: 0.9375\n",
                        "Train Epoch: 2 [8000/60000] Loss: 0.153147 Acc: 1.0000\n",
                        "Train Epoch: 2 [9600/60000] Loss: 0.111316 Acc: 1.0000\n",
                        "Train Epoch: 2 [11200/60000] Loss: 0.260144 Acc: 0.8750\n",
                        "Train Epoch: 2 [12800/60000] Loss: 0.293208 Acc: 0.8750\n",
                        "Train Epoch: 2 [14400/60000] Loss: 0.497738 Acc: 0.8125\n",
                        "Train Epoch: 2 [16000/60000] Loss: 0.139806 Acc: 0.9375\n",
                        "Train Epoch: 2 [17600/60000] Loss: 0.599039 Acc: 0.7500\n",
                        "Train Epoch: 2 [19200/60000] Loss: 0.991703 Acc: 0.7500\n",
                        "Train Epoch: 2 [20800/60000] Loss: 0.591648 Acc: 0.8125\n",
                        "Train Epoch: 2 [22400/60000] Loss: 0.453511 Acc: 0.8125\n",
                        "Train Epoch: 2 [24000/60000] Loss: 0.172162 Acc: 0.9375\n",
                        "Train Epoch: 2 [25600/60000] Loss: 0.242667 Acc: 0.9375\n",
                        "Train Epoch: 2 [27200/60000] Loss: 0.270870 Acc: 0.8750\n",
                        "Train Epoch: 2 [28800/60000] Loss: 0.206989 Acc: 0.9375\n",
                        "Train Epoch: 2 [30400/60000] Loss: 0.423992 Acc: 0.9375\n",
                        "Train Epoch: 2 [32000/60000] Loss: 0.222603 Acc: 0.9375\n",
                        "Train Epoch: 2 [33600/60000] Loss: 0.384388 Acc: 0.8125\n",
                        "Train Epoch: 2 [35200/60000] Loss: 0.242987 Acc: 0.8750\n",
                        "Train Epoch: 2 [36800/60000] Loss: 0.471211 Acc: 0.8125\n",
                        "Train Epoch: 2 [38400/60000] Loss: 0.269068 Acc: 0.8125\n",
                        "Train Epoch: 2 [40000/60000] Loss: 1.025971 Acc: 0.8125\n",
                        "Train Epoch: 2 [41600/60000] Loss: 0.623445 Acc: 0.8750\n",
                        "Train Epoch: 2 [43200/60000] Loss: 0.259233 Acc: 0.9375\n",
                        "Train Epoch: 2 [44800/60000] Loss: 0.315603 Acc: 0.9375\n",
                        "Train Epoch: 2 [46400/60000] Loss: 0.507506 Acc: 0.8750\n",
                        "Train Epoch: 2 [48000/60000] Loss: 0.153242 Acc: 0.9375\n",
                        "Train Epoch: 2 [49600/60000] Loss: 0.546787 Acc: 0.7500\n",
                        "Train Epoch: 2 [51200/60000] Loss: 0.141178 Acc: 1.0000\n",
                        "Train Epoch: 2 [52800/60000] Loss: 0.157901 Acc: 0.9375\n",
                        "Train Epoch: 2 [54400/60000] Loss: 0.320888 Acc: 0.9375\n",
                        "Train Epoch: 2 [56000/60000] Loss: 0.478951 Acc: 0.8125\n",
                        "Train Epoch: 2 [57600/60000] Loss: 0.230408 Acc: 0.9375\n",
                        "Train Epoch: 2 [59200/60000] Loss: 0.223631 Acc: 0.9375\n",
                        "Elapsed 88.06s, 29.35 s/epoch, 0.01 s/batch, ets 205.46s\n",
                        "\n",
                        "Test set: Average loss: 0.3378, Accuracy: 8789/10000 (88%)\n",
                        "\n",
                        "Train Epoch: 3 [1600/60000] Loss: 0.427523 Acc: 0.8750\n",
                        "Train Epoch: 3 [3200/60000] Loss: 0.084044 Acc: 1.0000\n",
                        "Train Epoch: 3 [4800/60000] Loss: 0.244562 Acc: 0.8750\n",
                        "Train Epoch: 3 [6400/60000] Loss: 0.403845 Acc: 0.8125\n",
                        "Train Epoch: 3 [8000/60000] Loss: 0.305718 Acc: 0.8750\n",
                        "Train Epoch: 3 [9600/60000] Loss: 0.595400 Acc: 0.8125\n",
                        "Train Epoch: 3 [11200/60000] Loss: 0.134840 Acc: 1.0000\n",
                        "Train Epoch: 3 [12800/60000] Loss: 0.596034 Acc: 0.8125\n",
                        "Train Epoch: 3 [14400/60000] Loss: 0.103820 Acc: 0.9375\n",
                        "Train Epoch: 3 [16000/60000] Loss: 0.099578 Acc: 0.9375\n",
                        "Train Epoch: 3 [17600/60000] Loss: 0.921068 Acc: 0.6875\n",
                        "Train Epoch: 3 [19200/60000] Loss: 0.333007 Acc: 0.8125\n",
                        "Train Epoch: 3 [20800/60000] Loss: 0.086315 Acc: 1.0000\n",
                        "Train Epoch: 3 [22400/60000] Loss: 0.563565 Acc: 0.9375\n",
                        "Train Epoch: 3 [24000/60000] Loss: 0.208006 Acc: 0.9375\n",
                        "Train Epoch: 3 [25600/60000] Loss: 0.267797 Acc: 0.8750\n",
                        "Train Epoch: 3 [27200/60000] Loss: 0.254492 Acc: 0.8750\n",
                        "Train Epoch: 3 [28800/60000] Loss: 0.109339 Acc: 1.0000\n",
                        "Train Epoch: 3 [30400/60000] Loss: 0.346091 Acc: 0.8125\n",
                        "Train Epoch: 3 [32000/60000] Loss: 0.346970 Acc: 0.8750\n",
                        "Train Epoch: 3 [33600/60000] Loss: 0.430881 Acc: 0.8750\n",
                        "Train Epoch: 3 [35200/60000] Loss: 0.107700 Acc: 0.9375\n",
                        "Train Epoch: 3 [36800/60000] Loss: 0.251682 Acc: 0.9375\n",
                        "Train Epoch: 3 [38400/60000] Loss: 0.173252 Acc: 0.9375\n",
                        "Train Epoch: 3 [40000/60000] Loss: 0.940703 Acc: 0.7500\n",
                        "Train Epoch: 3 [41600/60000] Loss: 0.315964 Acc: 0.8125\n",
                        "Train Epoch: 3 [43200/60000] Loss: 0.322884 Acc: 0.8750\n",
                        "Train Epoch: 3 [44800/60000] Loss: 0.098059 Acc: 0.9375\n",
                        "Train Epoch: 3 [46400/60000] Loss: 0.260389 Acc: 0.8750\n",
                        "Train Epoch: 3 [48000/60000] Loss: 0.746046 Acc: 0.8750\n",
                        "Train Epoch: 3 [49600/60000] Loss: 0.202402 Acc: 0.9375\n",
                        "Train Epoch: 3 [51200/60000] Loss: 1.194059 Acc: 0.6875\n",
                        "Train Epoch: 3 [52800/60000] Loss: 0.326567 Acc: 0.8750\n",
                        "Train Epoch: 3 [54400/60000] Loss: 0.602699 Acc: 0.7500\n",
                        "Train Epoch: 3 [56000/60000] Loss: 0.168015 Acc: 0.9375\n",
                        "Train Epoch: 3 [57600/60000] Loss: 0.294162 Acc: 0.9375\n",
                        "Train Epoch: 3 [59200/60000] Loss: 0.480250 Acc: 0.7500\n",
                        "Elapsed 121.78s, 30.44 s/epoch, 0.01 s/batch, ets 182.66s\n",
                        "\n",
                        "Test set: Average loss: 0.3222, Accuracy: 8832/10000 (88%)\n",
                        "\n",
                        "Train Epoch: 4 [1600/60000] Loss: 0.162149 Acc: 0.9375\n",
                        "Train Epoch: 4 [3200/60000] Loss: 0.182171 Acc: 0.9375\n",
                        "Train Epoch: 4 [4800/60000] Loss: 0.226603 Acc: 0.8750\n",
                        "Train Epoch: 4 [6400/60000] Loss: 0.322694 Acc: 0.8750\n",
                        "Train Epoch: 4 [8000/60000] Loss: 0.200421 Acc: 0.9375\n",
                        "Train Epoch: 4 [9600/60000] Loss: 0.236157 Acc: 0.8750\n",
                        "Train Epoch: 4 [11200/60000] Loss: 0.149621 Acc: 0.9375\n",
                        "Train Epoch: 4 [12800/60000] Loss: 0.289983 Acc: 0.8125\n",
                        "Train Epoch: 4 [14400/60000] Loss: 0.154965 Acc: 1.0000\n",
                        "Train Epoch: 4 [16000/60000] Loss: 0.053304 Acc: 1.0000\n",
                        "Train Epoch: 4 [17600/60000] Loss: 0.200880 Acc: 0.9375\n",
                        "Train Epoch: 4 [19200/60000] Loss: 0.388276 Acc: 0.8125\n",
                        "Train Epoch: 4 [20800/60000] Loss: 0.090711 Acc: 1.0000\n",
                        "Train Epoch: 4 [22400/60000] Loss: 0.116229 Acc: 1.0000\n",
                        "Train Epoch: 4 [24000/60000] Loss: 0.516561 Acc: 0.6875\n",
                        "Train Epoch: 4 [25600/60000] Loss: 0.167673 Acc: 0.9375\n",
                        "Train Epoch: 4 [27200/60000] Loss: 0.065089 Acc: 1.0000\n",
                        "Train Epoch: 4 [28800/60000] Loss: 0.311708 Acc: 0.8750\n",
                        "Train Epoch: 4 [30400/60000] Loss: 0.082873 Acc: 1.0000\n",
                        "Train Epoch: 4 [32000/60000] Loss: 0.204618 Acc: 0.8750\n",
                        "Train Epoch: 4 [33600/60000] Loss: 0.087944 Acc: 1.0000\n",
                        "Train Epoch: 4 [35200/60000] Loss: 0.607353 Acc: 0.8125\n",
                        "Train Epoch: 4 [36800/60000] Loss: 0.712243 Acc: 0.6875\n",
                        "Train Epoch: 4 [38400/60000] Loss: 0.303581 Acc: 0.8125\n",
                        "Train Epoch: 4 [40000/60000] Loss: 0.143136 Acc: 0.9375\n",
                        "Train Epoch: 4 [41600/60000] Loss: 0.375515 Acc: 0.8125\n",
                        "Train Epoch: 4 [43200/60000] Loss: 0.360172 Acc: 0.8750\n",
                        "Train Epoch: 4 [44800/60000] Loss: 0.151252 Acc: 0.9375\n",
                        "Train Epoch: 4 [46400/60000] Loss: 0.326669 Acc: 0.8750\n",
                        "Train Epoch: 4 [48000/60000] Loss: 0.188614 Acc: 0.9375\n",
                        "Train Epoch: 4 [49600/60000] Loss: 0.452835 Acc: 0.8125\n",
                        "Train Epoch: 4 [51200/60000] Loss: 0.155276 Acc: 0.9375\n",
                        "Train Epoch: 4 [52800/60000] Loss: 0.481901 Acc: 0.7500\n",
                        "Train Epoch: 4 [54400/60000] Loss: 0.224668 Acc: 0.9375\n",
                        "Train Epoch: 4 [56000/60000] Loss: 0.362907 Acc: 0.8125\n",
                        "Train Epoch: 4 [57600/60000] Loss: 0.419493 Acc: 0.8750\n",
                        "Train Epoch: 4 [59200/60000] Loss: 0.381453 Acc: 0.8750\n",
                        "Elapsed 156.18s, 31.24 s/epoch, 0.01 s/batch, ets 156.18s\n",
                        "\n",
                        "Test set: Average loss: 0.3040, Accuracy: 8893/10000 (89%)\n",
                        "\n",
                        "Train Epoch: 5 [1600/60000] Loss: 0.256238 Acc: 0.8750\n",
                        "Train Epoch: 5 [3200/60000] Loss: 0.428584 Acc: 0.8750\n",
                        "Train Epoch: 5 [4800/60000] Loss: 0.195219 Acc: 0.8750\n",
                        "Train Epoch: 5 [6400/60000] Loss: 0.040595 Acc: 1.0000\n",
                        "Train Epoch: 5 [8000/60000] Loss: 0.296390 Acc: 0.8750\n",
                        "Train Epoch: 5 [9600/60000] Loss: 0.362841 Acc: 0.8125\n",
                        "Train Epoch: 5 [11200/60000] Loss: 0.131110 Acc: 0.9375\n",
                        "Train Epoch: 5 [12800/60000] Loss: 0.181560 Acc: 0.9375\n",
                        "Train Epoch: 5 [14400/60000] Loss: 0.268013 Acc: 0.8125\n",
                        "Train Epoch: 5 [16000/60000] Loss: 0.326171 Acc: 0.8750\n",
                        "Train Epoch: 5 [17600/60000] Loss: 0.345984 Acc: 0.8125\n",
                        "Train Epoch: 5 [19200/60000] Loss: 0.630278 Acc: 0.8125\n",
                        "Train Epoch: 5 [20800/60000] Loss: 0.245370 Acc: 0.9375\n",
                        "Train Epoch: 5 [22400/60000] Loss: 0.147206 Acc: 1.0000\n",
                        "Train Epoch: 5 [24000/60000] Loss: 0.413817 Acc: 0.9375\n",
                        "Train Epoch: 5 [25600/60000] Loss: 0.151066 Acc: 0.9375\n",
                        "Train Epoch: 5 [27200/60000] Loss: 0.128213 Acc: 0.9375\n",
                        "Train Epoch: 5 [28800/60000] Loss: 0.402840 Acc: 0.7500\n",
                        "Train Epoch: 5 [30400/60000] Loss: 0.299865 Acc: 0.8750\n",
                        "Train Epoch: 5 [32000/60000] Loss: 0.412007 Acc: 0.8125\n",
                        "Train Epoch: 5 [33600/60000] Loss: 0.058921 Acc: 1.0000\n",
                        "Train Epoch: 5 [35200/60000] Loss: 0.069104 Acc: 1.0000\n",
                        "Train Epoch: 5 [36800/60000] Loss: 0.153952 Acc: 0.9375\n",
                        "Train Epoch: 5 [38400/60000] Loss: 0.307578 Acc: 0.8750\n",
                        "Train Epoch: 5 [40000/60000] Loss: 0.193740 Acc: 0.9375\n",
                        "Train Epoch: 5 [41600/60000] Loss: 0.060166 Acc: 1.0000\n",
                        "Train Epoch: 5 [43200/60000] Loss: 0.228912 Acc: 0.8750\n",
                        "Train Epoch: 5 [44800/60000] Loss: 0.344086 Acc: 0.8125\n",
                        "Train Epoch: 5 [46400/60000] Loss: 0.649841 Acc: 0.7500\n",
                        "Train Epoch: 5 [48000/60000] Loss: 0.574097 Acc: 0.8125\n",
                        "Train Epoch: 5 [49600/60000] Loss: 0.622568 Acc: 0.7500\n",
                        "Train Epoch: 5 [51200/60000] Loss: 0.098854 Acc: 1.0000\n",
                        "Train Epoch: 5 [52800/60000] Loss: 0.056002 Acc: 1.0000\n",
                        "Train Epoch: 5 [54400/60000] Loss: 0.267878 Acc: 0.9375\n",
                        "Train Epoch: 5 [56000/60000] Loss: 0.218362 Acc: 0.9375\n",
                        "Train Epoch: 5 [57600/60000] Loss: 0.251214 Acc: 0.9375\n",
                        "Train Epoch: 5 [59200/60000] Loss: 0.104162 Acc: 0.9375\n",
                        "Elapsed 189.64s, 31.61 s/epoch, 0.01 s/batch, ets 126.43s\n",
                        "\n",
                        "Test set: Average loss: 0.2981, Accuracy: 8941/10000 (89%)\n",
                        "\n",
                        "Train Epoch: 6 [1600/60000] Loss: 0.120331 Acc: 1.0000\n",
                        "Train Epoch: 6 [3200/60000] Loss: 0.088540 Acc: 1.0000\n",
                        "Train Epoch: 6 [4800/60000] Loss: 0.320145 Acc: 0.8750\n",
                        "Train Epoch: 6 [6400/60000] Loss: 0.439323 Acc: 0.8125\n",
                        "Train Epoch: 6 [8000/60000] Loss: 0.246124 Acc: 0.9375\n",
                        "Train Epoch: 6 [9600/60000] Loss: 0.174402 Acc: 0.9375\n",
                        "Train Epoch: 6 [11200/60000] Loss: 0.377854 Acc: 0.8125\n",
                        "Train Epoch: 6 [12800/60000] Loss: 0.081276 Acc: 1.0000\n",
                        "Train Epoch: 6 [14400/60000] Loss: 0.130831 Acc: 1.0000\n",
                        "Train Epoch: 6 [16000/60000] Loss: 0.270276 Acc: 0.9375\n",
                        "Train Epoch: 6 [17600/60000] Loss: 0.183727 Acc: 0.9375\n",
                        "Train Epoch: 6 [19200/60000] Loss: 0.153893 Acc: 0.9375\n",
                        "Train Epoch: 6 [20800/60000] Loss: 0.470673 Acc: 0.7500\n",
                        "Train Epoch: 6 [22400/60000] Loss: 0.808942 Acc: 0.7500\n",
                        "Train Epoch: 6 [24000/60000] Loss: 0.140249 Acc: 0.8750\n",
                        "Train Epoch: 6 [25600/60000] Loss: 0.254104 Acc: 0.8125\n",
                        "Train Epoch: 6 [27200/60000] Loss: 0.323585 Acc: 0.8750\n",
                        "Train Epoch: 6 [28800/60000] Loss: 0.449497 Acc: 0.8125\n",
                        "Train Epoch: 6 [30400/60000] Loss: 0.148266 Acc: 0.9375\n",
                        "Train Epoch: 6 [32000/60000] Loss: 0.207676 Acc: 0.8125\n",
                        "Train Epoch: 6 [33600/60000] Loss: 0.212550 Acc: 0.8750\n",
                        "Train Epoch: 6 [35200/60000] Loss: 0.102742 Acc: 0.9375\n",
                        "Train Epoch: 6 [36800/60000] Loss: 0.114962 Acc: 0.9375\n",
                        "Train Epoch: 6 [38400/60000] Loss: 0.347839 Acc: 0.8750\n",
                        "Train Epoch: 6 [40000/60000] Loss: 0.060832 Acc: 1.0000\n",
                        "Train Epoch: 6 [41600/60000] Loss: 0.109037 Acc: 1.0000\n",
                        "Train Epoch: 6 [43200/60000] Loss: 0.401483 Acc: 0.8125\n",
                        "Train Epoch: 6 [44800/60000] Loss: 0.541247 Acc: 0.8750\n",
                        "Train Epoch: 6 [46400/60000] Loss: 0.037572 Acc: 1.0000\n",
                        "Train Epoch: 6 [48000/60000] Loss: 0.256399 Acc: 0.8750\n",
                        "Train Epoch: 6 [49600/60000] Loss: 0.370337 Acc: 0.8125\n",
                        "Train Epoch: 6 [51200/60000] Loss: 0.282310 Acc: 0.8750\n",
                        "Train Epoch: 6 [52800/60000] Loss: 0.298393 Acc: 0.8750\n",
                        "Train Epoch: 6 [54400/60000] Loss: 0.243510 Acc: 0.9375\n",
                        "Train Epoch: 6 [56000/60000] Loss: 0.217457 Acc: 0.8750\n",
                        "Train Epoch: 6 [57600/60000] Loss: 0.215597 Acc: 0.9375\n",
                        "Train Epoch: 6 [59200/60000] Loss: 0.050439 Acc: 1.0000\n",
                        "Elapsed 225.83s, 32.26 s/epoch, 0.01 s/batch, ets 96.78s\n",
                        "\n",
                        "Test set: Average loss: 0.2913, Accuracy: 8964/10000 (90%)\n",
                        "\n",
                        "Train Epoch: 7 [1600/60000] Loss: 0.316690 Acc: 0.9375\n",
                        "Train Epoch: 7 [3200/60000] Loss: 0.359324 Acc: 0.8125\n",
                        "Train Epoch: 7 [4800/60000] Loss: 0.194912 Acc: 0.9375\n",
                        "Train Epoch: 7 [6400/60000] Loss: 0.842426 Acc: 0.6875\n",
                        "Train Epoch: 7 [8000/60000] Loss: 0.393595 Acc: 0.9375\n",
                        "Train Epoch: 7 [9600/60000] Loss: 0.313593 Acc: 0.9375\n",
                        "Train Epoch: 7 [11200/60000] Loss: 0.235173 Acc: 0.9375\n",
                        "Train Epoch: 7 [12800/60000] Loss: 0.309444 Acc: 0.8750\n",
                        "Train Epoch: 7 [14400/60000] Loss: 0.224615 Acc: 0.8750\n",
                        "Train Epoch: 7 [16000/60000] Loss: 0.224110 Acc: 0.8750\n",
                        "Train Epoch: 7 [17600/60000] Loss: 0.304145 Acc: 0.8125\n",
                        "Train Epoch: 7 [19200/60000] Loss: 0.448283 Acc: 0.9375\n",
                        "Train Epoch: 7 [20800/60000] Loss: 0.030175 Acc: 1.0000\n",
                        "Train Epoch: 7 [22400/60000] Loss: 0.069306 Acc: 0.9375\n",
                        "Train Epoch: 7 [24000/60000] Loss: 0.068248 Acc: 1.0000\n",
                        "Train Epoch: 7 [25600/60000] Loss: 0.455995 Acc: 0.8750\n",
                        "Train Epoch: 7 [27200/60000] Loss: 0.105475 Acc: 0.9375\n",
                        "Train Epoch: 7 [28800/60000] Loss: 0.219851 Acc: 0.9375\n",
                        "Train Epoch: 7 [30400/60000] Loss: 0.194720 Acc: 1.0000\n",
                        "Train Epoch: 7 [32000/60000] Loss: 0.070942 Acc: 1.0000\n",
                        "Train Epoch: 7 [33600/60000] Loss: 0.149743 Acc: 1.0000\n",
                        "Train Epoch: 7 [35200/60000] Loss: 0.487025 Acc: 0.8125\n",
                        "Train Epoch: 7 [36800/60000] Loss: 0.264809 Acc: 0.8750\n",
                        "Train Epoch: 7 [38400/60000] Loss: 0.757018 Acc: 0.7500\n",
                        "Train Epoch: 7 [40000/60000] Loss: 0.040541 Acc: 1.0000\n",
                        "Train Epoch: 7 [41600/60000] Loss: 0.243880 Acc: 0.8125\n",
                        "Train Epoch: 7 [43200/60000] Loss: 0.029322 Acc: 1.0000\n",
                        "Train Epoch: 7 [44800/60000] Loss: 0.111192 Acc: 0.9375\n",
                        "Train Epoch: 7 [46400/60000] Loss: 0.166994 Acc: 0.8750\n",
                        "Train Epoch: 7 [48000/60000] Loss: 0.050444 Acc: 1.0000\n",
                        "Train Epoch: 7 [49600/60000] Loss: 0.119799 Acc: 0.9375\n",
                        "Train Epoch: 7 [51200/60000] Loss: 0.270668 Acc: 0.9375\n",
                        "Train Epoch: 7 [52800/60000] Loss: 0.165168 Acc: 0.9375\n",
                        "Train Epoch: 7 [54400/60000] Loss: 0.141340 Acc: 0.9375\n",
                        "Train Epoch: 7 [56000/60000] Loss: 0.209231 Acc: 0.9375\n",
                        "Train Epoch: 7 [57600/60000] Loss: 0.226873 Acc: 0.8750\n",
                        "Train Epoch: 7 [59200/60000] Loss: 0.304419 Acc: 0.8750\n",
                        "Elapsed 263.04s, 32.88 s/epoch, 0.01 s/batch, ets 65.76s\n",
                        "\n",
                        "Test set: Average loss: 0.2788, Accuracy: 8998/10000 (90%)\n",
                        "\n",
                        "Train Epoch: 8 [1600/60000] Loss: 0.028265 Acc: 1.0000\n",
                        "Train Epoch: 8 [3200/60000] Loss: 0.518425 Acc: 0.8125\n",
                        "Train Epoch: 8 [4800/60000] Loss: 0.230819 Acc: 0.8750\n",
                        "Train Epoch: 8 [6400/60000] Loss: 0.251735 Acc: 0.9375\n",
                        "Train Epoch: 8 [8000/60000] Loss: 0.071869 Acc: 1.0000\n",
                        "Train Epoch: 8 [9600/60000] Loss: 0.309257 Acc: 0.9375\n",
                        "Train Epoch: 8 [11200/60000] Loss: 0.215396 Acc: 0.9375\n",
                        "Train Epoch: 8 [12800/60000] Loss: 0.196038 Acc: 0.9375\n",
                        "Train Epoch: 8 [14400/60000] Loss: 0.699209 Acc: 0.8750\n",
                        "Train Epoch: 8 [16000/60000] Loss: 0.126155 Acc: 1.0000\n",
                        "Train Epoch: 8 [17600/60000] Loss: 0.356286 Acc: 0.8750\n",
                        "Train Epoch: 8 [19200/60000] Loss: 0.043404 Acc: 1.0000\n",
                        "Train Epoch: 8 [20800/60000] Loss: 0.113086 Acc: 1.0000\n",
                        "Train Epoch: 8 [22400/60000] Loss: 0.217596 Acc: 0.8750\n",
                        "Train Epoch: 8 [24000/60000] Loss: 0.304567 Acc: 0.8125\n",
                        "Train Epoch: 8 [25600/60000] Loss: 0.207344 Acc: 0.9375\n",
                        "Train Epoch: 8 [27200/60000] Loss: 0.089331 Acc: 0.9375\n",
                        "Train Epoch: 8 [28800/60000] Loss: 0.095899 Acc: 0.9375\n",
                        "Train Epoch: 8 [30400/60000] Loss: 0.252278 Acc: 0.9375\n",
                        "Train Epoch: 8 [32000/60000] Loss: 0.105449 Acc: 1.0000\n",
                        "Train Epoch: 8 [33600/60000] Loss: 0.155541 Acc: 0.9375\n",
                        "Train Epoch: 8 [35200/60000] Loss: 0.050711 Acc: 1.0000\n",
                        "Train Epoch: 8 [36800/60000] Loss: 0.200406 Acc: 0.9375\n",
                        "Train Epoch: 8 [38400/60000] Loss: 0.230039 Acc: 0.9375\n",
                        "Train Epoch: 8 [40000/60000] Loss: 0.087481 Acc: 0.9375\n",
                        "Train Epoch: 8 [41600/60000] Loss: 0.073841 Acc: 1.0000\n",
                        "Train Epoch: 8 [43200/60000] Loss: 0.209431 Acc: 0.9375\n",
                        "Train Epoch: 8 [44800/60000] Loss: 0.260174 Acc: 0.8750\n",
                        "Train Epoch: 8 [46400/60000] Loss: 0.361265 Acc: 0.8750\n",
                        "Train Epoch: 8 [48000/60000] Loss: 0.178971 Acc: 0.8750\n",
                        "Train Epoch: 8 [49600/60000] Loss: 0.095572 Acc: 1.0000\n",
                        "Train Epoch: 8 [51200/60000] Loss: 0.103228 Acc: 0.9375\n",
                        "Train Epoch: 8 [52800/60000] Loss: 0.341900 Acc: 0.9375\n",
                        "Train Epoch: 8 [54400/60000] Loss: 0.026090 Acc: 1.0000\n",
                        "Train Epoch: 8 [56000/60000] Loss: 0.640859 Acc: 0.8125\n",
                        "Train Epoch: 8 [57600/60000] Loss: 0.273218 Acc: 0.8750\n",
                        "Train Epoch: 8 [59200/60000] Loss: 0.496298 Acc: 0.8125\n",
                        "Elapsed 297.97s, 33.11 s/epoch, 0.01 s/batch, ets 33.11s\n",
                        "\n",
                        "Test set: Average loss: 0.2743, Accuracy: 9019/10000 (90%)\n",
                        "\n",
                        "Train Epoch: 9 [1600/60000] Loss: 0.315983 Acc: 0.8750\n",
                        "Train Epoch: 9 [3200/60000] Loss: 0.185083 Acc: 0.9375\n",
                        "Train Epoch: 9 [4800/60000] Loss: 0.032578 Acc: 1.0000\n",
                        "Train Epoch: 9 [6400/60000] Loss: 0.121883 Acc: 0.9375\n",
                        "Train Epoch: 9 [8000/60000] Loss: 0.070406 Acc: 0.9375\n",
                        "Train Epoch: 9 [9600/60000] Loss: 0.020857 Acc: 1.0000\n",
                        "Train Epoch: 9 [11200/60000] Loss: 0.186853 Acc: 0.8750\n",
                        "Train Epoch: 9 [12800/60000] Loss: 0.158433 Acc: 1.0000\n",
                        "Train Epoch: 9 [14400/60000] Loss: 0.139180 Acc: 1.0000\n",
                        "Train Epoch: 9 [16000/60000] Loss: 0.190562 Acc: 0.9375\n",
                        "Train Epoch: 9 [17600/60000] Loss: 0.257993 Acc: 0.9375\n",
                        "Train Epoch: 9 [19200/60000] Loss: 0.352524 Acc: 0.8750\n",
                        "Train Epoch: 9 [20800/60000] Loss: 0.247917 Acc: 0.8750\n",
                        "Train Epoch: 9 [22400/60000] Loss: 0.060403 Acc: 1.0000\n",
                        "Train Epoch: 9 [24000/60000] Loss: 0.243590 Acc: 0.9375\n",
                        "Train Epoch: 9 [25600/60000] Loss: 0.115624 Acc: 1.0000\n",
                        "Train Epoch: 9 [27200/60000] Loss: 0.120244 Acc: 0.9375\n",
                        "Train Epoch: 9 [28800/60000] Loss: 0.250643 Acc: 0.9375\n",
                        "Train Epoch: 9 [30400/60000] Loss: 0.140334 Acc: 0.9375\n",
                        "Train Epoch: 9 [32000/60000] Loss: 0.382922 Acc: 0.8125\n",
                        "Train Epoch: 9 [33600/60000] Loss: 0.748817 Acc: 0.6875\n",
                        "Train Epoch: 9 [35200/60000] Loss: 0.289317 Acc: 0.8750\n",
                        "Train Epoch: 9 [36800/60000] Loss: 0.301999 Acc: 0.9375\n",
                        "Train Epoch: 9 [38400/60000] Loss: 0.205721 Acc: 0.8750\n",
                        "Train Epoch: 9 [40000/60000] Loss: 0.051490 Acc: 1.0000\n",
                        "Train Epoch: 9 [41600/60000] Loss: 0.072888 Acc: 1.0000\n",
                        "Train Epoch: 9 [43200/60000] Loss: 0.137609 Acc: 0.9375\n",
                        "Train Epoch: 9 [44800/60000] Loss: 0.602826 Acc: 0.8125\n",
                        "Train Epoch: 9 [46400/60000] Loss: 0.187137 Acc: 0.9375\n",
                        "Train Epoch: 9 [48000/60000] Loss: 0.163047 Acc: 0.9375\n",
                        "Train Epoch: 9 [49600/60000] Loss: 0.094071 Acc: 1.0000\n",
                        "Train Epoch: 9 [51200/60000] Loss: 0.712299 Acc: 0.8125\n",
                        "Train Epoch: 9 [52800/60000] Loss: 0.069611 Acc: 1.0000\n",
                        "Train Epoch: 9 [54400/60000] Loss: 0.032531 Acc: 1.0000\n",
                        "Train Epoch: 9 [56000/60000] Loss: 0.117922 Acc: 1.0000\n",
                        "Train Epoch: 9 [57600/60000] Loss: 0.090963 Acc: 1.0000\n",
                        "Train Epoch: 9 [59200/60000] Loss: 0.358621 Acc: 0.7500\n",
                        "Elapsed 331.22s, 33.12 s/epoch, 0.01 s/batch, ets 0.00s\n",
                        "\n",
                        "Test set: Average loss: 0.2807, Accuracy: 8968/10000 (90%)\n",
                        "\n",
                        "Total time: 336.82, Best Loss: 0.274\n"
                    ]
                }
            ],
            "source": [
                "model = LeNet()\n",
                "modelBN = LeNetBN() \n",
                "\n",
                "model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc = main(model)\n",
                "\n",
                "modelBN, epoch_train_loss_bn, epoch_train_acc_bn, epoch_test_loss_bn, epoch_test_acc_bn = main(modelBN)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:green\">10. Plot Loss</font> <a name=\"plot-loss\"></a>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {
                "lines_to_next_cell": 2
            },
            "outputs": [
                {
                    "data": {
                        "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAIjCAYAAAA0vUuxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAACp+ElEQVR4nOzdeXhM1/8H8Pcksm+ESCJCiCAhYgmpUFujCCnab6uVIooWRdsUpbVTWm2VopSiWlVq1yKWELXvQQmKRILELpGQdeb3x/nNlkwme+5M8n49zzy5c+fOvWdGMO8553yOTKFQKEBERERERET5MpG6AURERERERIaOwYmIiIiIiKgADE5EREREREQFYHAiIiIiIiIqAIMTERERERFRARiciIiIiIiICsDgREREREREVAAGJyIiIiIiogIwOBERERERERWAwYmIyACFhYXBw8OjWM+dNm0aZDJZ6TbIwMTFxUEmk+GXX34p92vLZDJMmzZNdf+XX36BTCZDXFxcgc/18PBAWFhYqbanJL8rRERUeAxORERFIJPJCnWLioqSuqmV3pgxYyCTyXD9+vV8j/niiy8gk8lw4cKFcmxZ0d29exfTpk1DdHS01E1RUYbXb7/9VuqmEBGViypSN4CIyJj89ttvWvd//fVX7N27N89+b2/vEl1n+fLlkMvlxXrupEmTMGHChBJdvyIIDQ3FwoULsXbtWkyZMkXnMX/88Qd8fX3RrFmzYl9nwIABePvtt2FhYVHscxTk7t27mD59Ojw8PNC8eXOtx0ryu0JERIXH4EREVATvvvuu1v3jx49j7969efbn9vz5c1hbWxf6OmZmZsVqHwBUqVIFVarwn/eAgAA0aNAAf/zxh87gdOzYMcTGxuKrr74q0XVMTU1hampaonOUREl+V4iIqPA4VI+IqJR16tQJTZs2xZkzZ9ChQwdYW1vj888/BwBs27YNPXv2RK1atWBhYQFPT0/MnDkTOTk5WufIPW9Fc1jUsmXL4OnpCQsLC7Ru3RqnTp3Seq6uOU4ymQyjRo3C1q1b0bRpU1hYWKBJkyaIiIjI0/6oqCj4+/vD0tISnp6e+Omnnwo9b+rQoUN48803UadOHVhYWMDd3R2ffPIJXrx4kef12dra4s6dO+jTpw9sbW3h5OSEsWPH5nkvnj59irCwMDg4OKBq1aoYNGgQnj59WmBbANHrdOXKFZw9ezbPY2vXroVMJsM777yDzMxMTJkyBa1atYKDgwNsbGzw8ssv48CBAwVeQ9ccJ4VCgVmzZqF27dqwtrZG586dcenSpTzPffz4McaOHQtfX1/Y2trC3t4ePXr0wPnz51XHREVFoXXr1gCAwYMHq4aDKud36ZrjlJaWhk8//RTu7u6wsLBAo0aN8O2330KhUGgdV5Tfi+K6f/8+hgwZAmdnZ1haWsLPzw+rV6/Oc9y6devQqlUr2NnZwd7eHr6+vliwYIHq8aysLEyfPh1eXl6wtLRE9erV0b59e+zdu7fU2kpEpA+/kiQiKgOPHj1Cjx498Pbbb+Pdd9+Fs7MzAPEh29bWFuHh4bC1tcX+/fsxZcoUpKSk4JtvvinwvGvXrsWzZ8/wwQcfQCaTYe7cuXj99ddx8+bNAnseDh8+jM2bN2PkyJGws7PDDz/8gDfeeAPx8fGoXr06AODcuXPo3r07XF1dMX36dOTk5GDGjBlwcnIq1OvesGEDnj9/jhEjRqB69eo4efIkFi5ciNu3b2PDhg1ax+bk5KBbt24ICAjAt99+i3379uG7776Dp6cnRowYAUAEkN69e+Pw4cMYPnw4vL29sWXLFgwaNKhQ7QkNDcX06dOxdu1atGzZUuvaf/75J15++WXUqVMHDx8+xM8//4x33nkHw4YNw7Nnz7BixQp069YNJ0+ezDM8riBTpkzBrFmzEBwcjODgYJw9exavvvoqMjMztY67efMmtm7dijfffBP16tXDvXv38NNPP6Fjx464fPkyatWqBW9vb8yYMQNTpkzB+++/j5dffhkAEBgYqPPaCoUCr732Gg4cOIAhQ4agefPm2L17N8aNG4c7d+7g+++/1zq+ML8XxfXixQt06tQJ169fx6hRo1CvXj1s2LABYWFhePr0KT766CMAwN69e/HOO+/glVdewddffw0AiImJwZEjR1THTJs2DXPmzMHQoUPRpk0bpKSk4PTp0zh79iy6du1aonYSERWKgoiIiu3DDz9U5P6ntGPHjgoAiqVLl+Y5/vnz53n2ffDBBwpra2tFenq6at+gQYMUdevWVd2PjY1VAFBUr15d8fjxY9X+bdu2KQAo/vrrL9W+qVOn5mkTAIW5ubni+vXrqn3nz59XAFAsXLhQtS8kJERhbW2tuHPnjmrff//9p6hSpUqec+qi6/XNmTNHIZPJFLdu3dJ6fQAUM2bM0Dq2RYsWilatWqnub926VQFAMXfuXNW+7Oxsxcsvv6wAoFi1alWBbWrdurWidu3aipycHNW+iIgIBQDFTz/9pDpnRkaG1vOePHmicHZ2Vrz33nta+wEopk6dqrq/atUqBQBFbGysQqFQKO7fv68wNzdX9OzZUyGXy1XHff755woAikGDBqn2paena7VLoRB/1hYWFlrvzalTp/J9vbl/V5Tv2axZs7SO+9///qeQyWRavwOF/b3QRfk7+c033+R7zPz58xUAFGvWrFHty8zMVLRt21Zha2urSElJUSgUCsVHH32ksLe3V2RnZ+d7Lj8/P0XPnj31tomIqCxxqB4RURmwsLDA4MGD8+y3srJSbT979gwPHz7Eyy+/jOfPn+PKlSsFnrdfv36oVq2a6r6y9+HmzZsFPjcoKAienp6q+82aNYO9vb3quTk5Odi3bx/69OmDWrVqqY5r0KABevToUeD5Ae3Xl5aWhocPHyIwMBAKhQLnzp3Lc/zw4cO17r/88star2Xnzp2oUqWKqgcKEHOKRo8eXaj2AGJe2u3bt/HPP/+o9q1duxbm5uZ48803Vec0NzcHAMjlcjx+/BjZ2dnw9/fXOcxPn3379iEzMxOjR4/WGt748ccf5znWwsICJibiv+KcnBw8evQItra2aNSoUZGvq7Rz506YmppizJgxWvs//fRTKBQK7Nq1S2t/Qb8XJbFz5064uLjgnXfeUe0zMzPDmDFjkJqaioMHDwIAqlatirS0NL3D7qpWrYpLly7hv//+K3G7iIiKg8GJiKgMuLm5qT6Ia7p06RL69u0LBwcH2Nvbw8nJSVVYIjk5ucDz1qlTR+u+MkQ9efKkyM9VPl/53Pv37+PFixdo0KBBnuN07dMlPj4eYWFhcHR0VM1b6tixI4C8r8/S0jLPEEDN9gDArVu34OrqCltbW63jGjVqVKj2AMDbb78NU1NTrF27FgCQnp6OLVu2oEePHlohdPXq1WjWrJlq/oyTkxN27NhRqD8XTbdu3QIAeHl5ae13cnLSuh4gQtr3338PLy8vWFhYoEaNGnBycsKFCxeKfF3N69eqVQt2dnZa+5WVHpXtUyro96Ikbt26BS8vL1U4zK8tI0eORMOGDdGjRw/Url0b7733Xp55VjNmzMDTp0/RsGFD+Pr6Yty4cQZfRp6IKhYGJyKiMqDZ86L09OlTdOzYEefPn8eMGTPw119/Ye/evao5HYUpKZ1f9TZFrkn/pf3cwsjJyUHXrl2xY8cOfPbZZ9i6dSv27t2rKmKQ+/WVVyW6mjVromvXrti0aROysrLw119/4dmzZwgNDVUds2bNGoSFhcHT0xMrVqxAREQE9u7diy5dupRpqe/Zs2cjPDwcHTp0wJo1a7B7927s3bsXTZo0KbcS42X9e1EYNWvWRHR0NLZv366an9WjRw+tuWwdOnTAjRs3sHLlSjRt2hQ///wzWrZsiZ9//rnc2klElRuLQxARlZOoqCg8evQImzdvRocOHVT7Y2NjJWyVWs2aNWFpaalzwVh9i8gqXbx4EdeuXcPq1asxcOBA1f6SVD2rW7cuIiMjkZqaqtXrdPXq1SKdJzQ0FBEREdi1axfWrl0Le3t7hISEqB7fuHEj6tevj82bN2sNr5s6dWqx2gwA//33H+rXr6/a/+DBgzy9OBs3bkTnzp2xYsUKrf1Pnz5FjRo1VPcLU9FQ8/r79u3Ds2fPtHqdlENBle0rD3Xr1sWFCxcgl8u1ep10tcXc3BwhISEICQmBXC7HyJEj8dNPP2Hy5MmqHk9HR0cMHjwYgwcPRmpqKjp06IBp06Zh6NCh5faaiKjyYo8TEVE5UX6zr/lNfmZmJn788UepmqTF1NQUQUFB2Lp1K+7evavaf/369TzzYvJ7PqD9+hQKhVZJ6aIKDg5GdnY2lixZotqXk5ODhQsXFuk8ffr0gbW1NX788Ufs2rULr7/+OiwtLfW2/cSJEzh27FiR2xwUFAQzMzMsXLhQ63zz58/Pc6ypqWmenp0NGzbgzp07WvtsbGwAoFBl2IODg5GTk4NFixZp7f/+++8hk8kKPV+tNAQHByMpKQnr169X7cvOzsbChQtha2urGsb56NEjreeZmJioFiXOyMjQeYytrS0aNGigepyIqKyxx4mIqJwEBgaiWrVqGDRoEMaMGQOZTIbffvutXIdEFWTatGnYs2cP2rVrhxEjRqg+gDdt2hTR0dF6n9u4cWN4enpi7NixuHPnDuzt7bFp06YSzZUJCQlBu3btMGHCBMTFxcHHxwebN28u8vwfW1tb9OnTRzXPSXOYHgD06tULmzdvRt++fdGzZ0/ExsZi6dKl8PHxQWpqapGupVyPas6cOejVqxeCg4Nx7tw57Nq1S6sXSXndGTNmYPDgwQgMDMTFixfx+++/a/VUAYCnpyeqVq2KpUuXws7ODjY2NggICEC9evXyXD8kJASdO3fGF198gbi4OPj5+WHPnj3Ytm0bPv74Y61CEKUhMjIS6enpefb36dMH77//Pn766SeEhYXhzJkz8PDwwMaNG3HkyBHMnz9f1SM2dOhQPH78GF26dEHt2rVx69YtLFy4EM2bN1fNh/Lx8UGnTp3QqlUrODo64vTp09i4cSNGjRpVqq+HiCg/DE5EROWkevXq+Pvvv/Hpp59i0qRJqFatGt5991288sor6Natm9TNAwC0atUKu3btwtixYzF58mS4u7tjxowZiImJKbDqn5mZGf766y+MGTMGc+bMgaWlJfr27YtRo0bBz8+vWO0xMTHB9u3b8fHHH2PNmjWQyWR47bXX8N1336FFixZFOldoaCjWrl0LV1dXdOnSReuxsLAwJCUl4aeffsLu3bvh4+ODNWvWYMOGDYiKiipyu2fNmgVLS0ssXboUBw4cQEBAAPbs2YOePXtqHff5558jLS0Na9euxfr169GyZUvs2LEDEyZM0DrOzMwMq1evxsSJEzF8+HBkZ2dj1apVOoOT8j2bMmUK1q9fj1WrVsHDwwPffPMNPv300yK/loJEREToXDDXw8MDTZs2RVRUFCZMmIDVq1cjJSUFjRo1wqpVqxAWFqY69t1338WyZcvw448/4unTp3BxcUG/fv0wbdo01RC/MWPGYPv27dizZw8yMjJQt25dzJo1C+PGjSv110REpItMYUhfdRIRkUHq06cPS0ETEVGlxjlORESk5cWLF1r3//vvP+zcuROdOnWSpkFEREQGgD1ORESkxdXVFWFhYahfvz5u3bqFJUuWICMjA+fOncuzNhEREVFlwTlORESkpXv37vjjjz+QlJQECwsLtG3bFrNnz2ZoIiKiSo09TkRERERERAWQfI7T4sWL4eHhAUtLSwQEBODkyZP5HpuVlYUZM2bA09MTlpaW8PPz01nJh4iIiIiIqDRJGpzWr1+P8PBwTJ06FWfPnoWfnx+6deuG+/fv6zx+0qRJ+Omnn7Bw4UJcvnwZw4cPR9++fXHu3LlybjkREREREVUmkg7VCwgIQOvWrVWrm8vlcri7u2P06NF51rAAgFq1auGLL77Ahx9+qNr3xhtvwMrKCmvWrCnUNeVyOe7evQs7OzvIZLLSeSFERERERGR0FAoFnj17hlq1aqnWjcuPZMUhMjMzcebMGUycOFG1z8TEBEFBQTh27JjO52RkZMDS0lJrn5WVFQ4fPpzvdTIyMpCRkaG6f+fOHfj4+JSw9UREREREVFEkJCSgdu3aeo+RLDg9fPgQOTk5cHZ21trv7Oyc7+r03bp1w7x589ChQwd4enoiMjISmzdvRk5OTr7XmTNnDqZPn55nf0JCAuzt7Uv2IoiIiIiIyGilpKTA3d0ddnZ2BR5rVOXIFyxYgGHDhqFx48aQyWTw9PTE4MGDsXLlynyfM3HiRISHh6vuK98ce3t7BiciIiIiIirUFB7JikPUqFEDpqamuHfvntb+e/fuwcXFRedznJycsHXrVqSlpeHWrVu4cuUKbG1tUb9+/XyvY2FhoQpJDEtERERERFQckgUnc3NztGrVCpGRkap9crkckZGRaNu2rd7nWlpaws3NDdnZ2di0aRN69+5d1s0lIiIiIqJKTNKheuHh4Rg0aBD8/f3Rpk0bzJ8/H2lpaRg8eDAAYODAgXBzc8OcOXMAACdOnMCdO3fQvHlz3LlzB9OmTYNcLsf48eOlfBlERERERFTBSRqc+vXrhwcPHmDKlClISkpC8+bNERERoSoYER8fr1UWMD09HZMmTcLNmzdha2uL4OBg/Pbbb6hatapEr4CIiIio/CgUCmRnZ+stjEVE2szMzGBqalri80i6jpMUUlJS4ODggOTkZM53IiIiIqORmZmJxMREPH/+XOqmEBkVmUyG2rVrw9bWNs9jRckGRlVVj4iIiKgyksvliI2NhampKWrVqgVzc/NCVQEjquwUCgUePHiA27dvw8vLq0Q9TwxORERERAYuMzMTcrkc7u7usLa2lro5REbFyckJcXFxyMrKKlFwkqyqHhEREREVjebcbyIqnNLqneXfPiIiIiIiogIwOBERERERERWAwYmIiIiIjIaHhwfmz58v+Tmo8mFxCCIiIiIqM506dULz5s1LLaicOnUKNjY2pXIuoqJgcCIiIiIiSSkUCuTk5KBKlYI/mjo5OZVDi4jy4lA9IiIiImOkUABpadLcFIpCNTEsLAwHDx7EggULIJPJIJPJEBcXh6ioKMhkMuzatQutWrWChYUFDh8+jBs3bqB3795wdnaGra0tWrdujX379mmdM/cwO5lMhp9//hl9+/aFtbU1vLy8sH379iK9lfHx8ejduzdsbW1hb2+Pt956C/fu3VM9fv78eXTu3Bl2dnawt7dHq1atcPr0aQDArVu3EBISgmrVqsHGxgZNmjTBzp07i3R9Mg7scSIiIiIyRs+fA7a20lw7NRUoxHC5BQsW4Nq1a2jatClmzJgBQL2mDgBMmDAB3377LerXr49q1aohISEBwcHB+PLLL2FhYYFff/0VISEhuHr1KurUqZPvdaZPn465c+fim2++wcKFCxEaGopbt27B0dGxwDbK5XJVaDp48CCys7Px4Ycfol+/foiKigIAhIaGokWLFliyZAlMTU0RHR0NMzMzAMCHH36IzMxM/PPPP7CxscHly5dhK9WfC5UpBiciIiIiKhMODg4wNzeHtbU1XFxc8jw+Y8YMdO3aVXXf0dERfn5+qvszZ87Eli1bsH37dowaNSrf64SFheGdd94BAMyePRs//PADTp48ie7duxfYxsjISFy8eBGxsbFwd3cHAPz6669o0qQJTp06hdatWyM+Ph7jxo1D48aNAQBeXl6q58fHx+ONN96Ar68vAKB+/foFXpOME4OTlB48AHbtAvz9AR8fqVtDRERExsTaWvT8SHXtUuDv7691PzU1FdOmTcOOHTuQmJiI7OxsvHjxAvHx8XrP06xZM9W2jY0N7O3tcf/+/UK1ISYmBu7u7qrQBAA+Pj6oWrUqYmJi0Lp1a4SHh2Po0KH47bffEBQUhDfffBOenp4AgDFjxmDEiBHYs2cPgoKC8MYbb2i1hyoOznGS0kcfAYMGAb/9JnVLiIiIyNjIZGK4nBQ3maxUXkLu6nhjx47Fli1bMHv2bBw6dAjR0dHw9fVFZmam3vMoh82p3xoZ5HJ5qbQRAKZNm4ZLly6hZ8+e2L9/P3x8fLBlyxYAwNChQ3Hz5k0MGDAAFy9ehL+/PxYuXFhq1ybDweAkpZ49xc8dO6RtBxEREVEZMTc3R05OTqGOPXLkCMLCwtC3b1/4+vrCxcVFNR+qrHh7eyMhIQEJCQmqfZcvX8bTp0/hozEiqGHDhvjkk0+wZ88evP7661i1apXqMXd3dwwfPhybN2/Gp59+iuXLl5dpm0kaDE5S6t4dMDEBLl4ECuiCJiIiIjJGHh4eOHHiBOLi4vDw4UO9PUFeXl7YvHkzoqOjcf78efTv379Ue450CQoKgq+vL0JDQ3H27FmcPHkSAwcORMeOHeHv748XL15g1KhRiIqKwq1bt3DkyBGcOnUK3t7eAICPP/4Yu3fvRmxsLM6ePYsDBw6oHqOKhcFJStWrA23bim32OhEREVEFNHbsWJiamsLHxwdOTk565yvNmzcP1apVQ2BgIEJCQtCtWze0bNmyTNsnk8mwbds2VKtWDR06dEBQUBDq16+P9evXAwBMTU3x6NEjDBw4EA0bNsRbb72FHj16YPr06QCAnJwcfPjhh/D29kb37t3RsGFD/Pjjj2XaZpKGTKEoZCH+CiIlJQUODg5ITk6Gvb291M0BvvoKmDhRDNv7+2+pW0NEREQGKD09HbGxsahXrx4sLS2lbg6RUdH396co2YA9TlJTznOKjBTrMRARERERkcFhcJJa06aAuzuQng4cOCB1a4iIiIiISAcGJ6nJZECvXmKbQ/WIiIiIiAwSg5Mh0CxLXrmmnBERERERGQUGJ0PQpQtgZQUkJIjS5EREREREZFAYnAyBlZUITwDLkhMRERERGSAGJ0OhnOfE4EREREREZHAYnAxFcLD4eewY8OiRtG0hIiIiIiItDE6Gok4doFkzQC4HIiKkbg0REREREWlgcDIkyup6LEtOREREpOLh4YH58+er7stkMmzdujXf4+Pi4iCTyRAdHV2i65bWeQoSFhaGPn36lOk1qOQYnAyJMjhFRADZ2dK2hYiIiMhAJSYmokePHqV6Tl3hxd3dHYmJiWjatGmpXouME4OTIXnpJcDREXj6FDh6VOrWEBERERkkFxcXWFhYlPl1TE1N4eLigipVqpT5tcjwMTgZElNTQPntCavrERERkR4KBZCWJs1NoShcG5ctW4ZatWpBLpdr7e/duzfee+89AMCNGzfQu3dvODs7w9bWFq1bt8a+ffv0njf3UL2TJ0+iRYsWsLS0hL+/P86dO6d1fE5ODoYMGYJ69erBysoKjRo1woIFC1SPT5s2DatXr8a2bdsgk8kgk8kQFRWlc6jewYMH0aZNG1hYWMDV1RUTJkxAtsZIoU6dOmHMmDEYP348HB0d4eLigmnTphXuDft/GRkZGDNmDGrWrAlLS0u0b98ep06dUj3+5MkThIaGwsnJCVZWVvDy8sKqVasAAJmZmRg1ahRcXV1haWmJunXrYs6cOUW6PunG+GxoevUCfv9dzHP6+mupW0NEREQG6vlzwNZWmmunpgI2NgUf9+abb2L06NE4cOAAXnnlFQDA48ePERERgZ07d/7/uVIRHByML7/8EhYWFvj1118REhKCq1evok6dOoVoSyp69eqFrl27Ys2aNYiNjcVHH32kdYxcLkft2rWxYcMGVK9eHUePHsX7778PV1dXvPXWWxg7dixiYmKQkpKiCiCOjo64e/eu1nnu3LmD4OBghIWF4ddff8WVK1cwbNgwWFpaaoWj1atXIzw8HCdOnMCxY8cQFhaGdu3aoWvXrgW/aQDGjx+PTZs2YfXq1ahbty7mzp2Lbt264fr163B0dMTkyZNx+fJl7Nq1CzVq1MD169fx4sULAMAPP/yA7du3488//0SdOnWQkJCAhISEQl2X9GNwMjTduomep8uXgbg4wMND6hYRERERFUu1atXQo0cPrF27VhWcNm7ciBo1aqBz584AAD8/P/j5+ameM3PmTGzZsgXbt2/HqFGjCrzG2rVrIZfLsWLFClhaWqJJkya4ffs2RowYoTrGzMwM06dPV92vV68ejh07hj///BNvvfUWbG1tYWVlhYyMDLi4uOR7rR9//BHu7u5YtGgRZDIZGjdujLt37+Kzzz7DlClTYGIiBnM1a9YMU6dOBQB4eXlh0aJFiIyMLFRwSktLw5IlS/DLL7+o5nEtX74ce/fuxYoVKzBu3DjEx8ejRYsW8Pf3ByCKZyjFx8fDy8sL7du3h0wmQ926dQu8JhUOh+oZmmrVgHbtxDaH6xEREVE+rK1Fz48UN2vrwrczNDQUmzZtQkZGBgDg999/x9tvv60KGampqRg7diy8vb1RtWpV2NraIiYmBvHx8YU6f0xMDJo1awZLS0vVvrZt2+Y5bvHixWjVqhWcnJxga2uLZcuWFfoamtdq27YtZDKZal+7du2QmpqK27dvq/Y1a9ZM63murq64f/9+oa5x48YNZGVloZ3y8yBE8GvTpg1iYmIAACNGjMC6devQvHlzjB8/Hkc15saHhYUhOjoajRo1wpgxY7Bnz54ivUbKH4OTIWJZciIiIiqATCaGy0lx08gNBQoJCYFCocCOHTuQkJCAQ4cOITQ0VPX42LFjsWXLFsyePRuHDh1CdHQ0fH19kZmZWWrv1bp16zB27FgMGTIEe/bsQXR0NAYPHlyq19BkZmamdV8mk+WZ51USPXr0wK1bt/DJJ5/g7t27eOWVVzB27FgAQMuWLREbG4uZM2fixYsXeOutt/C///2v1K5dmTE4GaJevcTPAwfEDEwiIiIiI2VpaYnXX38dv//+O/744w80atQILVu2VD1+5MgRhIWFoW/fvvD19YWLiwvi4uIKfX5vb29cuHAB6enpqn3Hjx/XOubIkSMIDAzEyJEj0aJFCzRo0AA3btzQOsbc3Bw5OTkFXuvYsWNQaFTHOHLkCOzs7FC7du1Ct1kfT09PmJub48iRI6p9WVlZOHXqFHx8fFT7nJycMGjQIKxZswbz58/HsmXLVI/Z29ujX79+WL58OdavX49Nmzbh8ePHpdK+yozByRB5e4u5TRkZQGSk1K0hIiIiKpHQ0FDs2LEDK1eu1OptAsQcoM2bNyM6Ohrnz59H//79i9Q7079/f8hkMgwbNgyXL1/Gzp078e233+a5xunTp7F7925cu3YNkydP1qpSB4h5QhcuXMDVq1fx8OFDZGVl5bnWyJEjkZCQgNGjR+PKlSvYtm0bpk6divDwcNXQw5KysbHBiBEjMG7cOERERODy5csYNmwYnj9/jiFDhgAApkyZgm3btuH69eu4dOkS/v77b3h7ewMA5s2bhz/++ANXrlzBtWvXsGHDBri4uKBq1aql0r7KjMHJEMlk6uF6nOdERERERq5Lly5wdHTE1atX0b9/f63H5s2bh2rVqiEwMBAhISHo1q2bVo9UQWxtbfHXX3/h4sWLaNGiBb744gt8nasy8QcffIDXX38d/fr1Q0BAAB49eoSRI0dqHTNs2DA0atQI/v7+cHJy0urxUXJzc8POnTtx8uRJ+Pn5Yfjw4RgyZAgmTZpUhHejYF999RXeeOMNDBgwAC1btsT169exe/duVKtWDYDoHZs4cSKaNWuGDh06wNTUFOvWrQMA2NnZYe7cufD390fr1q0RFxeHnTt3llqwq8xkCkVhK/FXDCkpKXBwcEBycjLs7e2lbk7+IiLEmk5ubkBCQtEGExMREVGFkp6ejtjYWNSrV0+rCAIRFUzf35+iZANGT0PVqZMoWXPnDnD+vNStISIiIiKq1BicDJWlJRAUJLZZXY+IiIiISFIMToaM85yIiIiIiAwCg5MhUwanEyeABw+kbQsRERERUSXG4GTI3NyA5s0BhQLYtUvq1hARERERVVoMToZOuRguh+sREREREUmGwcnQKYfrRUQAOhZiIyIiIiKissfgZOhatwZq1ABSUgAdC7EREREREVHZY3AydKamQHCw2GZZciIiIiIiSTA4GQOWJSciIiICAHh4eGD+/PmSn6M8TJs2Dc2bN9d7TFxcHGQyGaKjo8ulTZUZg5MxePVVoEoV4MoV4MYNqVtDREREVGidOnXCxx9/XGrnO3XqFN5///1SO58hGzt2LCIjI1X3w8LC0KdPn1I5t4eHB2QyGWQyGUxNTVGrVi0MGTIET548UR0TFRUFmUyGJk2aICcnR+v5VatWxS+//FIqbTEWDE7GoGpVoH17sc1eJyIiIqpgFAoFsrOzC3Wsk5MTrK2ty7hFhsHW1hbVq1cvs/PPmDEDiYmJiI+Px++//45//vkHY8aMyXPczZs38euvv5ZZO4wFg5OxYFlyIiIi0iUtLf9benrhj33xonDHFkFYWBgOHjyIBQsWqHo34uLiVD0Zu3btQqtWrWBhYYHDhw/jxo0b6N27N5ydnWFra4vWrVtj3759WufMPcxOJpPh559/Rt++fWFtbQ0vLy9s3769SO2Mj49H7969YWtrC3t7e7z11lu4d++e6vHz58+jc+fOsLOzg729PVq1aoXTp08DAG7duoWQkBBUq1YNNjY2aNKkCXbu3KnzOosWLULTpk1V97du3QqZTIalS5eq9gUFBWHSpEkAtIfqTZs2DatXr8a2bdtU72VUVJTqeTdv3kTnzp1hbW0NPz8/HDt2rMDXbWdnBxcXF7i5uaFz584YNGgQzp49m+e40aNHY+rUqcjIyCjwnBUZg5OxUM5ziooCUlMlbQoREREZEFvb/G9vvKF9bM2a+R/bo4f2sR4euo8rggULFqBt27YYNmwYEhMTkZiYCHd3d9XjEyZMwFdffYWYmBg0a9YMqampCA4ORmRkJM6dO4fu3bsjJCQE8fHxeq8zffp0vPXWW7hw4QKCg4MRGhqKx48fF6qNcrkcvXv3xuPHj3Hw4EHs3bsXN2/eRL9+/VTHhIaGonbt2jh16hTOnDmDCRMmwMzMDADw4YcfIiMjA//88w8uXryIr7/+Grb5vE8dO3bE5cuX8eDBAwDAwYMHUaNGDVUAysrKwrFjx9CpU6c8zx07dizeeustdO/eXfVeBgYGqh7/4osvMHbsWERHR6Nhw4Z45513Ct2LBwB37tzBX3/9hYCAgDyPffzxx8jOzsbChQsLfb6KiMHJWDRqBHh6ApmZQK5vXoiIiIgMkYODA8zNzWFtbQ0XFxe4uLjA1NRU9fiMGTPQtWtXeHp6wtHREX5+fvjggw/QtGlTeHl5YebMmfD09CywByksLAzvvPMOGjRogNmzZyM1NRUnT54sVBsjIyNx8eJFrF27Fq1atUJAQAB+/fVXHDx4EKdOnQIgeqSCgoLQuHFjeHl54c0334Sfn5/qsXbt2sHX1xf169dHr1690KFDB53Xatq0KRwdHXHw4EEAYg7Rp59+qrp/8uRJZGVlaQUiJVtbW1hZWcHCwkL1Xpqbm6seHzt2LHr27ImGDRti+vTpuHXrFq5fv673tX/22Weq89auXRsymQzz5s3Lc5y1tTWmTp2KOXPmIDk5uRDvasXE4GQsZDJ1rxPLkhMREZFSamr+t02btI+9fz//Y3ft0j42Lk73caXI398/10tJxdixY+Ht7Y2qVavC1tYWMTExBfY4NWvWTLVtY2MDe3t73L9/v1BtiImJgbu7u1ZPmI+PD6pWrYqYmBgAQHh4OIYOHYqgoCB89dVXuKFRrGvMmDGYNWsW2rVrh6lTp+LChQv5Xksmk6FDhw6IiorC06dPcfnyZYwcORIZGRm4cuUKDh48iNatWxdrDpfme+Dq6goABb4H48aNQ3R0NC5cuKAqQtGzZ888hSAAYMiQIahevTq+/vrrIretopA8OC1evBgeHh6wtLREQEBAgd8OzJ8/H40aNYKVlRXc3d3xySefID33+N2KShmcdu4E5HJp20JERESGwcYm/5ulZeGPtbIq3LGl2nTt840dOxZbtmzB7NmzcejQIURHR8PX1xeZmZl6z6McNqckk8kgL8XPStOmTcOlS5fQs2dP7N+/Hz4+PtiyZQsAYOjQobh58yYGDBiAixcvwt/fX++Qtk6dOiEqKgqHDh1CixYtYG9vrwpTBw8eRMeOHYvVRs33QCaTAUCB70GNGjXQoEEDeHl5oUuXLpg/fz6OHj2KAwcO5Dm2SpUq+PLLL7FgwQLcvXu3WG00dpIGp/Xr1yM8PBxTp07F2bNn4efnh27duuWbjteuXYsJEyZg6tSpiImJwYoVK7B+/Xp8/vnn5dxyiXTsKP7BSkwEzp2TujVEREREBTI3N9fZg6HLkSNHEBYWhr59+8LX1xcuLi6Ii4sr0/Z5e3sjISEBCQkJqn2XL1/G06dP4ePjo9rXsGFDfPLJJ9izZw9ef/11rFq1SvWYu7s7hg8fjs2bN+PTTz/F8uXL872ecp7Thg0bVHOZOnXqhH379uHIkSM65zcpFeW9LA7lMMoXuQuF/L8333wTTZo0wfTp08usDYZM0uA0b948DBs2DIMHD4aPjw+WLl0Ka2trrFy5UufxR48eRbt27dC/f394eHjg1VdfxTvvvFPoMaxGz8IC6NpVbLO6HhERERkBDw8PnDhxAnFxcXj48KHeXhAvLy9s3rwZ0dHROH/+PPr371+qPUe6BAUFwdfXF6GhoTh79ixOnjyJgQMHomPHjvD398eLFy8watQoREVF4datWzhy5AhOnToFb29vAKJwwu7duxEbG4uzZ8/iwIEDqsd0adasGapVq4a1a9dqBaetW7ciIyMD7dq1y/e5Hh4euHDhAq5evYqHDx8iKyurRK/92bNnSEpKQmJiIk6ePIlx48bByclJ5xwrpa+++gorV65EWhErLFYEkgWnzMxMnDlzBkFBQerGmJggKCgo3/KJgYGBOHPmjCoo3bx5Ezt37kRwcHC+18nIyEBKSorWzagpy5JznhMREREZgbFjx8LU1BQ+Pj5wcnLSO19p3rx5qFatGgIDAxESEoJu3bqhZcuWZdo+mUyGbdu2oVq1aujQoQOCgoJQv359rF+/HoDohXn06BEGDhyIhg0b4q233kKPHj1UvS45OTn48MMP4e3tje7du6Nhw4b48ccf9V7v5ZdfhkwmQ/v/X6ezWbNmsLe3h7+/f57hi5qGDRuGRo0awd/fH05OTjhy5EiJXvuUKVPg6uqKWrVqoVevXrCxscGePXv0rh3VpUsXdOnSpUgV+yoKmUKhUEhx4bt378LNzQ1Hjx5F27ZtVfvHjx+PgwcP4sSJEzqf98MPP2Ds2LGqhdKGDx+OJUuW5HudadOm6exOTE5Ohr29fclfSHlLTARq1RLbSUmAs7O07SEiIqIyl56ejtjYWNSrVw+WuectEZFe+v7+pKSkwMHBoVDZQPLiEEURFRWF2bNn48cff8TZs2exefNm7NixAzNnzsz3ORMnTkRycrLqpjl+1Si5ugKtWont3NVviIiIiIioTFSR6sI1atSAqamp1qrMAHDv3j24uLjofM7kyZMxYMAADB06FADg6+uLtLQ0vP/++/jiiy9gYpI3B1pYWMDCwqL0X4CUevYEzpwRw/XCwqRuDRERERFRhSdZj5O5uTlatWqlqhkPiJKJkZGRWkP3ND1//jxPOFJW/5BoxKE0lPOc9uwRC+ISEREREVGZknSoXnh4OJYvX47Vq1cjJiYGI0aMQFpaGgYPHgwAGDhwICZOnKg6PiQkBEuWLMG6desQGxuLvXv3YvLkyQgJCdFahbrCa9VKzG169gw4dEjq1hARERERVXiSDdUDgH79+uHBgweYMmUKkpKS0Lx5c0RERMD5/wsexMfHa/UwTZo0CTKZDJMmTcKdO3fg5OSEkJAQfPnll1K9BGmYmAA9egC//CLKkr/yitQtIiIiIiKq0CSrqieVolTOMGibNgH/+x/g5QVcuyZ1a4iIiKgMsaoeUfFVyqp6pKFrV8DMDPjvPwYnIiIiIqIyxuBkrOztgQ4dxPaOHdK2hYiIiIiogmNwMmY9e4qfDE5ERERERGWKwcmYKcuS//MPkJIibVuIiIiIyoiHhwfmz5+vui+TybB169Z8j4+Li4NMJkN0dHSJrlta5ylIWFgY+vTpU6bXKA1RUVGQyWR4+vSp3uNy/3lVFAxOxszLS9yysoC9e6VuDREREVG5SExMRI8ePUr1nLrCi7u7OxITE9G0adNSvZaxCgwMRGJiIhwcHAAAv/zyC6pWrVoq5w4LC4NMJlPdqlevju7du+PChQtax8lkMlhaWuLWrVta+/v06YOwsLBSaUt+GJyMnbLXicP1iIiIqJJwcXGBhYVFmV/H1NQULi4uqFJF0hV8DIa5uTlcXFwgk8nK5Pzdu3dHYmIiEhMTERkZiSpVqqCX8rOuBplMhilTppRJG/RhcDJ2mvOc5HJp20JERETlLi0t/1t6euGPffGicMcWxbJly1CrVi3Ic31G6d27N9577z0AwI0bN9C7d284OzvD1tYWrVu3xr59+/SeN/dQvZMnT6JFixawtLSEv78/zp07p3V8Tk4OhgwZgnr16sHKygqNGjXCggULVI9PmzYNq1evxrZt21Q9HlFRUTqH6h08eBBt2rSBhYUFXF1dMWHCBGRnZ6se79SpE8aMGYPx48fD0dERLi4umDZtWpHet4yMDIwZMwY1a9aEpaUl2rdvj1OnTqkef/LkCUJDQ+Hk5AQrKyt4eXlh1apVAIDMzEyMGjUKrq6usLS0RN26dTFnzhyd1/n3339hYmKCBw8eAAAeP34MExMTvP3226pjZs2ahfbt2wPQHqoXFRWFwYMHIzk5WfWeab7O58+f47333oOdnR3q1KmDZcuWFfi6LSws4OLiAhcXFzRv3hwTJkxAQkKCqn1Ko0aNwpo1a/Dvv/8W7g0tJQxOxu7llwE7O+D+feDMGalbQ0REROXM1jb/2xtvaB9bs2b+x+Ye+ebhofu4onjzzTfx6NEjHDhwQLXv8ePHiIiIQGhoKAAgNTUVwcHBiIyMxLlz59C9e3eEhIQgPj6+UNdITU1Fr1694OPjgzNnzmDatGkYO3as1jFyuRy1a9fGhg0bcPnyZUyZMgWff/45/vzzTwDA2LFj8dZbb2n1eAQGBua51p07dxAcHIzWrVvj/PnzWLJkCVasWIFZs2ZpHbd69WrY2NjgxIkTmDt3LmbMmIG9RZhWMX78eGzatAmrV6/G2bNn0aBBA3Tr1g2PHz8GAEyePBmXL1/Grl27EBMTgyVLlqBGjRoAgB9++AHbt2/Hn3/+iatXr+L333+Hh4eHzus0adIE1atXx8GDBwEAhw4d0roPiKDYqVOnPM8NDAzE/PnzYW9vr3rPNN/37777ThViR44ciREjRuDq1auFfg9SU1OxZs0aNGjQANWrV9d6rF27dujVqxcmTJhQ6POVBgYnY2duDrz6qtj++29p20JERESkoVq1aujRowfWrl2r2rdx40bUqFEDnTt3BgD4+fnhgw8+QNOmTeHl5YWZM2fC09MT27dvL9Q11q5dC7lcjhUrVqBJkybo1asXxo0bp3WMmZkZpk+fDn9/f9SrVw+hoaEYPHiwKjjZ2trCyspKq8fD3Nw8z7V+/PFHuLu7Y9GiRWjcuDH69OmD6dOn47vvvtPqVWvWrBmmTp0KLy8vDBw4EP7+/oiMjCzU60lLS8OSJUvwzTffoEePHvDx8cHy5cthZWWFFStWAADi4+PRokUL+Pv7w8PDA0FBQQgJCVE95uXlhfbt26Nu3bpo37493nnnHZ3Xkslk6NChA6KiogBA1YuUkZGBK1euICsrC0ePHkXHjh3zPNfc3BwODg6QyWSq98xWI1kHBwdj5MiRaNCgAT777DPUqFFDK0Dr8vfff8PW1ha2traws7PD9u3bsX79epiY5I0sc+bMQUREBA4dOlSo97U0MDhVBCxLTkREVGmlpuZ/27RJ+9j79/M/dtcu7WPj4nQfV1ShoaHYtGkTMjIyAAC///473n77bdWH4dTUVIwdOxbe3t6oWrUqbG1tERMTU+gep5iYGDRr1gyWlpaqfW3bts1z3OLFi9GqVSs4OTnB1tYWy5YtK/Q1NK/Vtm1brTk+7dq1Q2pqKm7fvq3a16xZM63nubq64v79+4W6xo0bN5CVlYV27dqp9pmZmaFNmzaIiYkBAIwYMQLr1q1D8+bNMX78eBw9elR1bFhYGKKjo9GoUSOMGTMGe/bs0Xu9jh07qoLTwYMH0aVLF1WYOnXqVJ62FJbme6AMVwW9B507d0Z0dDSio6Nx8uRJdOvWDT169MhTCAIAfHx8MHDgwHLtdWJwqgiCg8XPM2eAxERp20JERETlysYm/5tGlijwWCurwh1bVCEhIVAoFNixYwcSEhJw6NAh1TA9QAyT27JlC2bPno1Dhw4hOjoavr6+yMzMLMa7odu6deswduxYDBkyBHv27EF0dDQGDx5cqtfQZGZmpnVfJpPlmedVEsow8cknn+Du3bt45ZVXVMPkWrZsidjYWMycORMvXrzAW2+9hf/973/5nqtTp064fPky/vvvP1y+fBnt27dHp06dEBUVhYMHD8Lf3x/W1tZFbmNx3gMbGxs0aNAADRo0QOvWrfHzzz8jLS0Ny5cv13n89OnTcfbsWb2l6UsTg1NF4OwMtG4ttnfulLYtRERERBosLS3x+uuv4/fff8cff/yBRo0aoWXLlqrHjxw5grCwMPTt2xe+vr5wcXFBXFxcoc/v7e2NCxcuIF2jEsbx48e1jjly5AgCAwMxcuRItGjRAg0aNMCNGze0jjE3N0dOTk6B1zp27BgUCoXWue3s7FC7du1Ct1kfT09PmJub48iRI6p9WVlZOHXqFHx8fFT7nJycMGjQIKxZswbz58/XKr5gb2+Pfv36Yfny5Vi/fj02bdqkmh+Vm6+vL6pVq4ZZs2ahefPmsLW1RadOnXDw4EFERUXpnN+kVJj3rCRkMhlMTEzwInflkv/n7u6OUaNG4fPPPy/TdigxOFUULEtOREREBio0NBQ7duzAypUrtXqbAMDLywubN29GdHQ0zp8/j/79+xepd6Z///6QyWQYNmwYLl++jJ07d+Lbb7/Nc43Tp09j9+7duHbtGiZPnqxVpQ4Qi7ZeuHABV69excOHD5GVlZXnWiNHjkRCQgJGjx6NK1euYNu2bZg6dSrCw8N1zsMpDhsbG4wYMQLjxo1DREQELl++jGHDhuH58+cYMmQIAGDKlCnYtm0brl+/jkuXLuHvv/+Gt7c3AGDevHn4448/cOXKFVy7dg0bNmyAi4tLvustKec5/f7776qQ1KxZM2RkZCAyMlLn/CYlDw8PpKamIjIyEg8fPsTz589L9NozMjKQlJSEpKQkxMTEYPTo0UhNTVXN39Jl4sSJuHv3boGVGEsDg1NFoZzntGcP8P9jiImIiIgMQZcuXeDo6IirV6+if//+Wo/NmzcP1apVQ2BgIEJCQtCtWzetHqmC2Nra4q+//sLFixfRokULfPHFF/j666+1jvnggw/w+uuvo1+/fggICMCjR48wcuRIrWOGDRuGRo0awd/fH05OTlo9Pkpubm7YuXMnTp48CT8/PwwfPhxDhgzBpEmTivBuFOyrr77CG2+8gQEDBqBly5a4fv06du/ejWrVqgEQPT0TJ05Es2bN0KFDB5iammLdunUAADs7O8ydOxf+/v5o3bo14uLisHPnTr3BrmPHjsjJyVEFJxMTE3To0AEymUzv/KbAwEAMHz4c/fr1g5OTE+bOnVui1x0REQFXV1e4uroiICAAp06dwoYNG/T2ejk6OuKzzz7T6nEsKzKFZl9jJZCSkgIHBwckJyfD3t5e6uaUHrkcqF1bzHHaswfo2lXqFhEREVEpSU9PR2xsLOrVq6dVBIGICqbv709RsgF7nCoKExN1kQiWJSciIiIiKlUMThWJcp7T338DlasjkYiIiIioTDE4VSRBQWJB3Js3gSKszExERERERPoxOFUktraAsvIJq+sREREREZUaBqeKRnO4HhEREVUolaymF1GpKK2/NwxOFY2yLPnhw0BysrRtISIiolJhZmYGACVeJ4eoMsrMzAQAmJqalug8VUqjMWRAPD2Bxo2BK1dEWfI335S6RURERFRCpqamqFq1Ku7fvw8AsLa2hkwmk7hVRIZPLpfjwYMHsLa2RpUqJYs+DE4VUc+eIjj9/TeDExERUQXh4uICAKrwRESFY2Jigjp16pT4ywYGp4qoVy/gu++AXbuAnByghN2SREREJD2ZTAZXV1fUrFkTWVlZUjeHyGiYm5vDxKTkM5QYnCqidu0ABwfgwQPg1CngpZekbhERERGVElNT0xLP1SCiomNxiIrIzAzo1k1ssyw5EREREVGJMThVVMrqeixLTkRERERUYgxOFVWPHoBMBkRHA3fuSN0aIiIiIiKjxuBUUTk5AQEBYpvD9YiIiIiISoTBqSJTDtdjcCIiIiIiKhEGp4qsVy/xc98+ID1d2rYQERERERkxBqeKzM8PcHMDnj8HoqKkbg0RERERkdFicKrIZDIO1yMiIiIiKgUMThWdZllyhULathARERERGSkGp4rulVcACwsgLg6IiZG6NURERERERonBqaKzsQE6dxbbXAyXiIiIiKhYGJwqA85zIiIiIiIqEQanykAZnI4cAZ48kbYtRERERERGiMGpMqhXD/DxAXJygN27pW4NEREREZHRYXCqLJSL4XK4HhERERFRkTE4VRbK4Xq7domeJyIiIiIiKjQGp8oiMBCoWhV49Ag4cULq1hARERERGRUGp8qiShWge3exzbLkRERERERFwuBUmXCeExERERFRsTA4VSbduwMmJsCFC0B8vNStISIiIiIyGgxOlUn16sBLL4ntnTulbQsRERERkRFhcKpslMP1OM+JiIiIiKjQGJwqG2VZ8v37gRcvpG0LEREREZGRYHCqbHx9AXd3EZoOHJC6NURERERERoHBqbKRydS9ThyuR0RERERUKAxOlZFmWXKFQtq2EBEREREZAQanyqhzZ8DSUpQk//dfqVtDRERERGTwGJwqI2tr4JVXxDYXwyUiIiIiKhCDU2XFeU5ERERERIXG4FRZKYPTsWPAo0fStoWIiIiIyMAZRHBavHgxPDw8YGlpiYCAAJw8eTLfYzt16gSZTJbn1lMZBKhw6tQRpcnlciAiQurWEBEREREZNMmD0/r16xEeHo6pU6fi7Nmz8PPzQ7du3XD//n2dx2/evBmJiYmq27///gtTU1O8+eab5dzyCkAZNjnPiYiIiIhIL8mD07x58zBs2DAMHjwYPj4+WLp0KaytrbFy5Uqdxzs6OsLFxUV127t3L6ytrRmcikNZljwiAsjOlrYtREREREQGTNLglJmZiTNnziAoKEi1z8TEBEFBQTh27FihzrFixQq8/fbbsLGx0fl4RkYGUlJStG70/156CXB0BJ48EXOdiIiIiIhIJ0mD08OHD5GTkwNnZ2et/c7OzkhKSirw+SdPnsS///6LoUOH5nvMnDlz4ODgoLq5u7uXuN0Vhqkp0KOH2OZwPSIiIiKifEk+VK8kVqxYAV9fX7Rp0ybfYyZOnIjk5GTVLSEhoRxbaARYlpyIiIiIqEBVpLx4jRo1YGpqinv37mntv3fvHlxcXPQ+Ny0tDevWrcOMGTP0HmdhYQELC4sSt7XC6tZN9DxdugTExQEeHlK3iIiIiIjI4Eja42Rubo5WrVohMjJStU8ulyMyMhJt27bV+9wNGzYgIyMD7777blk3s2JzdAQCA8U2h+sREREREekk+VC98PBwLF++HKtXr0ZMTAxGjBiBtLQ0DB48GAAwcOBATJw4Mc/zVqxYgT59+qB69erl3eSKh2XJiYiIiIj0knSoHgD069cPDx48wJQpU5CUlITmzZsjIiJCVTAiPj4eJiba+e7q1as4fPgw9uzZI0WTK55evYAJE4D9+4G0NCCfCoVERERERJWVTKFQKKRuRHlKSUmBg4MDkpOTYW9vL3VzDINCAdSrB9y6BWzfDoSESN0iIiIiIqIyV5RsIPlQPTIAMpl6MVwO1yMiIiIiyoPBiQTNeU6VqxOSiIiIiKhADE4kdO4MWFsDt28DFy5I3RoiIiIiIoPC4ESCpSXwyitim4vhEhERERFpYXAiNc5zIiIiIiLSicGJ1IKDxc/jx4EHD6RtCxERERGRAWFwIrXatQE/P1EcIiJC6tYQERERERkMBifSphyux3lOREREREQqDE6kTVmWfPduICtL2rYQERERERkIBifS1qYNUKMGkJwMHD0qdWuIiIiIiAwCgxNpMzUFevQQ2xyuR0REREQEgMGJdGFZciIiIiIiLQxOlNerr4qep5gY4OZNqVtDRERERCQ5BifKq2pV4OWXxTZ7nYiIiIiIGJwoH8rqepznRERERETE4ET5UAanqCggNVXSphARERERSY3BiXRr3BioXx/IzAT27ZO6NUREREREkmJwIt1kMnWvE+c5EREREVElx+BE+dMsS65QSNsWIiIiIiIJMThR/jp2BGxsgMRE4Nw5qVtDRERERCQZBifKn4UF0LWr2OZwPSIiIiKqxBicSD+WJSciIiIiYnCiAgQHi5+nTgH37knbFiIiIiIiiTA4kX61agEtW4riELt2Sd0aIiIiIiJJMDhRwViWnIiIiIgqOQYnKpiyLPnu3WJBXCIiIiKiSobBiQrm7w/UrAk8ewYcPix1a4iIiIiIyh2DExXMxERdJILD9YiIiIioEmJwosJhWXIiIiIiqsQYnKhwXn0VqFIFuHYN+O8/qVtDRERERFSuGJyocOztgQ4dxDaH6xERERFRJcPgRIWnrK7H4ERERERElQyDExWecp7TwYOiwh4RERERUSXB4ESF17Ah4OUFZGUBe/dK3RoiIiIionLD4ERFw+p6RERERFQJMThR0SiD086dgFwubVuIiIiIiMoJgxMVTYcOgK0tcO8ecPas1K0hIiIiIioXDE5UNObmYk0ngMP1iIiIiKjSYHCiomNZciIiIiKqZBicqOh69BA/T58GEhOlbQsRERERUTlgcKKic3EBWrcW27t2SdsWIiIiIqJywOBExcOy5ERERERUiTA4UfEo5znt3QtkZEjbFiIiIiKiMsbgRMXTooUYspeaCvzzj9StISIiIiIqUwxOVDwmJkBwsNhmdT0iIiIiquAYnKj4lMP1/v4bUCikbQsRERERURlicKLiCwoCzMyAGzeAa9ekbg0RERERUZlhcKLis7MDOnUS2xyuR0REREQVGIMTlQzLkhMRERFRJcDgRCWjnOd06BCQnCxtW4iIiIiIygiDE5WMpyfQqBGQnQ3s2SN1a4iIiIiIygSDE5WcsteJ85yIiIiIqIJicKKSU85z2rkTkMulbQsRERERURlgcKKSa98esLcHHjwATp2SujVERERERKVO8uC0ePFieHh4wNLSEgEBATh58qTe458+fYoPP/wQrq6usLCwQMOGDbFz585yai3pZGYGdOsmtjlcj4iIiIgqIEmD0/r16xEeHo6pU6fi7Nmz8PPzQ7du3XD//n2dx2dmZqJr166Ii4vDxo0bcfXqVSxfvhxubm7l3HLKg2XJiYiIiKgCkykUCoVUFw8ICEDr1q2xaNEiAIBcLoe7uztGjx6NCRMm5Dl+6dKl+Oabb3DlyhWYmZkV65opKSlwcHBAcnIy7O3tS9R+0nD/PuDiAigUwO3bAMMsERERERm4omQDyXqcMjMzcebMGQQFBakbY2KCoKAgHDt2TOdztm/fjrZt2+LDDz+Es7MzmjZtitmzZyMnJyff62RkZCAlJUXrRmWgZk2gTRuxzaGTRERERFTBSBacHj58iJycHDg7O2vtd3Z2RlJSks7n3Lx5Exs3bkROTg527tyJyZMn47vvvsOsWbPyvc6cOXPg4OCgurm7u5fq6yANLEtORERERBWU5MUhikIul6NmzZpYtmwZWrVqhX79+uGLL77A0qVL833OxIkTkZycrLolJCSUY4srGeU8p717gfR0adtCRERERFSKJAtONWrUgKmpKe7du6e1/969e3BxcdH5HFdXVzRs2BCmpqaqfd7e3khKSkJmZqbO51hYWMDe3l7rZlBu3QJ27ZK6FaWjeXOgVi3g+XPg4EGpW0NEREREVGokC07m5uZo1aoVIiMjVfvkcjkiIyPRtm1bnc9p164drl+/DrnGIqvXrl2Dq6srzM3Ny7zNpe7UKaBhQyA0FHj6VOrWlJxMxup6RERERFQhSTpULzw8HMuXL8fq1asRExODESNGIC0tDYMHDwYADBw4EBMnTlQdP2LECDx+/BgfffQRrl27hh07dmD27Nn48MMPpXoJJdOyJdCgAfDkCfDdd1K3pnQog9OOHaLCHhERERFRBVBFyov369cPDx48wJQpU5CUlITmzZsjIiJCVTAiPj4eJibqbOfu7o7du3fjk08+QbNmzeDm5oaPPvoIn332mVQvoWRMTYFZs4DXXwe+/x4YNQrIVSzD6LzyCmBhAcTGAleuAN7eUreIiIiIiKjEJF3HSQoGt46TQiHKeJ8+DXz0ETB/vtQtKrnu3YHdu4G5c4Fx46RuDRERERGRTkaxjhP9P5kMmD1bbC9ZAsTHS9ue0sCy5ERERERUwTA4GYKgIKBzZyAzE5gxQ+rWlJxyntPhw2L+FhERERGRkWNwMgQyGfDll4CNDeDmZvxFFerVA3x8gJwcYM8eqVtDRERERFRiDE6Gom1b4M4dYPp0EaSMHcuSExEREVEFwuBkSBwcpG5B6VHOc9q1S/Q8EREREREZMQYnQ3TwIKCxfpVRCgwEqlYFHj0CTpyQujVERERERCXC4GRo7twRxSK++koUVzBWVaoA3bqJbVbXIyIiIiIjx+BkaNzcgCFDxPbEicZdKIJlyYmIiIiogmBwMkSTJwMWFqLHafduqVtTfN27i0IX588DCQlSt4aIiIiIqNgYnAyRmxswapTY/vxzQC6Xtj3FVaOGqBYIADt3StsWIiIiIqISYHAyVBMmALa2wLlzwObNUrem+FiWnIiIiIgqAAYnQ1WjBvDpp2J78mQgO1va9hSXcp5TZCTw4oW0bSEiIiIiKiYGJ0MWHg60bi2G6xnrori+vkDt2iI0HTggdWuIiIiIiIqFwcmQ2dsDJ08CAwYApqZSt6Z4ZDJW1yMiIiIio8fgZEyMtTS55jwnY30NRERERFSpMTgZg+xsYNkywN8fSE2VujVF16ULYGkJxMcDly5J3RoiIiIioiJjcDIGCgXw9dfA2bPADz9I3Zqis7YW4QngcD0iIiIiMkoMTsbAzAyYMUNsz50LPHkibXuKg2XJiYiIiMiIFSs4rV69Gjs0eg7Gjx+PqlWrIjAwELdu3Sq1xpGGd94RFeqSk4FvvpG6NUWnDE5HjwKPH0vbFiIiIiKiIipWcJo9ezasrKwAAMeOHcPixYsxd+5c1KhRA5988kmpNpD+n4kJMGuW2F6wAEhKkrY9RVW3LtC0KSCXAxERUreGiIiIiKhIihWcEhIS0KBBAwDA1q1b8cYbb+D999/HnDlzcOjQoVJtIGkICQFeegl4/hz48kupW1N0LEtOREREREaqWMHJ1tYWjx49AgDs2bMHXbt2BQBYWlrixYsXpdc60iaTAbNni+1ly4D796VtT1Eph+vt2iUqBRIRERERGYliBaeuXbti6NChGDp0KK5du4bg4GAAwKVLl+Dh4VGa7aPcOncGJk4E/vkHqFlT6tYUzUsvAY6OorjF8eNSt4aIiIiIqNCKFZwWL16Mtm3b4sGDB9i0aROqV68OADhz5gzeeeedUm0g6TB7NhAQIHUriq5KFaB7d7HN6npEREREZERkCoVCIXUjylNKSgocHByQnJwMe3t7qZtTco8fi14cY7F2LRAaKgpFXLwodWuIiIiIqBIrSjYoVo9TREQEDh8+rLq/ePFiNG/eHP3798cTY1xjyBgpFMC4cYCbG3D6tNStKbzu3UWFwH//BVi6noiIiIiMRLGC07hx45CSkgIAuHjxIj799FMEBwcjNjYW4eHhpdpAyodMBty7B6SnA5MmSd2awnN0BAIDxTar6xERERGRkShWcIqNjYWPjw8AYNOmTejVqxdmz56NxYsXY9euXaXaQNJj2jQxb2j3buDgQalbU3gsS05ERERERqZYwcnc3BzPnz8HAOzbtw+vvvoqAMDR0VHVE0XloH59YNgwsf3552L4njFQliXfv1+sSUVEREREZOCKFZzat2+P8PBwzJw5EydPnkTP//8gfO3aNdSuXbtUG0gFmDQJsLQEjh4Fdu6UujWF06QJULeuGGa4f7/UrSEiIiIiKlCxgtOiRYtQpUoVbNy4EUuWLIGbmxsAYNeuXeiuLDdN5aNWLWD0aLH9xReAXC5tewpDJlP3OrEsOREREREZAZYjrwgePRLD9rKyxMKyzZpJ3aKC7doFBAcDtWsD8fEiTBERERERlaOiZIMqxb1ITk4Otm7dipiYGABAkyZN8Nprr8HU1LS4p6Tiql4dWLcOaN4ccHWVujWF06kTYGUF3L4NXLgA+PlJ3SIiIiIionwVKzhdv34dwcHBuHPnDho1agQAmDNnDtzd3bFjxw54enqWaiOpEHr0kLoFRWNlBbzyihiqt2MHgxMRERERGbRizXEaM2YMPD09kZCQgLNnz+Ls2bOIj49HvXr1MGbMmNJuIxXV0aOi8IKhY1lyIiIiIjISxQpOBw8exNy5c+Ho6KjaV716dXz11Vc4aEzrCVVEw4YB7doBS5dK3ZKCBQeLn8eOAQ8fStsWIiIiIiI9ihWcLCws8OzZszz7U1NTYW5uXuJGUQkEBIifs2cDOv6MDIq7uxiip1AAERFSt4aIiIiIKF/FCk69evXC+++/jxMnTkChUEChUOD48eMYPnw4XnvttdJuIxXFoEGAlxfw4AEwf77UrSkYy5ITERERkREoVnD64Ycf4OnpibZt28LS0hKWlpYIDAxEgwYNMN8YPqxXZGZmwIwZYvvbb0WpckOmnOe0e7cop05EREREZIBKtI7T9evXVeXIvb290aBBg1JrWFmpkOs45SaXAy1bAufPA+PHA19/LXWL8peTA7i4iDlOUVFAx45St4iIiIiIKomiZINCB6fw8PBCN2DevHmFPra8VYrgBIhKdb16ibLf168DtWpJ3aL8DRwI/PYbMG4cMHeu1K0hIiIiokqiTBbAPXfuXKGOk8lkhT0llaXgYCAwELhzB4iLM+zg1LOnCE5//83gREREREQGqURD9YyRofU4JSWJkWplIiEBqFkTsLAoowuUkqdPgRo1xLC9mzeBevWkbhERERERVQJFyQbFKg5BpWPrVqB+fWDBAjEtqdS5uxt+aAKAqlWB9u3FNhfDJSIiIiIDxOAkoT//BF68AD7+WNREuH69jC6UnQ2sWAFcvlxGFygFLEtORERERAaMwUlCa9YAP/4I2NgAhw8DzZqVUe9TeDgwdCgwaVIpn7gUKcuSHzgApKZK2xYiIiIiolwYnCRkYgKMGAH8+y/QpYu696lTp1LufRo+XFxsyxbg1KlSPHEpatxYzG3KzAQiI6VuDRERERGRFgYnA+DhAezdq+59OnSolHuffHyAAQPE9uefl8IJy4BMpu514jwnIiIiIjIwDE4GQtn7dPEi0LlzGfQ+TZsGmJkB+/YB+/eXwgnLgHKe044dQOUq9khEREREBo7BycDUqyeyTe7epx9+KGHvk4cH8MEHYvuLLwwzmHTsKF703btAdLTUrSEiIiIiUmFwMkC6ep8++khs37hRghN/8QVgZQUcPw789VeptbfUWFoCQUFim9X1iIiIiMiAMDgZMGXv0+LFoiPmn39E79PChcXsfXJxEQmsUyfAza20m1s6OM+JiIiIiAyQTKEwxDFbZacoqwMbkthY4L33gKgocb9DB2DlSsDTs4gnys4GTE1FMQZDdPeuCHUyGZCUBNSsKXWLiIiIiKiCKko2YI+TkahXT1TpLnHvU5UqhhuaAKBWLaBFCzEHa9cuqVtDRERERASAwcmomJgAI0cCFy6I0XbPnwNjxog1oIo89+nRI2D8eOCXX8qgpSXE4XpEREREZGAMIjgtXrwYHh4esLS0REBAAE6ePJnvsb/88gtkMpnWzdLSshxbK7369UXv06JFgLU1cPBgMXqf/vgD+OYbYNIkUX3CkCjLku/eDWRlSdsWIiIiIiIYQHBav349wsPDMXXqVJw9exZ+fn7o1q0b7t+/n+9z7O3tkZiYqLrdunWrHFtsGExMgA8/FJX3OnbU7n26ebMQJxg2DKhTB7hzR9Q+NyStWwNOTkBKCnD4sNStISIiIiKSPjjNmzcPw4YNw+DBg+Hj44OlS5fC2toaK1euzPc5MpkMLi4uqpuzs3M5ttiw1K8v1rPV7H3y9RX39fY+WViIRXEBYM4cEVIMhYkJEBwstlmWnIiIiIgMgKTBKTMzE2fOnEGQcu0eACYmJggKCsKxY8fyfV5qairq1q0Ld3d39O7dG5cuXcr32IyMDKSkpGjdKhpdvU+jRxei92nAAKBRIzHf6fvvy629hcJ5TkRERERkQCQNTg8fPkROTk6eHiNnZ2ckJSXpfE6jRo2wcuVKbNu2DWvWrIFcLkdgYCBu376t8/g5c+bAwcFBdXN3dy/112EolL1PCxdqz31avDif3qcqVYCZM8X2d98BDx+Wa3v16tpVtO/qVeD6dalbQ0RERESVnORD9Yqqbdu2GDhwIJo3b46OHTti8+bNcHJywk8//aTz+IkTJyI5OVl1S0hIKOcWly8TE2DUKFF5r2NHIC1N3H/llXx6n954Q5T/fvYMmDu33NubLwcHsVgVwF4nIiIiIpKcpMGpRo0aMDU1xb1797T237t3Dy4uLoU6h5mZGVq0aIHr+fRKWFhYwN7eXutWGXh6avc+RUXl0/tkYiLmOA0fDnz0kVTN1U1ZXY/znIiIiIhIYpIGJ3Nzc7Rq1QqRkZGqfXK5HJGRkWjbtm2hzpGTk4OLFy/C1dW1rJpptDR7nzp00O59io3VOLBbN2DJEsDNTbK26qQMTgcPih4xIiIiIiKJSD5ULzw8HMuXL8fq1asRExODESNGIC0tDYMHDwYADBw4EBMnTlQdP2PGDOzZswc3b97E2bNn8e677+LWrVsYOnSoVC/B4Hl6AgcOAD/8oO598vXVM/fJUNZOatgQaNBAtGffPqlbQ0RERESVmOTBqV+/fvj2228xZcoUNG/eHNHR0YiIiFAVjIiPj0diYqLq+CdPnmDYsGHw9vZGcHAwUlJScPToUfj4+Ej1EoyCiYmotJe79ykoSKP36do14LXXgLAwKZuqJpNxuB4RERERGQSZQqFQSN2I8pSSkgIHBwckJydXmvlOucnlordpwgRRutzGRtSFGN7mLExatxKBJTpaTIqS2r59osKei4tYrNdE8qxPRERERBVEUbIBP4VWQrp6nz78EAga3xKpwW8CCgUwebLUzRQ6dABsbYGkJODsWalbQ0RERESVFINTJaac+7RgAWBlJbZfPjATcpkJsH07cPy41E0EzM2BV18V2yxLTkREREQSYXCq5ExMgDFjRO/Tyy8D0S8aYZUiDADwIvxz0fskNc5zIiIiIiKJMTgRAFG8LipK9D59bTEVGTCH1bED2P5xpO7Ke+UpOFj8PH1aDNkjIiIiIipnDE6koux92vlvHWyvNQIAIP9hEbp2BeLiJGyYiwvg7y+2d+6UsCFEREREVFkxOFEeDRoAb5z5HAf7fI/Bluuwfz/QtKlYI1ey3iflcD3OcyIiIiIiCTA4kU4mLjXRccvHOHXREu3bi8p7I0dCut6nXr3Ezz17gMxMCRpARERERJUZgxPp1aABcDAyGysn3YSVFbB/P+DrCyxdWs51I1q2BJydgdRU4J9/yvHCREREREQMTlSQK1dg4ueLwb8H4fypTLRvL7LLiBGi9+nWrXJqh4kJh+sRERERkWQYnEg/d3fg6VMgNhZe/6zAwYPA99+LdZ8iI8Xcp59+KqfeJ5YlJyIiIiKJMDiRfjY2wKRJYnvmTJikP8fHHwPnzwPt2onep+HDy6n3qWtXwMwMuH4duHatjC9GRERERKTG4EQFGzYM8PAAEhOBxYsBAF5eKP/eJzs7oGNHsc1eJyIiIiIqRwxOVDBzc2DaNLH91VdAcjIAwNQUOnufXn21DHuflNX1OM+JiIiIiMoRgxMVzrvvAt7ewOPHwHffaT2k7H2aNw+wtAT27ROV95YtK4PeJ+U8p3/+UQU4IiIiIqKyxuBEhWNqCsyaJbZPn86TiExNgU8+AS5cEL1Pz54BH3xQBr1PDRoADRsC2dnA3r2leGIiIiIiovwxOFHh9e0LREWJYXIymc5DyqX3STlcb80aICurlE5KRERERJQ/BicqPJlMFGfIJzQpKXufzp8HAgPVvU/dugHx8aXQjjfeED+3bQOaNxer8hIRERERlSEGJyqeJ0+AzZv1HtKwoZiKpOx92rtXVN5bvryEvU+BgcAvvwA1agCXLwOvvAK8+WYppTIiIiIiorwYnKjokpKA+vWBt94qcD0lXb1P778PdO9ewpwzaJC49ujRgIkJsHEj0LixmIeVnl6CExMRERER5cXgREXn4iIqQOTkAFOnFuopyt6n774TvU979pRC71O1asAPPwDnzgEdOgAvXgCTJwNNmgDbt5fhglJEREREVNkwOFHxfPml+LlunehOKgRTUyA8HIiOLuXep2bNRNGKP/4A3NyAmzeB3r1F6fICesSIiIiIiAqDwYmKx88PePttsf3FF0V6aqNGunuffv65BJ1EMploz5UrwIQJgJkZsGuXOPGECWJ1XiIiIiKiYmJwouKbMUN0I+3YARw5UqSnavY+tW0rep+GDSuF3idbW2DOHODSJaBHD1Gu/OuvRVpbu5bD94iIiIioWBicqPi8vID33hPbn39erFDSqBFw6BDw7bel2PukbNuOHWKuU/36wN27QGgo0KmTWKWXiIiIiKgIGJyoZKZMAWxsRPWHjIxincLUFPj007y9Tz16AAkJJWibTAaEhIjep1mzACsrMUawRQtRje/JkxKcnIiIiIgqEwYnKpnatUW6Wb5cdBmVQO7ep927Re/TihUl7H2ytBTzsK5cEes9yeXAokUi7C1fLqoDEhERERHpweBEJVetWqmdSrP36aWXgJQUYOhQIDi4hL1PAFCnDvDnn0BkpChZ/vChKOsXEAAcP14azSciIiKiCorBiUrPv/+KIXCl0IPTqBFw+DDwzTeAhQUQEVFKvU8A0KWLWPvp++8Be3vgzBkxRnDwYODevRK3nYiIiIgqHgYnKh0ZGSKQLFoErFlTKqc0NQXGjtXd+3T7dglPbmYGfPyxWOdp8GCx75dfxPC9778X1fiIiIiIiP4fgxOVDgsLYNw4sT11arELRejSuHHe3qcmTYCvviqF4XvOzsDKlWKonr+/SGfh4UDz5sD+/aXRfCIiIiKqABicqPR8+CHg6grcuiWKLpQiXb1PEycCdeuKCuPLlgGPH5fgAgEBwIkTot01agCXLwOvvCKKSZRoYSkiIiIiqggYnKj0WFsDkyeL7VmzgLS0Ur+Esvdp5UqgY0cx3+ngQeCDDwAXF6BPH1H/4cWLYpzcxESMBbx2TczVMjEBNm4UF501C0hPL+2XQ0RERERGQqZQlHiqvVFJSUmBg4MDkpOTYW9vL3VzKp7MTBE0YmOBOXOACRPK9HIJCcC6dcDvvwPnz6v329kBffuKNW+7dAGqVCnGyS9cEAHqn3/E/fr1xfynkBCxRhQRERERGbWiZAMGJyp9a9YAAwYAVasCN2+WarlyfS5dAtauFbe4OPV+Z2egXz8Rolq3LmLmUSiA9evFOME7d8S+7t2BBQtEIQkiIiIiMloMTnowOJWDnBwxPygkBBg5ErCyKtfLKxTA0aMiQK1fDzx6pH6sQQOgf38RooqUe1JTgS+/BL77TlTcMzMTRSQmTQJsbUv9NRARERFR2WNw0oPBqZwoFAYxnC0rC9izR4SorVuB58/Vj/n7ixD19tuipkWh/Pcf8NFHwK5d4n6tWqLc3zvvGMTrJSIiIqLCY3DSg8FJAgYSolJTge3bxXyo3bvV6/SamIh5UP37A6+/Djg4FHAihQL4+2+xDtTNm2Jfhw7AwoVAs2Zl+RKIiIiIqBQVJRuwqh6VHYVCdPP4+4sS5RKztRXhaMcOIDERWLwYCAwE5HJg3z7gvffEfKg33wS2bNGzFJVMJoYhXrokqu1ZWYkCEi1aAKNGlbAuOhEREREZIvY4UdkKCgIiI0UqWbFC6tboFBsL/PGH6Im6fFm938EB+N//xHyojh1Fz5RO8fGieMSGDeJ+9eqiouB774kFqIiIiIjIIHGonh4MTuXsxAmxYq2JieihadxY6hblS6EQJc3XrhVB6vZt9WNubmIuVGgo0Lx5PiMP9+8HxowRrxMAWrUCFi0Sr5+IiIiIDA6H6pHhCAgAevcW4+GmTJG6NXrJZCIUzZ0rRhYeOAAMGyaqqt+5IwrqtWwJNGkiRugppzepdOkCnDsn1nqytwfOnAHatgUGDwbu3ZPgFRERERFRaWGPE5W9ixcBPz/RpXPmjEgfRiQjA4iIEEP5/voLSE9XP9a2rZg39dZbQM2aGk+6dw+YOBFYtUrct7cHpk0Tc6DMzMqz+URERESUDw7V08MQg5OBFJ0rW+++K5JHjx7Azp1St6bYUlKAzZvFcL7ISNGRBoipTK++KkJUnz4aSzsdPw6MHg2cPi3u+/gAP/wg1rkiIiIiIklxqJ4R2bsX6NpVx7Cvimb6dKBKFbH+0ZkzUrem2OztgbAwsTbU7dtiVF7r1qK0+a5dwIABoufpnXdExfLMli+JeV7LlwM1aojqE0FBonRffLzUL4eIiIiICok9ThKSywFfX/FZ2toamD1bjOSqsIXY5s4VPS49e1a4LrZr10Qv1O+/A9evq/dXry4yUmgoEOj9BCbTp4o66HK5KGP++eeiIp+lpXSNJyIiIqqkOFRPD0MKToD4kD1sGBAVJe63bQusXGnQxedID4VCjMr7/Xdg3TrtmhB164qeqFD/q2j6w/ti7ScAqF9fdF2FhFS4QElERERkyBic9DC04ASIzofly4Fx44BnzwALC2DqVNERUWHrCCQnA3Z2ehZHMn7Z2aIy3++/i3lRz56pH/P1VSC06Xm8s/991Ll3Suzs3h1YsABo2FCaBhMRERFVMpzjZGRMTIAPPhDL/wQHiypun38u5j9VSIsWAfXqAZs2Sd2SMlWlipi/9ssvoufpzz9F4QgzM+DiRRkm/NEcde+dRIfaN/GTyQg8ijgJNG0KTJgApKZK3XwiIiIi0sDgZEDc3UVBgd9+A4YOFQXoKqRHj4AnT4DJk0W3TCVgZSXmOm3ZIkLUsmVAp05iZN6h2/UwXP4jXGVJeC1rI9Z/HYvnXn5i0lTl6hAmIiIiMlgcqmcEHjwA/vc/4KuvxBwoo5eSIub1PHoErFgBvPee1C2STEKCmAu1di0QHa3eb4tn6IstCG16Aa/8MgBVWvlJ1kYiIiKiiopD9SqYKVNEHYF27YBPPgHS0qRuUQnZ24vFYQGxKGxGhqTNkZK7u5jbdu6cGKr5+eeAR105UmGH3zAQ3f/9Fm7+LhjTLAon9iSzA4qIiIhIIuxxMgKPH4vA9Ouv4n79+qKYRJcu0rarRF68ALy8gDt3REGEMWOkbpHBUCiAY8eA35c+w5/rFXiYqf499XRKRv/37RA6wASNGknYSCIiIqIKgFX19DDG4KS0a5coIpGQIO4PGwZ88w3g4CBtu4pt2TLxgpycxArAtrZSt8jgZGUBe789j7VfJ2BLcmc8h43qsZYtxfpQb78N1KolYSOJiIiIjBSDkx7GHJwAMT1owgRgyRJxf+xYEZ6MUlYW4O0NxMaKqgmvvSZ1iwxXVhbS5v2EbdPPYe2LvtiNbsiGqFUvkwGdO4sQ9frrQNWq0jaViIiIyFgY3RynxYsXw8PDA5aWlggICMDJkycL9bx169ZBJpOhT58+ZdtAA2JvD/z4o1gwt1MnYNIkqVtUAmZmwKpVYnIPQ5N+Zmaw+WwU+sfOxt+DN+MuamExRqKd6TEoFMD+/cCQIYCLC/DGG2LdqPR0qRtNREREVHFI3uO0fv16DBw4EEuXLkVAQADmz5+PDRs24OrVq6hZs2a+z4uLi0P79u1Rv359ODo6YuvWrYW6nrH3OOVHoQAGDgRCQkTZa5lM6hZRmTp+HBg9Gjh9GrHwwB9OH+F366G4fEs93NHBQYSo0FCgY0fA1FTC9hIREREZIKMaqhcQEIDWrVtj0aJFAAC5XA53d3eMHj0aEyZM0PmcnJwcdOjQAe+99x4OHTqEp0+fVvrg9OefQL9+YrtPH9Er5eoqaZOKLiYGcHQEnJ2lbolxkMuBlStFhcKHD6EAcCHoU/zuOQV/7LDH7dvqQ2vVEnOhQkOBFi0YrImIiIgAIxqql5mZiTNnziAoKEi1z8TEBEFBQTh27Fi+z5sxYwZq1qyJIUOGFHiNjIwMpKSkaN0qot69RdnyKlWArVsBHx/gl1+MaP3UuXOBpk2BmTOlbonxMDERKyVfuwaMHg2ZiQn89n2Hub+64NbQmYjak4Fhw8Scp7t3gXnzgFatgIYNRZXGffuAzEypXwQRERGRcZA0OD18+BA5OTlwztXD4OzsjKSkJJ3POXz4MFasWIHly5cX6hpz5syBg4OD6ubu7l7idhsiCwtg+nTgzBnx4fjpU2DwYKBHD+DWLalbVwht2ogelGXLRLEIKrxq1YAffhCLQXXoALx4AZNpU9DxA28s67UdSYkKbN0qhnBaWgLXrwPz5wNduwLVq4uCEitWAImJEr8OIiIiIgNmEMUhCuvZs2cYMGAAli9fjho1ahTqORMnTkRycrLqlqCs5V1BNWsmpr98/bUIU7t3A//7nxH0PHXqJD7JZ2WJBEhF16yZqBryxx+Am5sIoL17w6JvMHp7X8OffwL37gEbN4pQ7ewMpKaKgoZDh4rhfK1aiZ7LEydEjiUiIiIiQdI5TpmZmbC2tsbGjRu1KuMNGjQIT58+xbZt27SOj46ORosWLWCqMctd/v+f7kxMTHD16lV4enrqvWZFneOky7VrYq2nOXOAwECpW1MIp06JnicTE+DiRTHekIonNRX48kvgu+9EGDUzE+PzJk0C7OwAiGB09iywY4e4nTqlfQonJ9Fj2bMn8OqrLHNOREREFY/RFYdo06YNFi5cCEAEoTp16mDUqFF5ikOkp6fj+vXrWvsmTZqEZ8+eYcGCBWjYsCHMzc31Xq8yBSdA9DRpFgL44QdRpjo8XMyHMjivvy66QN54Q3SNUMn89x/w0Udi9WRAdCvNmCHe31xJ6N49cdiOHcCePWLNMCVTU6BdOxGievYUmZYFJoiIiMjYGVVwWr9+PQYNGoSffvoJbdq0wfz58/Hnn3/iypUrcHZ2xsCBA+Hm5oY5c+bofH5YWBir6hVSfDzQqJEITv7+oiCbr6/Urcrl8mVRJEKhEF0g/v5St8j4KRTA338DH38M3Lwp9lWpAnTpIkowvvaaGNqnISsLOHxYhKidO0XBQ00eHkBwsAhRnTsDVlbl8UKIiIiISpfRVNUDgH79+uHbb7/FlClT0Lx5c0RHRyMiIkJVMCI+Ph6JnLVeKtzdgcWLxfo+p0+L+SzTphlYZTUfH2DAAFHwIC5O6tZUDDKZWODr0iUx+c3HB8jOFt1KI0cCtWsDAQHAV18BV64AECP7OncGvv1WZNkbN4CFC4Hu3cXcubg4UfK+Z09RYKJXL2DJEhHOiYiIiCoiyXucyltl7nFSuntXfF5WTiFr2lT0PrVuLW27VO7dE5/OOamm7Fy7JurWb90K5C7936iR6Inq21f8Uphof7+Slgbs36+eG6W5XhQgfp+UQ/ratjXQIaFEREREMLKheuWNwUlQKMSiuaNHAw8eiKFW8fFAIYsVUkWSmAhs3y5CVGSkGKen5OoqFgnr21dUPsw1h1ChEHU8lCHq2DHtanzVqgHduokQ1b07f7+IiIjIsDA46cHgpO3hQ1E7wNtbFFwzKAqF+DRuZwd07Ch1ayqH5GRRIWLrVjG56dkz9WP29iIB9ekjyu39f3U+TY8eiRL4O3YAERHA48fqx2Qy4KWX1HOjmjdngQkiIiKSFoOTHgxOumlW3zt9Gli1Skx50fHZuPwsXAiMGSPWJzp3Ls+QMSpjGRnAgQOiyuG2bWIIpZK5ORAUpC4ukWsRawDIyRHrQSl7o86f1368Vi11iAoKAmxty/blEBEREeXG4KQHg5N+cjnQogVw4YIoJrFsmRhiJYlHj4D69UVd7D/+AN5+W6KGEORykYK2bBE3zWUBZDKxUFifPuLWoIHOU9y+LTqxduwA9u0Dnj9XP2ZuLjoVlXOj8jkFERERUalicNKDwalgkZFi4dzYWHF/0CBg3jzA0VGCxsyaBUyeLD5JX74syr2RtBQKUZ9861YRok6f1n68aVN1iGrZUud4vPR04OBBdW+Uskq6UsOG6hD18st5plYRERERlQoGJz0YnAonLQ344guxYK5CIUZi/fijWJ+2XD17Bnh6igoWy5aJREeGJSFBFJfYsgWIihJj9JTc3dUh6uWXdQZfhQK4elUdog4dEtXSlezsgK5dRYgKDgZcXMr6BREREVFlweCkB4NT0Rw9Crz3nvhgC4i6AeU+dG/BArF4q5ubGCJmaVnODaBCe/JEpJ8tW0R1CM3xeNWqifWk+vQBXn0VsLHReYqUFGDvXvXiu5pTqwCx/piyN8rfn1PfiIiIqPgYnPRgcCq69HRgxgzg5EmxZmq5f1BNTxdjtxISxJjBTz4p5wZQsbx4ISYzbd0qeqQePlQ/ZmkpwlPfvmL13HzqlMvlwNmz6t6oU6e0H3dyEgX+evYUp+PSX0RERFQUDE56MDgVX04OYGoqtlNTRX6ZOhWoXbscLr5iBfD998A334hPymRccnKAI0fU86Li4tSPmZiIYXx9+4o1ozw88j3NvXui13PHDhHiU1LUj1WpArRrp+6N8vZmuXMiIiLSj8FJDwan0vHxx2IEnb29yDJDh5ZxT5Ry3owyuZHxUihE2catW8UtOlr78ebNxXC+vn0BX998009WFnD4sLo36soV7cc9PNQhqlMnscgzERERkSYGJz0YnEpHTAwwZAhw7Ji436kT8PPPoo4DUZHExop1orZuFZUh5HL1Y/XqqYtLtGunNzjfvKkud37ggFiGSsnKCnjlFXWBiTp1yurFEBERkTFhcNKDwan05OSINWq/+ELUALCyEtXDP/qoDDuGXrwAFi8Wn4q/+KKMLkKSefAA+PtvEaL27BHz25Rq1BCL7fbpI1bM1dOFlJYG7N+v7o26fVv7cV9fdW/USy+JYX5ERERU+TA46cHgVPpu3hRVwvfvF/cnTgRmzy6ji0VEiDlOlpbAjRtArVpldCGSXFoasHu3CFF//y0q9inZ2Ijyjn36iPRTrVq+p1EogIsX1SHq2DHtTq1q1YBu3cRpunfPt04FERERVUAMTnowOJUNhUIM1Zs9W5Qwd3Utwwu9/LIoNNC5MzBlCtChA2tSV3RZWWIY35YtIkhpdiFVqSLGivbpI4pLFFCt5NEjkcd27BA5/PFj9WMmJkBAgLo3ys+PBSaIiIgqMgYnPRicylZWlvYap1Onis+yLVuW4kWOHAE6dlQXjKhXDwgLAwYPFguuUsWmUIga5coQdemS9uOtW6vnRRVQWi8nBzh+XN0bdeGC9uNubmJOVM+eYo6UrW1pvxgiIiKSEoOTHgxO5efvv8V6p6amwPjxonOo1NauPXsW+Okn4I8/gGfPxL6VK0V4osrlv/9EcYktW8Q4PM1/0ho2VIeogIACeyZv31YXmNi3T3v9XkAEJwcH9a1qVe37+e1T7rezY+coERGRIWFw0oPBqfzcvw+MHg38+ae437ixWI4pMLAUL/L8ObB5M7B2rbiQskvg55+B06dFkGrThuOtKoukJOCvv0SIiowEMjPVj7m4iO7PPn3EME8LC72nSk8HDh5U90bdvFny5slkIjwVJmTlt8/Wlr/OREREpYXBSQ8Gp/K3dSswYoT4TCuTiTD15ZdlOOxJoRBrASnHXXl7iwA1YID48EyVQ0qKmMS0datIPpqr5drZiTF4ffuKYiMF/FugUIi5UE+fAsnJ6p+at4L2aZZHLwkTE9HcovZ6ae6ztmb4IiIiAhic9GJwksaTJ0B4OPDLL+J++/bAP/+U0Yc3hQKIigJWrQI2bhQlzAExZrBHD1EC8LXXyuDCZLAyM8XiTlu3imF9iYnqx8zMxASmvn3F70UZheuMjPwDVmHDWFZW6bTF1LT4ww2V25aWDF9ERGT8GJz0YHCS1u7dwAcfAIsWAb16lcMFU1LEEL6VK9Wr9b7xhghUVDnJ5cDJkyJEbdkCXLumfkwmEws79ekjgpSXl1StzEOhEMMHSxK8kpPVNVVKysys8CHL0RGoXl37p7l56bSDiIioJBic9GBwkl56unaRiD//FPfLvBPoyhXR5fXqq0CXLmLftWvAO++Iqnz9+4tPdVS5xMSIELV1qwhUmnx81CGqVSuj72JRKMTyWMUNXcpbafyvYWurHaZyBytd+6pVK8PFtYmIqFJicNKDwcmwJCaKz6ZPn4r8smAB4ORUjg34/HNgzhyxbW4uigcMHizCFT+hVT537oihfFu3iqF92dnqx2rVAtq1E9X5AgJEkLKykqypUpHLgdTUwgevp0/FUN1Hj8Q8scePix+8ZDLRk6UvYOkKYPb2Rp95iYiojDA46cHgZFhevACmTQO+/VZ8IKtRA1i4EOjXr5w+6Dx8KCryrVoFREer99eqBQwcCHz2mfikRpXP06eiqMTWrcCuXaKrRlOVKkCzZuog9dJLYmgf643rJZeLt/bxYxGmlIFK86eubc3aHkVlalpwb5auAGZlxcBFRFTRMTjpweBkmE6fBt57D7h4Udx/7TVgyRKRX8pNdLQIUL//Lj6tVa0qusSU4wrlcn4orqzS08UcuRMnxIq5J06IMpG5Va0qyt+/9JI6UHH4Z6nIylL3XBUUtjT3KWvDFIeFRdGHE1avzvlbRETGhMFJDwYnw5WZKUbNffml+JDk6Ahcvy7mNZSrjAyxeu+DB8Dw4WKfQiGGZvn6iqF8HTowRFVmCgWQkKAOUSdOAGfOiICVm6endpBq3pyfrMvRixd5A5W+ni3ltuYozaKytS36cELO3yIikgaDkx4MTobv4kXR+9SuHTB/vtSt+X8nT4oPvUr16omCEoMGAXXrStYsMiBZWWLtMGWQOn5cu2KfkoUF0KKF9hA/Dw+OCTMgCoWYx1WUnq3Hj0WPmFxe/OtWraodpurXB/z9xc3bW4wOJSKi0sXgpAeDk3HIzhY35Si5a9eAPXuAkSMl6uhRKMSH4VWrgHXr1BMuZDJRoW/6dJH0iDQ9eSJCt+YQv8eP8x7n5KQdpFq3FnW8yajknr9VUM+W8mdh5m9ZW4u8rQxS/v5Aw4bs+CYiKikGJz0YnIyPXA507AgcPiwWzv35Z6BRIwkb9Py5WP9n5Upg/36xLzJSXeI8PV30KrAHgXJTKIAbN7SH+EVH513ZViYDGjfWHuLXtCm7HCoozflbyjD18KGolH/6tBgF+uxZ3ufZ2QEtW6qDVOvWopeK//QQERUeg5MeDE7GRy4HfvoJGD9eDJ+xsBAdPB9+CNjYSPwhIS5OLKYbHq7+6veTT8RKv2FhwIABgKurhA0kg5eeDpw7pz3ELy4u73HW1uLTsWbPlJtbuTeXyp9cLnrdT59W386e1V34ompV7V4pf3+gTh2GKSKi/DA46cHgZLxu3QI++EBkEiWZDFizRqxdCwCHDgGTJ4tvYm1txU25bWcH9OghvrgHxDe8167lPcbMrASNVCjE/Kdbt8R9U1Oge3cxaatXLxYFoMK5d097iN+pU7rHc7m5aQepVq3EtwlU4WVnq3uklLfoaFFkJzcnp7xhqlwrlhIRGTAGJz0YnIybQgH8+qvofbp/X+zbvBno21dsr10LhIbm//zVq8XyTIBYoqdXr7zHmJuLEPXtt6KAHiAKVkydqh2wNH+2bSsW8gWA54nJuLMiArZb18DuzAFY4zlMoBCLVI0ZI5IdUVHI5cCVK9pzpS5ezFuJwNRUfDOgDFIBAWLIHyfCVAqZmcClSyJEnTolfl68qLtCoKurenifv7/I3DVrln+biYikxuCkB4NTxaBQiKlGz54B9vZiFBMAxMeLz5WpqeKx3D9HjBCfJwGxpunIkerHMzK0r7FypTo47dwJ9OyZf3sWLBCZCBBzsV5+Wf2YDHLYyJ7DVvEMdlWrYNxcJwwbBkAuR/y/KZi5sKpWCNPcbtpUrKkKiA8/ycniMXZcEVJTxeQXzSF+d+/mPc7eXnw61pwvxU/IlUZ6uij2qOyVOnUKuHxZd/W/OnXy9kyV+3IQRETljMFJDwYnyk9mJpCWpg5SLi6iLDAgRt7t2qV+LHcgGzlSDAMERL2IPn3EY7r+dn3/PfDxxwD27sXR4Flol30w3zbNmKHuoPr3X7GMFCCGE+bu9Ro8WAxlBMTk8rlzdfeO1agBNGjAdVkrpNu3tYPU6dO6J8LUq6cOUQEBolybsoQlVXhpaWJYn+Ywv6tXdf975empHaRathRZnIioomBw0oPBicqLslcsd8jy8ADc3QFMnIhbX63FGryLZ7BDqrUzntX3Q2otLzxT2CE1VfSQDRggznf8uBgSmJ9p08RwQkAM11HO5dJlxAjgxx/F9rNnIpzVry9u9eqJm7IXj4xYdrZI3JpD/GJi8h5nZiYW5tUc4ufpyYoClUhKiig4oRmmbtzIe5xMJqqaaoap5s05tY6IjBeDkx4MTmRQzp8Xa0OtWSO6iZQCA4Ht2/N0C2VlafeKaf5s2FAskgmIjod583T3jt27J3q8Pv1U3YTmzfM2zdlZBKmhQ0VtC+X1ExNFTQJT01J/N6g8PH0qxmspe6ZOnAAePMh7XPXqQJs26iDVpg3HbVUyjx9rh6lTp8Rw6NxMTMQcT805U82asROTiIwDg5MeDE5kkDIzgb/+EiFq1y4xsSkmRv2N/3//iR6AUprkr1CoTx0bK3qfYmOBmzfFLTlZfezcucC4cWL7wgXAz090UNStK3qmlL1U9euLD00eHqXSRCovCoX4w9cc4nfunO7ybA0bas+VatashGUoydjcvy+m1mmGqcTEvMdVqSKGFmv2TDVtyvmZRGR4GJz0YHAig5eYKL7WDQgQ958/FyWwqlcXa0MNGiRSSxl68kQdpHx91QsO790rimTkXq9V6auvgM8+E9v//QdMmJA3XNWty2+iDV5GhuiK1Bzip2vclqWlKMemWRLd3Z1D/CqZu3e1h/idOiUW8M3NwkJ88aIZpry9ua4zEUmLwUkPBicyOidOAF27inF2gPhQ2qWLqAbRt2+5T0bKyREflG7e1O6lio0FJk5Ul3jfvh3o3Vv3OdzcgDlz1PO3Hj8W87Lq1xcZkdWzDdCDB+q1pU6cENtPn+Y9zsVFe66Uv7+oTEKVhkIBJCRol0U/fVr3r4u1tahNohmmGjbkvwFEVH4YnPRgcCKj9Py5WLBq1SpRtk/J3l4sbJVfQpHQzZtirSzNgBUbK+ZaAcC6dUC/fmL7r7+A114T2xYWYrifZk9Vr15iOSIyIHK5WEFac4jfhQsiWWuSycQfpI+P6F5Q/vT2ZqCqRBQK8W+AZs/UmTPq74M02dmJ6n3KINW6tfgVYkcmEZUFBic9GJzI6MXFiZV8f/lFbF+/LuY/AWJ8nJ2d+NbfACkUYghPbKz4IFSjhti/aZOYRxUfn/dzNwD88Qfw9ttie98+MQRQswqg8medOpxDIannz0U1Ac0hfgkJ+R9fu7YIUrlDFWvlVwrK7K0Zps6e1V1Bv2rVvGtM1anDMEVEJcfgpAeDE1UYcrn4lOHvr97Xt6/ovunRQwzl69XLqJJEdrb4nK3ZQ3XzJvDFF+ry6gsXqhcbzs3EBNi4UbwNgMiRJ0+qg5WzMz9olbukJLHiakyM9s979/J/Ts2a2kFK+dPVlX+AFVx2tvgV0QxT0dG6a5U4OeUNU7VqlXuTicjIMTjpweBEFVZODvDKK8BBjQV1a9QAQkNFiPLzk65tpejOHfFhKne4io0V31QfP66uq7FoETB6tPq51tbqdarq1xcLFysLX2hWGqRy8Pix+IScO1Dpqnet5OCQN0z5+IiuB06KqbAyM8UcSM05UxcvipCVm6urWF7Bw0P8WtStK2516ojHuIwCEeXG4KQHgxNVeFeuiGF8v/6qXSd4wACxr4JSKEQnhqOjupNt3Tpg6VIRqhISxDGajh5VLyq8eDEwY0beKoDKn+7u/NBVLlJTxe9w7l6qGzdEL6su1tZiElzuQOXpyZJtFVR6uphSp1nJ7/Ll/H9FAPGrULu2Okjl/lmnDhf+JqqMGJz0YHCiSiM7G9izRxSU2LZNrIg7apR47MYNkRKU1c98fSv8ejyZmaIzQ7MK4Pjx6uk0n34q3qL8HDki1iUGxPC/ixdF9a9GjcSQIfZWlbH0dDH2Mneguno1//r4ZmbqlaE1A1XDhqyJXwGlpYlhfZcvA7duib/vyp+3b+vuocrNyUl3qFL+rFGDf9eJKhoGJz0YnKhSevRIdMMoq5j98osYvqdkZSXW41GWkO7cudJN0E9JEXky9/C/mzdFDY64ODHUBwDGjgW++0793KpVRYBS3oYNEx/AqBxkZ4s/pNyBKiZGFKvQxcQk/0p/trbl234qFzk5ogNeM1Dl3lZW/NTH2lrdO6UrXLm5VfjvoIgqHAYnPRiciCC6SzZuVJeSzr3AyvbtQEiI2L5xQ0wsatUKsLEp96YaArlcfMus/KZ5+XLx9l29Kj545f5X9M4d9ST1efPEwsGNGql7qBo1Eh+w+M11GZLLxfhMXYUpdC0opOTurh2olNuOjuXWdCp/CoX4tdDspcodrJKSCj6PiYn4u597fpXmT1bhJzIsDE56MDgR5SKXiyFQx4+rb7t3i8pmADBlCjBzppjg4+ur7pV66SWuVAlRkOK//0SIunpVdH6sWKEORa+/DmzZkvd5Njbi7du7V925l5Qk9vODVRlSKMQbnTtMxcTor/Tn7Ky70p+LCxNwJZGRIbJ4fuEqPl539b/cqlbVHaiU287Olf6fVaJyxeCkB4MTURHNnCkqLNy9m/cxBwcxQ7tOHXE/O5uT8XM5c0ZUjb96VaxZc/Wq6MTLyREjJFNT1R+S3noL2LBBfGOtOfRPeatXj5/Ry5Sy0l/uQKWv0l/VqroDFSv9VTpyOXD/fv6h6tYt4MmTgs9jbi46PvMLV7Vrc4oeUWlicNKDwYmomG7fVi9qevy4SARmZuKTgPID4rvvisoJyh6pgACgWTOjWkuqPGRliZ6pu3fFdDKlzp2BqCjdzzE3F1N2lJX91q8XvV3KUMWRZGWouJX+lPOmNENV/fr8cqESe/ZMf4/VnTv6KwMqubjoL2JRrRq/ZCEqLAYnPRiciEpJVpaomODlpd7n5QVcv659nKUl0LIl0K4d8PXX/N+8AE+eqIf9afZSmZuLrKrk7699v0YN9Tyqpk2B8PDyb3ulU5xKf+bm+Vf6s7Ao3/aTwcnKEl+o5FfEIj4+/5onmmxt8w9VdeuKQjfM70QCg5MeDE5EZejxY9HjpOyVOnFCPTbFz0/UClb67DPRTRIQIFIAq5nplXuB3vHjRXC6elV8S63Jy0sELqV+/USp5txD/5ydmWPLhGalP81QdeWK/kp/np55w1T9+qyBTSoKhSiSqq+IxYMHBZ/H1FQM+cs9BNDeXsyzzO9mbc317Cq7rCzRc/rsmeiMV27nvq/vMc37iYlixLOUGJz0YHAiKkcKhbrwRJUqQP/+Yn9GhvgfWjmT2sREdJO89JK4tWsnPjRSoaSmaheosLERJdMB8Ufg6Ki7kJy9PfDKK8Dmzep916+LOVZcCLQMyOXiE66ueVT6Kv3Z2mqvxqx58/DghBfS8uKF+DXLL1wlJBRuTav8WFpqByl9QaugEJZ7n7k5vyMobRkZpRNwlNsZGaXbvvh4MadPSgxOejA4ERmA1FRRcELZM3X7tvbjr78ObNokthUKICICaN1afPNORSKXi3lTmsP+rl4VoyzlcqB7d2DXLvXxTk7Aw4fim+jcPVQ+PqKMOpWy/Cr93bgh/m4U9N90rVp5A5UyaLm4sEgFacnJEb9uuhYJTk0VvdO5b8+fF/xrWBpMTYsfugq6WVkZ/l8FhUIEk9IIOMrt/EYNl5SFhfhOx85OfdO8r+8xzfu1a0s/bJTBSQ8GJyIDdOeOek2p48eB//0PGD1aPHbzphjCBIifmuXQ/fxYeKKYMjJE71JOjqjfAYj/bOvWFSMudQkKEuXTlebMEd8UKoMV/0ktAxkZ4pPtzZvat9hYEayePdP/fEtL3T1V9eqJG4fIUiEoFGJKn65QVZjb8+f6Hy+rD/e5WVuXfi+ZjY0YPFHSgKO8X5LeQH0sLYseavQ9VpEWemZw0oPBicjIHD8OhIWJbpLcLCyAuXOBMWPE/dwr1VKxPHyoXaBCeevRQyzoCwApKaIavSYXF/Uiv127Am++KfYrFOKDETNuKVMoRMrVFapu3hTdCDk5+s9Rs2beQKXcdnPjhBYqF1lZxQtchQlqhSmmYYisrYsfanRtS92rY8iMLjgtXrwY33zzDZKSkuDn54eFCxeiTZs2Oo/dvHkzZs+ejevXryMrKwteXl749NNPMWDAgEJdi8GJyEg9eaIuPKHsmXr8GNi4EXjjDXHMvn2iJLpyrpSy8ARXlC119+8DkyerQ1VSkvbj778P/PST2E5KElW8zM11/2ffq5e6gzEjA/juO+3jNI+vWVMENCqErCwxoUVXqLp5M/+uRSUzMzGHKr/5VbmTM5EBksvFvK/S6BnTdVN+N2FrW7xQo+sxW1t+Z1GejCo4rV+/HgMHDsTSpUsREBCA+fPnY8OGDbh69Spq1qyZ5/ioqCg8efIEjRs3hrm5Of7++298+umn2LFjB7p161bg9RiciCoIhUIMVXJ2VgejL78EJk3SPs7EBGjSRASp8HCgcePyb2slkJysPYcqIEAEIkAUrtBX6+PDD4FFi8T2/fvijzQ/AwYAv/4qttPTRSDL70NI+/bABx+IYxUKYOXK/D/c2NtXwoIYT5+qg5RmoLp5U0yCK2j8lKOj7kBVv74Yw1mRxvIQ6aDsTa9SxfDnT1H+jCo4BQQEoHXr1lj0//9ryuVyuLu7Y/To0ZgwYUKhztGyZUv07NkTM2fOLPBYBieiCuzFC+DcOe2FeuPj1Y+fOwc0by62d+4Ejh5V90w5OUnS5MpALhfBKr/x/V5e4o8BEKWWx4/XfVxqqijMqBwu+OCB6IHKz7vvAr/9JrYzMvQXn3vtNWDbNvV9X191D1nu3q9mzcToUaVdu8SoUV3HGu23xjk5Yu5h7kClvN2/r//5pqYiPOVXtKJ6dQ6pJSKDUJRsIOmIx8zMTJw5cwYTJ05U7TMxMUFQUBCOHTtW4PMVCgX279+Pq1ev4uuvv9Z5TEZGBjI0aiempKSUvOFEZJisrIDAQHFTSkxUD+9r2lS9f+NGYNUq9f369bULT7RsyUHhpcTEBKhWTdwKUr06sGJF4c5brZpYGklXwHr2TMy1UsrKAkJC8p+grVkjITMT+Pff/K/bq5d2cHr9ddH7pUvXrsCePer7vXuLtujq9apXD3jrLfWx58+L8ObgoF5fp9yyhqmpKK1Ypw7QsWPex1NTRa+UrlAVGyvekLg4cdu/P+/z7ex0BypliXUuBkxEBkjSTwUPHz5ETk4OnHONy3B2dsaVK1fyfV5ycjLc3NyQkZEBU1NT/Pjjj+jatavOY+fMmYPp06eXaruJyIi4ugJ9+oibpuBg8fP4cVH+Wfmhb+1asf/JE/WqfOfPiw96Hh4cj2FAqlTRDkf62NoC27frfkwu165kZWoKHDyYf6+X5jXlclHcMfdxyvPlLoixd6/oGNWlfXvt4NS9u/bcMRMTEaAcHMTUvY0b1Y9NmyayivJxzZ9OTmK0aqmytRVfRGh+GaEklwP37uUfqu7cEW/S+fPilptMJgpT6CpYUb8+V24mIskY5depdnZ2iI6ORmpqKiIjIxEeHo769eujU6dOeY6dOHEiwsPDVfdTUlLgLvVKW0Qkvf/9T9wAMdfj1Cn18L7kZO2lzMeMAf75R4z1atBAfHJWlo9r1Eg9zoyMkomJdsAxNQU6dCj8c48f196nUKjLE+ceDP/bb/mXIvby0j62WjUxxDA5WWQRuVz8qj59mnfByJ9+ylugQ6lpU+DiRfX9Fi2Au3e1A5Zyu149YOpU9bF794oQmPs4Ozs93yGYmIgvLFxdxWLWub14ob/EelqaWFTo9m3x9y43K6v8Q1W9epVwshoRlRdJ5zhlZmbC2toaGzduRB+Nb4MHDRqEp0+fYpvmgHM9hg4dioSEBOzevbvAYznHiYiKRKEAunQR86EyM/M+XqeO+BCoNGeOeI4yXDVoID7oERWTQiGqfSUni1tKiqi70LKl+pg5c8Scr5QU7eOSk0U9lC1b1MfWqiVGsOqSO2R5e4vhkLp4e4t1epXCw0Ugy6/Xq2dP9bFPn4rReJaWuTqPFApRDz+/EusJCSJB6uPkpA5uLi7aPzW3uYYVEcGI5jiZm5ujVatWiIyMVAUnuVyOyMhIjBo1qtDnkcvlWvOYiIhKjUwGHDggJsvfuiVKxl27pi4h5+qqffwPP2h/9S+TiXCl7JnSHDqsUHDIERVIJlMvtFmrlu5jNKYKF+jYMRFclMFK82fuCuNNm4rcr3mcsthe7h6niAgx6lWX2rVF5lEKDhbtMDPLHbRkcHV1wh9/OIn5hgD+/FMUDbG3Bxyss+CQfg/2T+Ph8DgW9vf+g2PiJRGqbtwQDXzwQNwuXND/RtjYFByuXFxEEOMQXSKCAQzVCw8Px6BBg+Dv7482bdpg/vz5SEtLw+DBgwEAAwcOhJubG+bMmQNAzFny9/eHp6cnMjIysHPnTvz2229YsmSJlC+DiCo6U1P1cKAePXQfI5eL2tqadbmTk0XgunUrb49Vw4bia3fNoX/Kn9Wrl/1rokqpbl1xK4wNG7TvKxTq4YO5f51nzBA9Trp6vRwdtY9V1mnKyhKh6NEj9WNubtrHzp8vQpZgBqD2/98CYWsrhjkqfRCWgX+js1HL7hncrB6jlmkS3OS34ZZxE25p19DgySnIkhLVi/Bcvy5u+piaivKN+fViaf5k7zJRhSZ5cOrXrx8ePHiAKVOmICkpCc2bN0dERISqYER8fDxMNL7pSUtLw8iRI3H79m1YWVmhcePGWLNmDfr16yfVSyAiEkxMtNeRUg47UvZSac6bUn5oA4BLl/Keq0cPUTJdaccO0XPFoX8kIZlMDK/TVdpdOWWwMC5cEHO8cvd4paTk7YTt0kVkktzHJieLXihN//5ngaPnLQDYAHAB4KN6zNJSDHmEDEBqKmZ8kYHY/7LgZvkYblXuwU2eALeMm6iVeg01H1+BadId0XOVkyPGNuY3vlGTg0PB4crVVSRJ9jYTGR3J13Eqb5zjREQGQS4XpZpzD/27dk2MaRoyBPj5Z3FserqY8K4c2lenjuiZUvZOtWmjGtZEVJlkZWmvs3vqlOjcvXtXFO9T/rxzRwQnzSJ+bdqI43WxsxPBTJadBTx4gOU/ZuHR7RdwM01CLWUP1rMrsHsUpw5VRZkyYGZWcLhydRUVBHOXZiSiUmVUC+CWNwYnIjJ4z5+LW40a4v7du2KxoKtXxeSU3AYNAn75RWxnZgLvvKMdrBo2VJ+LiACIcu7XrqmDlTJoJSWJ4nyaI/hatwZOn857Djs7UQ3xzGmFSFpJSdixJROZD5JFD1b6DTg/u44q9+6IEycmAo8fF62h1avrD1fKbXt79mIRFQODkx4MTkRktJRD/zR7p65dE+XKhgwRx1y5Isqd5eboKAJUWBjwwQdin1yu7s0iIgCi/PrTp9rfNcydK0bUaoYs5dyqBg2A//5TH+vvD5w5o75vYiKyjZubOHbtqgyxzlViIs4few7zJ/fglhkLu0dxkN37/3CVlCRuykochWFpWbhiFzVrcnFvIg0MTnowOBFRhXb/PrBunXawio9XPz5jBjB5sti+fl18Xa4c+pe7SEWdOmJiPBHl8eyZCFBpaUCrVur9w4aJOVx374oMlJOjfszLS/yVVNIMWTY2Ilwpbw085Zg66rGqp+rRf49hn5wAswd31eFKOUxQWW2jMGQydcl2XeGqVi31Ps6npEqAwUkPBiciqnSePxch6do10RvVpInYHxGRf4VAQKyEOm2a2H74EPjrL3Woql6dw4KICpCTI77LUA4DVCiA3r3Vj3fpApw7p3sEbu6Q1aqVOLZmTe2AVasWUN8tA6GdNIYDaoYqzX337hW8DpamqlXVIUozUOXetrEp7ltEJDkGJz0YnIiI/p9CIepA6ypQ8d9/wKpVQP/+4tjdu4Hu3dXPrVZNHaIa/l979x4WdZn/f/wFKDAIKMpBEAy+9TUPeUgxSzteku5u665emebqaie/31o1ze1g+VO7chWsLK/CQ6bfrC1Ty+ygdlBLSy/8qphtmoVpKV9LAUVAhhCB3x93w2cGZhjchBF4Pq7rvvjMzD0zt7uj8Zr7vt93J+lPf5K6d/fNnwNo5IqLXQtZHD9uJnsmTbL6JCRI//d/7p9fPWTdcIPJSY5gVRW0Yst1WfgZXROb7T5cOdpPP5llvHUVFuY9XMXGmn584dJkVFZKJSVm9rWwsObP226zismuX2/2FTr32b790lg12mgOwAUA+JCfn9nIERkpDRjg+lh5uesao6AgKSXFWvqXny/t3GmaZE44dQSnjAxp5kwrWCUmmt/6EhKYqQLcaNXKhJ///E/PfY4eNdXRnQtZOK6r1345dMhMLtU8oipAnTq103ffWefEjR5t6lrExkqh8WbLY4itUja/XxQbdFqjrv62KlDt3hug8tzTCsk/rpC8Y7LlHlNISZ5CiuwKLMqSn3N6cyckxHu4io01v23z70S9KS21AkxhodS1q1W8cft288+6uyBUVCS9+aZZxS2Z0zfmzvX8Pnv2WMtYDxyQXn3V9fGzZ11P6WgMCE4AgJoCAlz3N918s2mS69I/xwxV795W3337pM2bTavOZpNWrpSGDjW3s7KkL74w/yV2hCuW/QA1+Pub6uQxMa5/3dzZsaNmtUDHdWKia9/Nm81yQld+kmzq3r2DRj1snUj8187mr7w7/xFXosPPvlu1ueuONXfoSH6EbOVFCjlXYH7a7Qr53q7o73OUqieqnvuOhum02ipEdtlUopCW5xUSGSJbVKhatQ9TlysrqsJVZftY+cX9GrSa0XlYZWVmdsbxx83Kko4ccQ02ztfPPGOOFZNMuFm2zHqs+uHVhw+bs90lc2RgWprncZw6ZQWn0FDz08/PXIeHmxYWZn4GBVnPu/lm87rOjzfGLXQEJwDAhQkJkXr0MM2dwYOl//kf1+IUx46Z385KSswyP4fPPpPuv9/1+W3bmgDVsaP5SvOaa8z9+fnmq/EOHVwP7wHg4vLLTauLFSvMEsCff7ZOQigpMT/j4137xsebX7od/ex2a2I6uI3NHIXwqwMbpYPZ7t8zoV2xUv+frWpZ4Lz3n9CuQqdqoGWSfjatzb/ylf9J26qHbtUmfaFOv4asnxXS4pxsLcsVElyu8Fbl2nT/O1UzV4u3d1dWXlvZ2toU0spPNtuvM2oh5pf24cNNIJXMP1GOIqOOx4ODf3suO3vW/NPlLtwUFkoTJljL1RYuNKui3fX75Rez1bTdr5OFzz8vLVni+X2nTbOCU2Gh9MMPNfu0amUCjPOqzORkacyYmiHIce0cvB98UJo40byO439HT/r1axrHDbLHCQDQMH75xXzlHRtrlUB/911p6VIrXDlqPDts3SrddJO5XrJEeuAB85tMbKwVrhw/hw2zvgoF0CDKykyAKitzXTL4v/9rjqxyhDDnQBYSIk2ebPV99FFzkoLdLtnPlquk6LzsRRUqsVcqPLBE+8c9W7VcsP8X85RR0svtWMJVoAK1qbo9WB/pEw1229fPr1LlS5bJr4NZIjh8Zhet3VBzCsQRto4ds/7ZmjvXfOdjs5lWfenbgQNWGPrLX8zyNk9OnTLfFUnmO6SXXvLc98gRc8aYZGaU3nzTCjbVA84DD1j/fxw5Yr63cu4TGkrRVAeKQ9SC4AQAl7CCAik72/yWkp0t3X679V//Z5+Vpk+vuc7EYds26cYbzfUrr0hz5tQMV47lgFdcYb5OBtConDljZnHsZ87JfixPJcdPy/7TGdl/LlD5qTMaGv5p1XLBV4/coG+L4lQim+wKkV0hVdcV8tfHsgrejNbr2qDbZFeIyhRY433L170v/w5mieDIh2K15i3PUyynT1sT6//932YCvnVr9yFn4UJrFunzz81SSE+zPRER3md2cOEITrUgOAFAI1ZRYXbIO4KVc8h6/nmzjE8yAau2XcvOIevDD83MV/WAFR/vukgfQONTVlZ14HDV4VrurnNyqkq1n1dAVcBy/Oyqg1UvmaHrdCS8l+yt26skNFpBbWwKb9dS4VFBCosJ0bXXSoFxkVJUlMrbRcs/NKS5bMVqlAhOtSA4AUAzcPKk+eq2erhy/MzMtNa81BayYmKkjRut3fj/+pd5XUfAat+er4CBpuD8efOlTG3hylG63bniaF2EhJhDh6OizEFcjuvqtx3XFMhpUJQjBwA0b47yY3UxaJDZkFA9YP3yiwlgba2N6Vq92jVktWhhZqYcSwDnzLF2T58+bfZjUVoZuPS1aGGVQ6+NY9bb+dyrnBzTcnNNc74uLTWbt44eNa0ubLa6hyxH0OLfmAbBjBMAANU5Dgc+dsxUD3Que7VypQlXP/1U85vnH36wgtMTT0ipqeaXGuf9VY7rYcMa3yEmAOqustJsyPIUqqrfzskxQetCBQfXPWRFRZnKEAStKizVqwXBCQBwUZw/b75tdl4OOHmydZKktxJZziErLU1as8Z1n1WHDmYpYEyMOUiYEuxA0+YIWnUNWbm5rrXE6yo4uO4hKzq6yQctglMtCE4AgAZjt5tDcpyXATquP/jACll//av0+uueX+fYMROoJCk9XfrkExOq3LWOHa0ZMgBNV2WlVFzsOVS5u11ScuHvExR0YXu0wsIaVdAiONWC4AQAuOR8/70pOuG8x8qxEf3ECbMs0FHhb+xY6Z//9Pxa2dnWyaVLlpgDZ9wFrNhY84sOxS2A5qO4+MKWDv47QSsw0HvIctxOTPT5Fz0Ep1oQnAAAjVpGhvT111aocm4nT0r5+XWfyXIOWcuWSdu3W6GqeshqZN8iA7gIHDNadZ3Vstsv7PWdT/X1EarqAQDQVF13nWnuVFa6hpt775WSk92HrNxc842vw9at0htveH5f55C1YoW0a5f7kBUTYwU3AI1bq1amOfZjemO3X9jSwaioeh3+xcaMEwAAzVF5uRQQYN3++GPpyy/dh6zCQlPty1GgYvRoU13Qk59+sso6v/GGtHdvzYDVvr0p9c5SQQA+xIwTAAConXNokqTBg01zp6TEtarfyJHSFVe47sNytPPnXb9F3rjRc8hq0UI6ftya+VqzxixDdLcni0NBAfgYwQkAANTOZnO9/ac/mVZdRYV05ozrZu+hQ6W4OBOqnIPWqVOmf7t2Vt9335XefNP9GEJDTQn3yEhz+513pG+/rblMMDqa0u0A6gXBCQAAXBz+/mb5nbM77jCtunPnzB4H55mvP/zBPN85YP38s5nxstuliAir71tvSatWuR9HmzbS4cPWWFauNDNZjmpekZGu18xmAagDghMAAGh4gYHmkF9nY8aY5sxxKGhOjmvISkkxM2HVqwpWVEgFBVLr1lbf994zywA9yc21ZrIWLTKVC6sHLMftyy/3eflkAL7B33wAAHDp8vMzpdDDwlzvv/de05yVl0unT5tlgM4ha8gQs5TPUckrL8+6Pn/edZZs27baQ1ZenrW8cO5cadMm97NYUVHS9ddb528BaPSoqgcAAJonx2yWcyj76COzrM9dyHKEMkclwBEjzJJBT06dskLZo49K69Z5DlnDh1tLBs+fN8GPc7OAescBuLUgOAEAgIsiM1PKyvIcsr76ygpZd9whvf2259dyDlkPPGDOynK3VDAqSpo0SXL8DnPqlPkZEUFpd+DfQHCqBcEJAAA0uKNHpWPHagasvDzTNm60gs/w4dLatZ5f6/Rpq1DGAw9IS5aY57ZrV3NGa+5cq++RI1JRkfU4BxUDnOMEAABwSbnsMtPq4tVXpWee8Ryy2rSx+p49a35WVFj9nKWlWdfz5klLl1q3w8NdQ9by5daZWl99ZQpuREebRtACCE4AAACXlFatpKQk07z55z9N4Dl1ygpOziHL+Rt0m82EoFOnTCGNwkLTjhwxjzsXsli0yDVkSaZSoSNErV1rCm5IpgrhDz+4hqzISM7TQpNDcAIAAGjMAgOl2FjTarNggWmOg4qr781yDlmxsVKPHq7VBwsKTDt0SAoJsfq+8or08ss13y8iwgSpTz81hyBL0pYt0sGDriErOtrs73KuhAhcgtjjBAAAAM8qK03QyskxLTdXGjbMqvr37LPShg3m/pwcM6NVUWE9v7DQqlw4fry0bFnN93Ds0crMlBISzH0ffCB9+WXNkBUVZUIZVQdxEbDHCQAAABeHn58JKhER0pVX1nz84YdNc3Ccp+UIWaGh1mN9+kj5+a4h7PRpa4+Wo5CFZA4uXr7c/ZhatJAOH5Y6djS3V682SwadA5bzdVgYQQu/GcEJAAAAF09AgFVworr77zfNWVmZtVzQcZaVJN1yi3kt55CVk2OWC1Y/uHjTJs8hSzL7uBx7xl55RfrsM88hKz7eBDOgGj4VAAAA8J2WLd3v0Ro92rTqSktN0HIOWUOGmOBTPWTl5prKg45qgZL0xRemqIYnP/5oVUBMT5c+/NBzyLrqKteiGmjSCE4AAABoPIKCpA4dXO/7859Nc8dudy1mMXq01KWL+5CVk+M6U5aZac7Y8uToUWu54LRpZjYrNNQsDQwNdb1OS5NiYkzfnTtNkYzqfRzXkZEUy7gEEZwAAADQdDmHJkkaONA0dyorXfdC3X+/dP317kPWyZOuISsvz+rnzuzZ1vWaNdLzz3se84EDUteu5vq558whx+7CWGio2V8WH2/6fvON9O237vuFhkrBwez1+g0ITgAAAIBUM1T062daXcyZIz34oFkaePasVFTkeu28J6tzZ+kPf6jZz9EcVQgl6aefTAl4T8aPt4LTW29JTz7pue+OHVL//ub6tdekF1/0POt1113WvrCjR80Y3IW3ZnQwMsEJAAAA+K1iYqyleN7813+Z5k71k4IefFAaOtR9GDt71jqIWDJLGAcMqNnHbjePOweyo0elPXs8j3HQICs4vfuuNGWK+34tW5py9Lfeam6vX2/OC/O0ZPHPf5Yuv9zz+17CCE4AAADApaL6rFfHjtY+Km/uu8+06srLpeJi12WLf/mL1Lu3+zB29qw1iyVJrVubQhjOfUpLzWNlZWYJoMORI+agY086d260wYkDcAEAAABcmLIyK2RFRVnhKStL2r3b85LFxx83IewSwQG4AAAAAOpPy5bWwcjOOnUyrQny9/UAAAAAAOBSR3ACAAAAAC8ITgAAAADgBcEJAAAAALwgOAEAAACAFwQnAAAAAPCC4AQAAAAAXhCcAAAAAMALghMAAAAAeEFwAgAAAAAvCE4AAAAA4AXBCQAAAAC8uCSC08KFC5WYmKjg4GD169dPu3bt8tj35Zdf1g033KCIiAhFREQoJSWl1v4AAAAA8Fv5PDitXr1aU6dO1axZs7R371717NlTgwcPVk5Ojtv+W7du1ahRo/TZZ58pIyNDCQkJGjRokI4fP97AIwcAAADQXPhVVlZW+nIA/fr1U9++fZWeni5JqqioUEJCgiZNmqRp06Z5fX55ebkiIiKUnp6usWPHeu1fWFio1q1bq6CgQOHh4b95/AAAAAAapwvJBj6dcTp37pwyMzOVkpJSdZ+/v79SUlKUkZFRp9ew2+0qKytT27Zt3T5eWlqqwsJClwYAAAAAF8KnwSkvL0/l5eWKiYlxuT8mJkYnTpyo02s89thjiouLcwlfzlJTU9W6deuqlpCQ8JvHDQAAAKB58fkep98iLS1Nq1at0rp16xQcHOy2z+OPP66CgoKqlp2d3cCjBAAAANDYtfDlm0dGRiogIEAnT550uf/kyZNq3759rc999tlnlZaWps2bN6tHjx4e+wUFBSkoKKjqtmNLF0v2AAAAgObNkQnqUvbBp8EpMDBQffr00ZYtWzR06FBJpjjEli1bNHHiRI/Pe/rppzVnzhx9/PHHSk5OvqD3LCoqkiSW7AEAAACQZDJC69ata+3j0+AkSVOnTtW4ceOUnJysa665RgsWLFBxcbHuvvtuSdLYsWPVoUMHpaamSpLmzZunmTNnauXKlUpMTKzaCxUaGqrQ0FCv7xcXF6fs7GyFhYXJz8+v/v5gdVRYWKiEhARlZ2dT5Q/1js8bGhqfOTQkPm9oaHzmGr/KykoVFRUpLi7Oa1+fB6eRI0cqNzdXM2fO1IkTJ9SrVy999NFHVQUjjh07Jn9/ayvW4sWLde7cOQ0fPtzldWbNmqUnn3zS6/v5+/srPj7+ov4ZLobw8HD+wqHB8HlDQ+Mzh4bE5w0Njc9c4+ZtpsnB5+c4NXecK4WGxOcNDY3PHBoSnzc0ND5zzUujrqoHAAAAAA2B4ORjQUFBmjVrlkvlP6C+8HlDQ+Mzh4bE5w0Njc9c88JSPQAAAADwghknAAAAAPCC4AQAAAAAXhCcAAAAAMALghMAAAAAeEFw8qGFCxcqMTFRwcHB6tevn3bt2uXrIaGJSk1NVd++fRUWFqbo6GgNHTpU3333na+HhWYiLS1Nfn5+mjJliq+Hgibs+PHjGjNmjNq1ayebzabu3btrz549vh4WmqDy8nLNmDFDSUlJstlsuvzyyzV79mxRb63pIzj5yOrVqzV16lTNmjVLe/fuVc+ePTV48GDl5OT4emhogrZt26YJEyZo586d2rRpk8rKyjRo0CAVFxf7emho4nbv3q2XXnpJPXr08PVQ0ITl5+drwIABatmypT788EN98803mj9/viIiInw9NDRB8+bN0+LFi5Wenq6DBw9q3rx5evrpp/Xiiy/6emioZ5Qj95F+/fqpb9++Sk9PlyRVVFQoISFBkyZN0rRp03w8OjR1ubm5io6O1rZt23TjjTf6ejhoos6ePavevXtr0aJF+sc//qFevXppwYIFvh4WmqBp06Zpx44d+uKLL3w9FDQDf/zjHxUTE6Ply5dX3Xf77bfLZrPp9ddf9+HIUN+YcfKBc+fOKTMzUykpKVX3+fv7KyUlRRkZGT4cGZqLgoICSVLbtm19PBI0ZRMmTNBtt93m8m8dUB/ef/99JScn64477lB0dLSuvvpqvfzyy74eFpqo/v37a8uWLcrKypIkffXVV9q+fbt+//vf+3hkqG8tfD2A5igvL0/l5eWKiYlxuT8mJkbffvutj0aF5qKiokJTpkzRgAEDdNVVV/l6OGiiVq1apb1792r37t2+HgqagSNHjmjx4sWaOnWqnnjiCe3evVsPPvigAgMDNW7cOF8PD03MtGnTVFhYqM6dOysgIEDl5eWaM2eORo8e7euhoZ4RnIBmZsKECdq/f7+2b9/u66GgicrOztbkyZO1adMmBQcH+3o4aAYqKiqUnJysuXPnSpKuvvpq7d+/X0uWLCE44aJbs2aN3njjDa1cuVLdunXTvn37NGXKFMXFxfF5a+IITj4QGRmpgIAAnTx50uX+kydPqn379j4aFZqDiRMnav369fr8888VHx/v6+GgicrMzFROTo569+5ddV95ebk+//xzpaenq7S0VAEBAT4cIZqa2NhYde3a1eW+Ll26aO3atT4aEZqyRx55RNOmTdOdd94pSerevbuOHj2q1NRUglMTxx4nHwgMDFSfPn20ZcuWqvsqKiq0ZcsWXXfddT4cGZqqyspKTZw4UevWrdOnn36qpKQkXw8JTdjAgQP19ddfa9++fVUtOTlZo0eP1r59+whNuOgGDBhQ44iFrKwsXXbZZT4aEZoyu90uf3/XX6EDAgJUUVHhoxGhoTDj5CNTp07VuHHjlJycrGuuuUYLFixQcXGx7r77bl8PDU3QhAkTtHLlSr333nsKCwvTiRMnJEmtW7eWzWbz8ejQ1ISFhdXYP9eqVSu1a9eOfXWoFw899JD69++vuXPnasSIEdq1a5eWLl2qpUuX+npoaIKGDBmiOXPmqGPHjurWrZu+/PJLPffcc7rnnnt8PTTUM8qR+1B6erqeeeYZnThxQr169dILL7ygfv36+XpYaIL8/Pzc3v/KK6/orrvuatjBoFm6+eabKUeOerV+/Xo9/vjjOnTokJKSkjR16lSNHz/e18NCE1RUVKQZM2Zo3bp1ysnJUVxcnEaNGqWZM2cqMDDQ18NDPSI4AQAAAIAX7HECAAAAAC8ITgAAAADgBcEJAAAAALwgOAEAAACAFwQnAAAAAPCC4AQAAAAAXhCcAAAAAMALghMAAAAAeEFwAgA0W1u3bpWfn5/OnDnj66EAAC5xBCcAAAAA8ILgBAAAAABeEJwAAD5RUVGh1NRUJSUlyWazqWfPnnr77berHncso9uwYYN69Oih4OBgXXvttdq/f7/L66xdu1bdunVTUFCQEhMTNX/+fJfHS0tL9dhjjykhIUFBQUG64oortHz5cpc+mZmZSk5OVkhIiPr376/vvvvO47h//PFH+fn56Z133tEtt9yikJAQ9ezZUxkZGRc0LgBA40JwAgD4RGpqql577TUtWbJEBw4c0EMPPaQxY8Zo27ZtLv0eeeQRzZ8/X7t371ZUVJSGDBmisrIySSbwjBgxQnfeeae+/vprPfnkk5oxY4ZWrFhR9fyxY8fqzTff1AsvvKCDBw/qpZdeUmhoqMt7TJ8+XfPnz9eePXvUokUL3XPPPV7HP336dD388MPat2+fOnXqpFGjRun8+fN1HhcAoHHxq6ysrPT1IAAAzUtpaanatm2rzZs367rrrqu6/7777pPdbtfKlSu1detW3XLLLVq1apVGjhwpSTp9+rTi4+O1YsUKjRgxQqNHj1Zubq4++eSTqtd49NFHtWHDBh04cEBZWVm68sortWnTJqWkpNQYh+M9Nm/erIEDB0qSNm7cqNtuu00lJSUKDg6u8Zwff/xRSUlJWrZsme69915J0jfffKNu3brp4MGD6ty5s9dxAQAaH2acAAAN7vvvv5fdbtett96q0NDQqvbaa6/p8OHDLn2dg1Xbtm115ZVX6uDBg5KkgwcPasCAAS79BwwYoEOHDqm8vFz79u1TQECAbrrpplrH06NHj6rr2NhYSVJOTs6//Rxv4wIAND4tfD0AAEDzc/bsWUnShg0b1KFDB5fHgoKCLtr72Gy2OvVr2bJl1bWfn58kswfrYj8HANB4EZwAAA2ua9euCgoK0rFjx7zOBu3cuVMdO3aUJOXn5ysrK0tdunSRJHXp0kU7duxw6b9jxw516tRJAQEB6t69uyoqKrRt2za3S/Xqi7dxAQAaH4ITAKDBhYWF6eGHH9ZDDz2kiooKXX/99SooKNCOHTsUHh6ucePGVfV96qmn1K5dO8XExGj69OmKjIzU0KFDJUl///vf1bdvX82ePVsjR45URkaG0tPTtWjRIklSYmKixo0bp3vuuUcvvPCCevbsqaNHjyonJ0cjRoyotz+ft3FJ0sCBAzVs2DBNnDix3sYBALh42OMEAPCJ2bNna8aMGUpNTVWXLl30u9/9Ths2bFBSUpJLv7S0NE2ePFl9+vTRiRMn9MEHHygwMFCS1Lt3b61Zs0arVq3SVVddpZkzZ+qpp57SXXfdVfX8xYsXa/jw4frb3/6mzp07a/z48SouLq7XP1tdxnX48GHl5eXV6zgAABcPVfUAAJckR8W7/Px8tWnTxtfDAQA0c8w4AQAAAIAXBCcAAAAA8IKlegAAAADgBTNOAAAAAOAFwQkAAAAAvCA4AQAAAIAXBCcAAAAA8ILgBAAAAABeEJwAAAAAwAuCEwAAAAB4QXACAAAAAC/+PyOwp4a9s2xRAAAAAElFTkSuQmCC",
                        "text/plain": [
                            "<Figure size 1000x600 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Plot loss\n",
                "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
                "x = range(len(epoch_train_loss))\n",
                "\n",
                "\n",
                "plt.figure\n",
                "plt.plot(x, epoch_train_loss, 'r', label=\"train loss\")\n",
                "plt.plot(x, epoch_test_loss, 'b', label=\"validation loss\")\n",
                "\n",
                "plt.plot(x, epoch_train_loss_bn, 'r--', label=\"train loss with BN\")\n",
                "plt.plot(x, epoch_test_loss_bn, 'b--',label=\"validation loss with BN\")\n",
                "\n",
                "plt.xlabel('epoch no.')\n",
                "plt.ylabel('loss')\n",
                "plt.legend(loc='upper right')\n",
                "plt.title('Training and Validation Loss')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The above curves show that when we use Batch Normalization, the training converges faster."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:blue\">Miscellaneous: Calculate Mean and Standard Deviation of Fashion MNIST </font> <a name=\"misc\"></a>\n",
                "\n",
                "Ideally, we should not use the same mean and standard deviation for Fashion MNIST and MNIST. Refrain, even when you find many continuing to do this, simply because it does not have a profound effect on the results.\n",
                "\n",
                "Let us find  the mean and standard deviation for Fashion MNIST and use it instead of MNIST.\n",
                "\n",
                "We need to simply find the mean and standard deviation of the whole dataset. So, we load the dataset and then use the functions given below:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor(0.2860)\n",
                        "tensor(0.3530)\n"
                    ]
                }
            ],
            "source": [
                "import torchvision\n",
                "train_transform = transforms.Compose([transforms.ToTensor()])\n",
                "train_set = torchvision.datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=train_transform)\n",
                "\n",
                "print(train_set.data.float().mean()/255)\n",
                "print(train_set.data.float().std()/255)"
            ]
        }
    ],
    "metadata": {
        "jupytext": {
            "encoding": "# -*- coding: utf-8 -*-"
        },
        "kernelspec": {
            "display_name": "tensorflow",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.18"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
